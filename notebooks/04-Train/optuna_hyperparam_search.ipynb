{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Search (Optuna)\n",
        "\n",
        "TPE sampler + MedianPruner over the multimodal RCA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/root/lemm')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fixed config (not tuned) - Same structure as train_multimodal_rca.ipynb\n",
        "BASE_DIR = Path('/root/lemm')\n",
        "MULTIMODAL_DIR = BASE_DIR / 'core_multimodal_tmp'\n",
        "METRICS_DIR = BASE_DIR / 'core_metrics_tmp'\n",
        "LOGS_DIR = BASE_DIR / 'core_logs_tmp'\n",
        "\n",
        "FIXED_CONFIG = {\n",
        "    'window_size': 22,\n",
        "    'n_metrics': 7,\n",
        "    'bin_seconds': 30,\n",
        "    'log_embed_dim': 384,\n",
        "    'epochs': 30,\n",
        "    'early_stop_patience': 10\n",
        "}\n",
        "\n",
        "# Metric file names - Same as train_multimodal_rca.ipynb\n",
        "METRIC_FILES = [\n",
        "    'pod_cpu_usage_total.parquet',\n",
        "    'pod_memory_working_set.parquet',\n",
        "    'pod_network_rx_bytes.parquet',\n",
        "    'pod_network_tx_bytes.parquet',\n",
        "    'pod_latency_server_p95.parquet',\n",
        "    'pod_latency_client_p95.parquet',\n",
        "    'pod_workload_ops.parquet',\n",
        "]\n",
        "\n",
        "# Split by DAY to guarantee zero temporal overlap (same as train_multimodal_rca.ipynb)\n",
        "# Nezha scenarios have ~8 min gaps but 10 min windows = 85% overlap within same day\n",
        "# By splitting by day, we eliminate ALL data leaks between train/val\n",
        "TRAIN_DAYS = {'20220822', '20230129'}  # Online Boutique day 1 + Train-Ticket day 1\n",
        "VAL_DAYS = {'20220823', '20230130'}    # Online Boutique day 2 + Train-Ticket day 2\n",
        "\n",
        "# Excluded scenarios - Same as train_multimodal_rca.ipynb\n",
        "EXCLUDED = {\n",
        "    '20220822_nezha_14', '20220822_nezha_22', '20220822_nezha_23',\n",
        "    '20220823_nezha_21', '20220823_nezha_24',\n",
        "    '20230130_nezha_15', '20230130_nezha_16',\n",
        "}\n",
        "\n",
        "def get_split(scenario_id):\n",
        "    \"\"\"Determine if scenario belongs to train or val set.\"\"\"\n",
        "    if '_nezha_' in scenario_id:\n",
        "        date_prefix = scenario_id.split('_nezha_')[0]\n",
        "        if date_prefix in VAL_DAYS:\n",
        "            return 'val'\n",
        "        elif date_prefix in TRAIN_DAYS:\n",
        "            return 'train'\n",
        "    # LEMMA scenarios always go to train\n",
        "    return 'train'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def discover_scenarios():\n",
        "    \"\"\"Discover all valid scenarios - same as train_multimodal_rca.ipynb\"\"\"\n",
        "    scenarios = []\n",
        "    for d in sorted(MULTIMODAL_DIR.iterdir()):\n",
        "        if not d.is_dir():\n",
        "            continue\n",
        "        if d.name in EXCLUDED:\n",
        "            continue\n",
        "        if not (d / 'manifest.json').exists():\n",
        "            continue\n",
        "        if not (d / 'ground_truth.json').exists():\n",
        "            continue\n",
        "        scenarios.append(d.name)\n",
        "    return scenarios\n",
        "\n",
        "def load_scenario_data(scenario_id, window_size, n_metrics):\n",
        "    \"\"\"Load scenario data - same structure as train_multimodal_rca.ipynb\"\"\"\n",
        "    multimodal_path = MULTIMODAL_DIR / scenario_id\n",
        "    metrics_path = METRICS_DIR / scenario_id\n",
        "    logs_path = LOGS_DIR / scenario_id\n",
        "    \n",
        "    # Load manifest and ground truth\n",
        "    with open(multimodal_path / 'manifest.json') as f:\n",
        "        manifest = json.load(f)\n",
        "    with open(multimodal_path / 'ground_truth.json') as f:\n",
        "        gt = json.load(f)\n",
        "    \n",
        "    pods = manifest['pods']\n",
        "    services = manifest['services']\n",
        "    rc_service = gt['root_cause_service']\n",
        "    fault_idx = gt['fault_time_idx']\n",
        "    label = services.index(rc_service) if rc_service in services else 0\n",
        "    \n",
        "    # Pod to service mapping (from manifest)\n",
        "    pod_to_service_idx = manifest.get('pod_to_service_idx', [])\n",
        "    if not pod_to_service_idx:\n",
        "        # Fallback: build from pod_to_service dict\n",
        "        pod_to_svc = manifest.get('pod_to_service', {})\n",
        "        pod_to_service_idx = []\n",
        "        for pod in pods:\n",
        "            svc = pod_to_svc.get(pod, None)\n",
        "            if svc and svc in services:\n",
        "                pod_to_service_idx.append(services.index(svc))\n",
        "            else:\n",
        "                pod_to_service_idx.append(-1)\n",
        "    \n",
        "    # Load metrics - same as train_multimodal_rca.ipynb\n",
        "    metrics_dict = {}\n",
        "    for metric_file in METRIC_FILES:\n",
        "        metric_name = metric_file.replace('pod_', '').replace('.parquet', '')\n",
        "        path = metrics_path / metric_file\n",
        "        if path.exists():\n",
        "            metrics_dict[metric_name] = pd.read_parquet(path)\n",
        "    \n",
        "    # Build metrics tensor: (T, n_pods, n_metrics)\n",
        "    metrics_list = []\n",
        "    for metric_name in ['cpu_usage_total', 'memory_working_set', 'network_rx_bytes', \n",
        "                        'network_tx_bytes', 'latency_server_p95', 'latency_client_p95', 'workload_ops']:\n",
        "        if metric_name in metrics_dict:\n",
        "            df = metrics_dict[metric_name]\n",
        "            metrics_list.append(df.values)  # (T, n_pods)\n",
        "        else:\n",
        "            # Fill with NaN if metric missing\n",
        "            first_metric = list(metrics_dict.values())[0] if metrics_dict else None\n",
        "            n_timesteps = len(first_metric) if first_metric is not None else window_size\n",
        "            metrics_list.append(np.full((n_timesteps, len(pods)), np.nan))\n",
        "    \n",
        "    metrics = np.stack(metrics_list, axis=-1).astype(np.float32)  # (T, n_pods, n_metrics)\n",
        "    n_bins = metrics.shape[0]\n",
        "    \n",
        "    # Load logs - same as train_multimodal_rca.ipynb\n",
        "    logs_file = logs_path / 'logs_service_texts.parquet'\n",
        "    log_texts = {svc: '[N-LGS-DST-TKN-LEZHSA]' for svc in services}\n",
        "    if logs_file.exists():\n",
        "        logs_df = pd.read_parquet(logs_file)\n",
        "        for service in services:\n",
        "            if service in logs_df.columns:\n",
        "                service_logs = logs_df[service].fillna('').tolist()\n",
        "                combined = ' | '.join([l for l in service_logs if l.strip()])\n",
        "                if combined.strip():\n",
        "                    log_texts[service] = combined[:512]  # Truncate for MiniLM\n",
        "    \n",
        "    return {\n",
        "        'metrics': metrics, 'n_bins': n_bins, 'fault_idx': fault_idx,\n",
        "        'pods': pods, 'services': services, 'log_texts': log_texts,\n",
        "        'label': label, 'rc_service': rc_service,\n",
        "        'pod_to_service_idx': pod_to_service_idx\n",
        "    }"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RCADataset(Dataset):\n",
        "    def __init__(self, scenario_ids, config, mode='train'):\n",
        "        self.scenario_ids = scenario_ids\n",
        "        self.config = config\n",
        "        self.mode = mode\n",
        "        \n",
        "        # PRE-LOAD ALL DATA INTO MEMORY (major speedup)\n",
        "        self.data_cache = {}\n",
        "        all_services = set()\n",
        "        for sid in scenario_ids:\n",
        "            data = load_scenario_data(sid, FIXED_CONFIG['window_size'], FIXED_CONFIG['n_metrics'])\n",
        "            self.data_cache[sid] = data\n",
        "            all_services.update(data['services'])\n",
        "        self.all_services = sorted(all_services)\n",
        "        print(f\"[Dataset] Cached {len(scenario_ids)} scenarios in memory\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.scenario_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sid = self.scenario_ids[idx]\n",
        "        data = self.data_cache[sid]  # Use cached data instead of loading\n",
        "        \n",
        "        metrics = data['metrics'].copy()  # Copy to avoid modifying cache\n",
        "        window = FIXED_CONFIG['window_size']\n",
        "        n_bins = data['n_bins']\n",
        "        fault_idx = data['fault_idx']\n",
        "        \n",
        "        if n_bins <= window:\n",
        "            if n_bins < window:\n",
        "                pad = window - n_bins\n",
        "                metrics = np.pad(metrics, ((0, pad), (0, 0), (0, 0)), constant_values=np.nan)\n",
        "        else:\n",
        "            jitter = self.config.get('jitter', 0) if self.mode == 'train' else 0\n",
        "            offset = np.random.randint(-jitter, jitter + 1) if jitter > 0 else 0\n",
        "            ideal_start = fault_idx - window // 2 + offset\n",
        "            start = max(0, min(ideal_start, n_bins - window))\n",
        "            metrics = metrics[start:start + window]\n",
        "        \n",
        "        valid_mask = ~np.isnan(metrics).any(axis=-1)\n",
        "        \n",
        "        for m in range(metrics.shape[-1]):\n",
        "            vals = metrics[:, :, m]\n",
        "            valid = ~np.isnan(vals)\n",
        "            if valid.sum() > 1:\n",
        "                mean = np.nanmean(vals)\n",
        "                std = np.nanstd(vals)\n",
        "                if std > 1e-8:\n",
        "                    metrics[:, :, m] = (vals - mean) / std\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            noise_std = self.config.get('noise_std', 0.0)\n",
        "            mask_prob = self.config.get('mask_prob', 0.0)\n",
        "            if noise_std > 0:\n",
        "                noise = np.random.normal(0, noise_std, metrics.shape).astype(np.float32)\n",
        "                metrics = metrics + noise * valid_mask[:, :, np.newaxis]\n",
        "            if mask_prob > 0:\n",
        "                mask_random = np.random.random(valid_mask.shape) > mask_prob\n",
        "                valid_mask = valid_mask & mask_random\n",
        "        \n",
        "        metrics = np.nan_to_num(metrics, nan=0.0)\n",
        "        \n",
        "        return {\n",
        "            'scenario_id': sid,\n",
        "            'metrics': torch.tensor(metrics, dtype=torch.float32),\n",
        "            'metrics_mask': torch.tensor(valid_mask, dtype=torch.bool),\n",
        "            'log_texts': data['log_texts'],\n",
        "            'pods': data['pods'],\n",
        "            'services': data['services'],\n",
        "            'pod_to_service_idx': data['pod_to_service_idx'],\n",
        "            'label': data['label'],\n",
        "            'rc_service': data['rc_service']\n",
        "        }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MetricEncoder(nn.Module):\n",
        "    def __init__(self, n_metrics, d_model, n_heads, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(n_metrics, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads,\n",
        "            dim_feedforward=d_model * 4, dropout=dropout,\n",
        "            activation='gelu', batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        B, W, P, M = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).reshape(B * P, W, M)\n",
        "        if mask is not None:\n",
        "            mask = mask.permute(0, 2, 1).reshape(B * P, W)\n",
        "            src_key_padding_mask = ~mask\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.output_proj(x)\n",
        "        x = x.view(B, P, -1)\n",
        "        return x\n",
        "\n",
        "class LogEncoder(nn.Module):\n",
        "    def __init__(self, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.sentence_model.eval()\n",
        "        for p in self.sentence_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(384, output_dim), nn.GELU(),\n",
        "            nn.Dropout(dropout), nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, log_texts_batch, services_batch):\n",
        "        all_texts = []\n",
        "        structure = []\n",
        "        for log_texts, services in zip(log_texts_batch, services_batch):\n",
        "            texts = [log_texts.get(s, '[N-LGS-DST-TKN-LEZHSA]') for s in services]\n",
        "            all_texts.extend(texts)\n",
        "            structure.append(len(services))\n",
        "        if not all_texts:\n",
        "            return []\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.sentence_model.encode(all_texts, convert_to_tensor=True)\n",
        "        # Clone to allow gradients through proj (sentence_model is frozen)\n",
        "        embeddings = embeddings.clone().detach().requires_grad_(True)\n",
        "        embeddings = self.proj(embeddings)\n",
        "        result = []\n",
        "        idx = 0\n",
        "        for n in structure:\n",
        "            result.append(embeddings[idx:idx + n])\n",
        "            idx += n\n",
        "        return result\n",
        "\n",
        "class FusionLayer(nn.Module):\n",
        "    def __init__(self, metric_dim, log_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.metric_dim = metric_dim\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(metric_dim * 2 + log_dim, output_dim), nn.GELU(),\n",
        "            nn.Dropout(dropout), nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, metric_emb, log_emb, pod_to_service_idx, services_batch):\n",
        "        B = metric_emb.size(0)\n",
        "        device = metric_emb.device\n",
        "        fused = []\n",
        "        for b in range(B):\n",
        "            services = services_batch[b]\n",
        "            n_svc = len(services)\n",
        "            p2s = pod_to_service_idx[b]\n",
        "            svc_features = []\n",
        "            for s_idx in range(n_svc):\n",
        "                pod_indices = [i for i, si in enumerate(p2s) if si == s_idx]\n",
        "                if pod_indices:\n",
        "                    pod_metrics = metric_emb[b, pod_indices]\n",
        "                    svc_max = pod_metrics.max(dim=0).values\n",
        "                    svc_mean = pod_metrics.mean(dim=0)\n",
        "                    svc_metric = torch.cat([svc_max, svc_mean], dim=-1)\n",
        "                else:\n",
        "                    svc_metric = torch.zeros(2 * self.metric_dim, device=device)\n",
        "                svc_log = log_emb[b][s_idx]\n",
        "                combined = torch.cat([svc_metric, svc_log], dim=-1)\n",
        "                svc_features.append(combined)\n",
        "            svc_tensor = torch.stack(svc_features)\n",
        "            fused.append(self.fusion(svc_tensor))\n",
        "        return fused\n",
        "\n",
        "class SimilarityClassifier(nn.Module):\n",
        "    def __init__(self, embed_dim, temperature):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.sentence_model.eval()\n",
        "        for p in self.sentence_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.proj = nn.Linear(384, embed_dim)\n",
        "    \n",
        "    def forward(self, fused_batch, services_batch):\n",
        "        B = len(fused_batch)\n",
        "        device = fused_batch[0].device\n",
        "        max_services = max(len(s) for s in services_batch)\n",
        "        logits = []\n",
        "        for b in range(B):\n",
        "            services = services_batch[b]\n",
        "            fused = fused_batch[b]\n",
        "            with torch.no_grad():\n",
        "                svc_emb = self.sentence_model.encode(services, convert_to_tensor=True)\n",
        "            # Clone to allow gradients through proj (sentence_model is frozen)\n",
        "            svc_emb = svc_emb.clone().detach().requires_grad_(True).to(device)\n",
        "            svc_emb = self.proj(svc_emb)\n",
        "            fused_norm = F.normalize(fused, dim=-1)\n",
        "            svc_norm = F.normalize(svc_emb, dim=-1)\n",
        "            sim = (fused_norm * svc_norm).sum(dim=-1) / self.temperature\n",
        "            n_svc = len(services)\n",
        "            if n_svc < max_services:\n",
        "                pad = torch.full((max_services - n_svc,), -100.0, device=device)\n",
        "                sim = torch.cat([sim, pad])\n",
        "            logits.append(sim)\n",
        "        return torch.stack(logits)\n",
        "\n",
        "class MultimodalRCA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.metric_encoder = MetricEncoder(\n",
        "            n_metrics=FIXED_CONFIG['n_metrics'],\n",
        "            d_model=config['d_model'], n_heads=config['n_heads'],\n",
        "            n_layers=config['n_layers'], dropout=config['dropout']\n",
        "        )\n",
        "        self.log_encoder = LogEncoder(output_dim=config['d_model'], dropout=config['dropout'])\n",
        "        self.fusion = FusionLayer(\n",
        "            metric_dim=config['d_model'], log_dim=config['d_model'],\n",
        "            output_dim=config['fusion_dim'], dropout=config['dropout']\n",
        "        )\n",
        "        self.classifier = SimilarityClassifier(\n",
        "            embed_dim=config['fusion_dim'], temperature=config['temperature']\n",
        "        )\n",
        "    \n",
        "    def forward(self, metrics, metrics_mask, log_texts, pod_to_service_idx, services):\n",
        "        metric_emb = self.metric_encoder(metrics, metrics_mask)\n",
        "        log_emb = self.log_encoder(log_texts, services)\n",
        "        fused = self.fusion(metric_emb, log_emb, pod_to_service_idx, services)\n",
        "        logits = self.classifier(fused, services)\n",
        "        return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def collate_fn(batch):\n",
        "    max_pods = max(s['metrics'].shape[1] for s in batch)\n",
        "    window = FIXED_CONFIG['window_size']\n",
        "    n_metrics = FIXED_CONFIG['n_metrics']\n",
        "    \n",
        "    metrics_padded, masks_padded, pod_to_service_padded = [], [], []\n",
        "    for s in batch:\n",
        "        m = s['metrics']\n",
        "        mask = s['metrics_mask']\n",
        "        n_pods = m.shape[1]\n",
        "        if n_pods < max_pods:\n",
        "            pad_m = torch.zeros(window, max_pods - n_pods, n_metrics)\n",
        "            pad_mask = torch.zeros(window, max_pods - n_pods, dtype=torch.bool)\n",
        "            m = torch.cat([m, pad_m], dim=1)\n",
        "            mask = torch.cat([mask, pad_mask], dim=1)\n",
        "        metrics_padded.append(m)\n",
        "        masks_padded.append(mask)\n",
        "        p2s = s['pod_to_service_idx'] + [-1] * (max_pods - n_pods)\n",
        "        pod_to_service_padded.append(p2s)\n",
        "    \n",
        "    return {\n",
        "        'scenario_ids': [s['scenario_id'] for s in batch],\n",
        "        'metrics': torch.stack(metrics_padded),\n",
        "        'metrics_mask': torch.stack(masks_padded),\n",
        "        'log_texts': [s['log_texts'] for s in batch],\n",
        "        'pods': [s['pods'] for s in batch],\n",
        "        'services': [s['services'] for s in batch],\n",
        "        'pod_to_service_idx': pod_to_service_padded,\n",
        "        'labels': torch.tensor([s['label'] for s in batch], dtype=torch.long),\n",
        "        'rc_services': [s['rc_service'] for s in batch]\n",
        "    }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_epoch(model, dataloader, optimizer, config):\n",
        "    model.train()\n",
        "    total_loss, n_batches = 0, 0\n",
        "    for batch in dataloader:\n",
        "        metrics = batch['metrics'].to(DEVICE)\n",
        "        metrics_mask = batch['metrics_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "        logits = model(metrics, metrics_mask, batch['log_texts'],\n",
        "                      batch['pod_to_service_idx'], batch['services'])\n",
        "        loss = F.cross_entropy(logits, labels, label_smoothing=config.get('label_smoothing', 0.0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.get('grad_clip_norm', 1.0))\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    return total_loss / n_batches\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_ranks = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            metrics = batch['metrics'].to(DEVICE)\n",
        "            metrics_mask = batch['metrics_mask'].to(DEVICE)\n",
        "            logits = model(metrics, metrics_mask, batch['log_texts'],\n",
        "                          batch['pod_to_service_idx'], batch['services'])\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(batch['labels'].tolist())\n",
        "            for i, label in enumerate(batch['labels']):\n",
        "                sorted_idx = logits[i].cpu().argsort(descending=True)\n",
        "                rank = (sorted_idx == label).nonzero(as_tuple=True)[0].item() + 1\n",
        "                all_ranks.append(rank)\n",
        "    acc1 = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
        "    mrr = sum(1.0/r for r in all_ranks) / len(all_ranks)\n",
        "    return {'acc1': acc1, 'mrr': mrr}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective(trial):\n",
        "    config = {\n",
        "        'd_model': trial.suggest_categorical('d_model', [64, 128, 256]),\n",
        "        'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),\n",
        "        'n_layers': trial.suggest_int('n_layers', 1, 3),\n",
        "        'fusion_dim': trial.suggest_categorical('fusion_dim', [128, 256]),\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
        "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True),\n",
        "        'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.1),\n",
        "        'grad_clip_norm': trial.suggest_float('grad_clip_norm', 0.5, 2.0),\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),  # Increased for better GPU utilization\n",
        "        'learning_rate': trial.suggest_float('lr', 1e-5, 5e-4, log=True),\n",
        "        'warmup_epochs': trial.suggest_int('warmup_epochs', 3, 15),\n",
        "        'temperature': trial.suggest_float('temperature', 0.05, 0.2),\n",
        "        'noise_std': trial.suggest_float('noise_std', 0.0, 0.2),\n",
        "        'mask_prob': trial.suggest_float('mask_prob', 0.0, 0.2),\n",
        "        'jitter': trial.suggest_int('jitter', 0, 5)\n",
        "    }\n",
        "    \n",
        "    if config['d_model'] % config['n_heads'] != 0:\n",
        "        return 0.0\n",
        "    \n",
        "    all_scenarios = discover_scenarios()\n",
        "    train_scenarios = [s for s in all_scenarios if get_split(s) == 'train']\n",
        "    val_scenarios = [s for s in all_scenarios if get_split(s) == 'val']\n",
        "    \n",
        "    train_dataset = RCADataset(train_scenarios, config, mode='train')\n",
        "    val_dataset = RCADataset(val_scenarios, config, mode='val')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "    \n",
        "    model = MultimodalRCA(config).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "    \n",
        "    best_mrr = 0.0\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(FIXED_CONFIG['epochs']):\n",
        "        if epoch < config['warmup_epochs']:\n",
        "            lr = config['learning_rate'] * (epoch + 1) / config['warmup_epochs']\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg['lr'] = lr\n",
        "        \n",
        "        train_epoch(model, train_loader, optimizer, config)\n",
        "        results = evaluate(model, val_loader)\n",
        "        mrr = results['mrr']\n",
        "        \n",
        "        trial.report(mrr, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "        \n",
        "        if mrr > best_mrr:\n",
        "            best_mrr = mrr\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= FIXED_CONFIG['early_stop_patience']:\n",
        "                break\n",
        "    \n",
        "    return best_mrr\n",
        "\n",
        "print('Objective function ready')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Objective function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optimized: ~8 hours with cache speedup, more random exploration\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    study_name='multimodal_rca_hpo',\n",
        "    # More random exploration to cover discrete space well\n",
        "    pruner=optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=15,       # 15 random trials (up from 10)\n",
        "        n_warmup_steps=12,         # 12 epochs before pruning each trial\n",
        "        interval_steps=1           # Check for pruning every epoch\n",
        "    ),\n",
        "    sampler=optuna.samplers.TPESampler(\n",
        "        n_startup_trials=15,       # Match pruner startup\n",
        "        seed=42                    # Reproducibility\n",
        "    )\n",
        ")\n",
        "\n",
        "print('='*70)\n",
        "print('OPTUNA HYPERPARAMETER SEARCH')\n",
        "print('='*70)\n",
        "print(f'Objective: Maximize MRR')\n",
        "print(f'Epochs per trial: {FIXED_CONFIG[\"epochs\"]}')\n",
        "print('='*70)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:41:47,809] A new study created in memory with name: multimodal_rca_hpo\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "OPTUNA HYPERPARAMETER SEARCH\n",
            "======================================================================\n",
            "Objective: Maximize MRR\n",
            "Epochs per trial: 30\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Balanced: 100 trials for ~8 hours total\n",
        "N_TRIALS = 100\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, gc_after_trial=True)\n",
        "print(f'\\nCompleted {len(study.trials)} trials')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 0. Best value: 0.495667:   1%|          | 1/100 [02:51<4:43:12, 171.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:44:39,265] Trial 0 finished with value: 0.4956665864999198 and parameters: {'d_model': 128, 'n_heads': 2, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.3832290311184182, 'weight_decay': 1.1527987128232396e-05, 'label_smoothing': 0.09699098521619944, 'grad_clip_norm': 1.7486639612006325, 'batch_size': 8, 'lr': 3.2877474139911175e-05, 'warmup_epochs': 9, 'temperature': 0.11479175279631737, 'noise_std': 0.058245828039608386, 'mask_prob': 0.1223705789444759, 'jitter': 0}. Best is trial 0 with value: 0.4956665864999198.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 0. Best value: 0.495667:   2%|▏         | 2/100 [04:39<3:39:18, 134.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:46:27,392] Trial 1 finished with value: 0.47889194139194136 and parameters: {'d_model': 256, 'n_heads': 2, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.16820964947491662, 'weight_decay': 1.5673095467235405e-05, 'label_smoothing': 0.09488855372533334, 'grad_clip_norm': 1.948448049611839, 'batch_size': 8, 'lr': 0.00014537555576161912, 'warmup_epochs': 8, 'temperature': 0.06830573522671683, 'noise_std': 0.09903538202225404, 'mask_prob': 0.006877704223043679, 'jitter': 5}. Best is trial 0 with value: 0.4956665864999198.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 0. Best value: 0.495667:   3%|▎         | 3/100 [07:30<4:04:00, 150.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:49:18,141] Trial 2 finished with value: 0.46569872393401807 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.4579309401710595, 'weight_decay': 0.0006218704727769079, 'label_smoothing': 0.09218742350231168, 'grad_clip_norm': 0.6327387530778792, 'batch_size': 32, 'lr': 4.574578205475403e-05, 'warmup_epochs': 6, 'temperature': 0.17431062637278943, 'noise_std': 0.07135066533871785, 'mask_prob': 0.05618690193747616, 'jitter': 3}. Best is trial 0 with value: 0.4956665864999198.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 3. Best value: 0.518283:   3%|▎         | 3/100 [10:11<4:04:00, 150.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:51:58,871] Trial 3 finished with value: 0.5182829207829208 and parameters: {'d_model': 128, 'n_heads': 2, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.3916028672163949, 'weight_decay': 0.0020597335357437196, 'label_smoothing': 0.007404465173409036, 'grad_clip_norm': 1.0376985928164089, 'batch_size': 16, 'lr': 3.649100451857357e-05, 'warmup_epochs': 3, 'temperature': 0.09664734825734933, 'noise_std': 0.06503666440534941, 'mask_prob': 0.1459212356676128, 'jitter': 3}. Best is trial 3 with value: 0.5182829207829208.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 3. Best value: 0.518283:   4%|▍         | 4/100 [10:11<4:07:41, 154.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:   5%|▌         | 5/100 [11:55<3:36:23, 136.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:53:43,402] Trial 4 finished with value: 0.5811022927689594 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.27101640734341986, 'weight_decay': 1.1919481947918725e-05, 'label_smoothing': 0.010789142699330446, 'grad_clip_norm': 0.5471437785301014, 'batch_size': 8, 'lr': 0.00034827974366176894, 'warmup_epochs': 6, 'temperature': 0.11155743845534447, 'noise_std': 0.15111022770860974, 'mask_prob': 0.045759633098324495, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:   6%|▌         | 6/100 [14:47<3:52:52, 148.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 21:56:35,286] Trial 5 finished with value: 0.5514734648067982 and parameters: {'d_model': 256, 'n_heads': 8, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.31573689676626027, 'weight_decay': 0.0026443593078398627, 'label_smoothing': 0.08960912999234932, 'grad_clip_norm': 0.9770052124577958, 'batch_size': 32, 'lr': 0.0002453480159918335, 'warmup_epochs': 14, 'temperature': 0.05104281957967861, 'noise_std': 0.10214946051551316, 'mask_prob': 0.08348220062975581, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:   7%|▋         | 7/100 [19:00<4:43:26, 182.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:00:48,624] Trial 6 finished with value: 0.51303907917943 and parameters: {'d_model': 256, 'n_heads': 8, 'n_layers': 2, 'fusion_dim': 128, 'dropout': 0.20071291833014568, 'weight_decay': 0.0003102740950912838, 'label_smoothing': 0.03008783098167697, 'grad_clip_norm': 0.9272607415662014, 'batch_size': 16, 'lr': 1.223096868463762e-05, 'warmup_epochs': 6, 'temperature': 0.1862398828949981, 'noise_std': 0.04791237813339449, 'mask_prob': 0.02897897441824462, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:   8%|▊         | 8/100 [22:53<5:04:48, 198.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:04:41,496] Trial 7 finished with value: 0.5423124805477747 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.31430987362990337, 'weight_decay': 1.865818136012483e-05, 'label_smoothing': 0.0835302495589238, 'grad_clip_norm': 0.9811700974576038, 'batch_size': 32, 'lr': 0.0001416320452869398, 'warmup_epochs': 3, 'temperature': 0.12681395874489215, 'noise_std': 0.04529915503958759, 'mask_prob': 0.12903455808189, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:   9%|▉         | 9/100 [27:13<5:30:22, 217.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:09:01,206] Trial 8 finished with value: 0.5544254909941184 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.3639936184136716, 'weight_decay': 0.0028292192255361887, 'label_smoothing': 0.055520081159946236, 'grad_clip_norm': 1.2944758675340098, 'batch_size': 32, 'lr': 0.00033867510216238227, 'warmup_epochs': 11, 'temperature': 0.1008544686573051, 'noise_std': 0.06984191492253218, 'mask_prob': 0.14519113577404788, 'jitter': 5}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  10%|█         | 10/100 [31:21<5:40:29, 226.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:13:08,700] Trial 9 finished with value: 0.5202098765432098 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3654007076432223, 'weight_decay': 1.0355826161899173e-05, 'label_smoothing': 0.016080805141749865, 'grad_clip_norm': 1.3231006840498791, 'batch_size': 8, 'lr': 0.0001621702307943825, 'warmup_epochs': 6, 'temperature': 0.09880995472389018, 'noise_std': 0.14929828102360485, 'mask_prob': 0.12992657980944294, 'jitter': 5}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  11%|█         | 11/100 [35:14<5:39:37, 228.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:17:02,119] Trial 10 finished with value: 0.4903812636165577 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.35245545039890513, 'weight_decay': 0.0024234491447023164, 'label_smoothing': 0.05026370931051921, 'grad_clip_norm': 1.3653558269395387, 'batch_size': 32, 'lr': 2.9993270329588456e-05, 'warmup_epochs': 3, 'temperature': 0.14682084438607518, 'noise_std': 0.03542213588140979, 'mask_prob': 0.18809171687058288, 'jitter': 5}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  12%|█▏        | 12/100 [39:28<5:46:54, 236.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:21:15,955] Trial 11 finished with value: 0.5424780774780774 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.2540390914407701, 'weight_decay': 0.0035761029634855065, 'label_smoothing': 0.031692200515627766, 'grad_clip_norm': 0.7542391200291387, 'batch_size': 16, 'lr': 9.300725537404403e-05, 'warmup_epochs': 4, 'temperature': 0.14225108400487546, 'noise_std': 0.19801077002085266, 'mask_prob': 0.028016803047304806, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  13%|█▎        | 13/100 [43:23<5:42:20, 236.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:25:11,070] Trial 12 finished with value: 0.5419715046512432 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.4652962210225885, 'weight_decay': 0.00034200085877350014, 'label_smoothing': 0.05015162946871996, 'grad_clip_norm': 1.6974427684501627, 'batch_size': 32, 'lr': 0.0003251564545592096, 'warmup_epochs': 7, 'temperature': 0.10633744289599162, 'noise_std': 0.018796387968173803, 'mask_prob': 0.1156560281992348, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  14%|█▍        | 14/100 [47:22<5:39:43, 237.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:29:10,203] Trial 13 finished with value: 0.5239791472144414 and parameters: {'d_model': 128, 'n_heads': 2, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.3088973040219217, 'weight_decay': 0.002041647020718277, 'label_smoothing': 0.02158210274968432, 'grad_clip_norm': 1.4343357137285004, 'batch_size': 32, 'lr': 8.289395383720038e-05, 'warmup_epochs': 11, 'temperature': 0.15891370005839925, 'noise_std': 0.19517041589250694, 'mask_prob': 0.10326006966023907, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  15%|█▌        | 15/100 [51:03<5:29:01, 232.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:32:51,432] Trial 14 finished with value: 0.5739798021911843 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.16931772802833833, 'weight_decay': 2.946531769463084e-05, 'label_smoothing': 0.025024289816459534, 'grad_clip_norm': 1.3238399970591808, 'batch_size': 8, 'lr': 0.00041907086648071184, 'warmup_epochs': 12, 'temperature': 0.13315310787671011, 'noise_std': 0.12234414924687045, 'mask_prob': 0.08392001248555798, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  16%|█▌        | 16/100 [55:05<5:29:08, 235.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:36:53,139] Trial 15 finished with value: 0.5470705220705221 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 128, 'dropout': 0.12033465363120699, 'weight_decay': 5.455418424847078e-05, 'label_smoothing': 0.001151555641127901, 'grad_clip_norm': 0.516313811933251, 'batch_size': 8, 'lr': 0.0004950196683805461, 'warmup_epochs': 15, 'temperature': 0.13377870961689356, 'noise_std': 0.14929030093805118, 'mask_prob': 0.07328442640807727, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  17%|█▋        | 17/100 [57:29<4:47:18, 207.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:39:17,092] Trial 16 finished with value: 0.5202748885101827 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.23715935832352925, 'weight_decay': 5.902063280496525e-05, 'label_smoothing': 0.03456937452482666, 'grad_clip_norm': 1.5426764423517132, 'batch_size': 8, 'lr': 0.0004815317874598218, 'warmup_epochs': 12, 'temperature': 0.08121441685918773, 'noise_std': 0.1423955604426823, 'mask_prob': 0.053384309188877745, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  18%|█▊        | 18/100 [1:01:04<4:46:47, 209.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:42:51,954] Trial 17 finished with value: 0.5416931216931217 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.1447300967032409, 'weight_decay': 4.698088329030802e-05, 'label_smoothing': 0.07021924859446878, 'grad_clip_norm': 1.1215314993639969, 'batch_size': 8, 'lr': 0.0001994345485449423, 'warmup_epochs': 13, 'temperature': 0.16274201372236835, 'noise_std': 0.1241126030825291, 'mask_prob': 0.07999613987674298, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  19%|█▉        | 19/100 [1:03:51<4:26:00, 197.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:45:39,189] Trial 18 finished with value: 0.5255796763149705 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 1, 'fusion_dim': 256, 'dropout': 0.24720494901309636, 'weight_decay': 0.00012394010600890351, 'label_smoothing': 0.016047427643090934, 'grad_clip_norm': 1.1574425690213608, 'batch_size': 8, 'lr': 0.00030640123446682343, 'warmup_epochs': 10, 'temperature': 0.12033230466846362, 'noise_std': 0.17217870695549853, 'mask_prob': 0.05101141384277922, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  20%|██        | 20/100 [1:05:44<3:49:14, 171.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:47:32,552] Trial 19 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  21%|██        | 21/100 [1:08:32<3:44:27, 170.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:50:19,648] Trial 20 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  22%|██▏       | 22/100 [1:11:59<3:56:07, 181.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:53:47,306] Trial 21 finished with value: 0.5300176366843034 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.4213156930028955, 'weight_decay': 0.00855801059063899, 'label_smoothing': 0.06980966400054013, 'grad_clip_norm': 1.240195914304413, 'batch_size': 8, 'lr': 0.00029952831147398145, 'warmup_epochs': 11, 'temperature': 0.11311156273997826, 'noise_std': 0.08535306106831572, 'mask_prob': 0.16215487498064857, 'jitter': 4}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  23%|██▎       | 23/100 [1:14:30<3:41:06, 172.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:56:17,810] Trial 22 finished with value: 0.5203181785534726 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.28338267639206216, 'weight_decay': 0.0008612298485045519, 'label_smoothing': 0.06040346341022424, 'grad_clip_norm': 1.2349206593346485, 'batch_size': 32, 'lr': 0.00039987550927044515, 'warmup_epochs': 12, 'temperature': 0.09686895424240177, 'noise_std': 0.13138112699679086, 'mask_prob': 0.09404130334943556, 'jitter': 4}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  24%|██▍       | 24/100 [1:17:15<3:35:41, 170.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 22:59:03,415] Trial 23 finished with value: 0.5315855424295323 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.19379187341470347, 'weight_decay': 2.7573806623740686e-05, 'label_smoothing': 0.04154580980764175, 'grad_clip_norm': 1.5422549282682705, 'batch_size': 16, 'lr': 0.00022184361901306492, 'warmup_epochs': 10, 'temperature': 0.1355939943910166, 'noise_std': 0.08816344599512897, 'mask_prob': 0.16143837975714065, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  25%|██▌       | 25/100 [1:20:36<3:44:07, 179.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:02:23,763] Trial 24 finished with value: 0.5358649991983325 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.276097592064039, 'weight_decay': 0.00011281032518470341, 'label_smoothing': 0.022731375258208154, 'grad_clip_norm': 0.8625940993764027, 'batch_size': 8, 'lr': 0.00036191215476585653, 'warmup_epochs': 13, 'temperature': 0.08494399814016797, 'noise_std': 0.16985683732703188, 'mask_prob': 0.06373600275400489, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  26%|██▌       | 26/100 [1:24:59<4:12:07, 204.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:06:46,782] Trial 25 finished with value: 0.5318593135259801 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.3298887557239407, 'weight_decay': 0.008399166209714733, 'label_smoothing': 0.06083718726421279, 'grad_clip_norm': 1.4173557085982544, 'batch_size': 32, 'lr': 0.00024507916059991064, 'warmup_epochs': 8, 'temperature': 0.14979912323106043, 'noise_std': 0.007803022053485063, 'mask_prob': 0.03794101726549515, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  27%|██▋       | 27/100 [1:28:41<4:15:22, 209.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:10:29,464] Trial 26 finished with value: 0.5331657848324515 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.42434383302252376, 'weight_decay': 2.084736203740847e-05, 'label_smoothing': 0.010946293944052993, 'grad_clip_norm': 1.1058440969135757, 'batch_size': 8, 'lr': 0.00012385122382375339, 'warmup_epochs': 11, 'temperature': 0.12699786798163518, 'noise_std': 0.11364835194553259, 'mask_prob': 0.10211659004377964, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  28%|██▊       | 28/100 [1:30:29<3:35:11, 179.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:12:17,439] Trial 27 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  29%|██▉       | 29/100 [1:32:22<3:08:39, 159.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:14:10,472] Trial 28 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  30%|███       | 30/100 [1:34:59<3:05:02, 158.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:16:47,166] Trial 29 finished with value: 0.5534740545163661 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.49833587255360284, 'weight_decay': 1.0041227522604653e-05, 'label_smoothing': 0.058706238447490254, 'grad_clip_norm': 1.8531492729425925, 'batch_size': 32, 'lr': 0.0004094646829497481, 'warmup_epochs': 9, 'temperature': 0.11423568963366451, 'noise_std': 0.1539108146803626, 'mask_prob': 0.11211045858431785, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  30%|███       | 30/100 [1:37:29<3:05:02, 158.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:19:16,889] Trial 30 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  31%|███       | 31/100 [1:37:29<2:59:20, 155.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  32%|███▏      | 32/100 [1:40:18<3:01:12, 159.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:22:05,953] Trial 31 finished with value: 0.5102388969055636 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.493232518487022, 'weight_decay': 1.0092884918568377e-05, 'label_smoothing': 0.05937978347128285, 'grad_clip_norm': 1.9409873572727059, 'batch_size': 32, 'lr': 0.0003990319792113139, 'warmup_epochs': 9, 'temperature': 0.11737777040544278, 'noise_std': 0.15913296288730389, 'mask_prob': 0.11792924553820872, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  33%|███▎      | 33/100 [1:42:26<2:47:58, 150.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:24:14,334] Trial 32 finished with value: 0.5320391119410728 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.4155564191589497, 'weight_decay': 1.342527063944103e-05, 'label_smoothing': 0.05886262985250757, 'grad_clip_norm': 1.833875491661162, 'batch_size': 32, 'lr': 0.00028506386399570317, 'warmup_epochs': 8, 'temperature': 0.10896472965634718, 'noise_std': 0.10250057642577144, 'mask_prob': 0.09159392433917136, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  33%|███▎      | 33/100 [1:45:58<2:47:58, 150.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:27:46,586] Trial 33 finished with value: 0.5316049382716049 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 1, 'fusion_dim': 128, 'dropout': 0.49816551282808275, 'weight_decay': 2.9320139610702495e-05, 'label_smoothing': 0.05456866058439854, 'grad_clip_norm': 1.4760013539074772, 'batch_size': 32, 'lr': 0.0004282963479786794, 'warmup_epochs': 12, 'temperature': 0.13059024761926039, 'noise_std': 0.18578289321603517, 'mask_prob': 0.14224471656749488, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  34%|███▍      | 34/100 [1:45:58<3:05:52, 168.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  34%|███▍      | 34/100 [1:48:48<3:05:52, 168.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:30:35,657] Trial 34 finished with value: 0.5638007054673722 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 128, 'dropout': 0.3915938084449787, 'weight_decay': 0.001261361149236636, 'label_smoothing': 0.008578713140243416, 'grad_clip_norm': 1.8180946231714583, 'batch_size': 32, 'lr': 0.00035266829557304785, 'warmup_epochs': 7, 'temperature': 0.1021106050993984, 'noise_std': 0.16129770813404312, 'mask_prob': 0.10904534289991677, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  35%|███▌      | 35/100 [1:48:48<3:03:05, 169.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  35%|███▌      | 35/100 [1:51:32<3:03:05, 169.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:33:20,009] Trial 35 finished with value: 0.5700232061635571 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.37919606714469456, 'weight_decay': 0.001271755232289605, 'label_smoothing': 0.007671905644722428, 'grad_clip_norm': 0.6789785884026921, 'batch_size': 32, 'lr': 0.0002606381617442105, 'warmup_epochs': 7, 'temperature': 0.07278733425136487, 'noise_std': 0.07154082865207743, 'mask_prob': 0.004693231081162277, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  36%|███▌      | 36/100 [1:51:32<2:58:47, 167.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  36%|███▌      | 36/100 [1:55:44<2:58:47, 167.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:37:31,895] Trial 36 finished with value: 0.5658473625140292 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.40278064790559054, 'weight_decay': 0.0009840075716925666, 'label_smoothing': 0.008064702216195386, 'grad_clip_norm': 0.6430703214903457, 'batch_size': 32, 'lr': 0.0001753025563502927, 'warmup_epochs': 7, 'temperature': 0.06961864071816912, 'noise_std': 0.13509585740857605, 'mask_prob': 0.005566635241909174, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  37%|███▋      | 37/100 [1:55:44<3:22:32, 192.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  37%|███▋      | 37/100 [2:00:08<3:22:32, 192.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:41:56,360] Trial 37 finished with value: 0.5477384960718295 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.46184551690512965, 'weight_decay': 0.00045838266169208347, 'label_smoothing': 0.01592663008842142, 'grad_clip_norm': 0.6313844410954206, 'batch_size': 8, 'lr': 0.00016828690470899253, 'warmup_epochs': 7, 'temperature': 0.06516099644952655, 'noise_std': 0.09822214504479954, 'mask_prob': 0.0014442997151568851, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  38%|███▊      | 38/100 [2:00:08<3:41:30, 214.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  38%|███▊      | 38/100 [2:04:44<3:41:30, 214.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:46:32,154] Trial 38 finished with value: 0.5427777777777778 and parameters: {'d_model': 128, 'n_heads': 8, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3835543791122773, 'weight_decay': 0.0012437502576649446, 'label_smoothing': 0.006814848447030723, 'grad_clip_norm': 0.6493051768674112, 'batch_size': 16, 'lr': 0.00012994342659856503, 'warmup_epochs': 5, 'temperature': 0.05874530775846595, 'noise_std': 0.13087797425198847, 'mask_prob': 0.016752127147749353, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  39%|███▉      | 39/100 [2:04:44<3:56:40, 232.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  39%|███▉      | 39/100 [2:06:37<3:56:40, 232.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:48:24,884] Trial 39 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  40%|████      | 40/100 [2:06:37<3:16:46, 196.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  40%|████      | 40/100 [2:10:59<3:16:46, 196.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:52:46,856] Trial 40 finished with value: 0.5626063952150908 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.4064349059123773, 'weight_decay': 0.0002039156271163522, 'label_smoothing': 0.0048761450742007394, 'grad_clip_norm': 0.762897691790038, 'batch_size': 32, 'lr': 6.0122816512101934e-05, 'warmup_epochs': 6, 'temperature': 0.058282320281537084, 'noise_std': 0.14049946907752583, 'mask_prob': 0.04373202612926256, 'jitter': 4}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  41%|████      | 41/100 [2:10:59<3:32:43, 216.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  41%|████      | 41/100 [2:13:55<3:32:43, 216.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:55:43,597] Trial 41 finished with value: 0.5311201761201761 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.4465293130426559, 'weight_decay': 0.0013174521842633004, 'label_smoothing': 0.011129129672022592, 'grad_clip_norm': 0.7008809882243399, 'batch_size': 32, 'lr': 0.00024564249253955034, 'warmup_epochs': 7, 'temperature': 0.07379369714397188, 'noise_std': 0.16314121003315157, 'mask_prob': 0.002759123421777754, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  42%|████▏     | 42/100 [2:13:55<3:17:38, 204.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  43%|████▎     | 43/100 [2:16:59<3:08:18, 198.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-14 23:58:47,256] Trial 42 finished with value: 0.539304152637486 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3762754016325732, 'weight_decay': 0.0004946007441005523, 'label_smoothing': 0.011361739655777608, 'grad_clip_norm': 0.8694452824063542, 'batch_size': 32, 'lr': 0.00020021905314460193, 'warmup_epochs': 8, 'temperature': 0.05210697586703822, 'noise_std': 0.18372139386274167, 'mask_prob': 0.019175063826506866, 'jitter': 1}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  43%|████▎     | 43/100 [2:19:47<3:08:18, 198.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:01:35,724] Trial 43 finished with value: 0.5285470484490092 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.39590197092547924, 'weight_decay': 0.0009927264122581078, 'label_smoothing': 0.0177328294027517, 'grad_clip_norm': 0.6006192409907012, 'batch_size': 32, 'lr': 0.00035191865018520944, 'warmup_epochs': 7, 'temperature': 0.09090922746763068, 'noise_std': 0.14175534695029662, 'mask_prob': 0.027830533245986683, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  44%|████▍     | 44/100 [2:19:48<2:56:40, 189.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  45%|████▌     | 45/100 [2:23:01<2:54:40, 190.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:04:49,258] Trial 44 finished with value: 0.5630836139169472 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3381693154177482, 'weight_decay': 0.0016068177969678905, 'label_smoothing': 0.0051406464359029375, 'grad_clip_norm': 1.022944203495627, 'batch_size': 32, 'lr': 0.00016257240839069544, 'warmup_epochs': 6, 'temperature': 0.07432963905605441, 'noise_std': 0.10830982456259061, 'mask_prob': 0.08590341596849055, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  46%|████▌     | 46/100 [2:25:02<2:32:46, 169.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:06:50,447] Trial 45 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  47%|████▋     | 47/100 [2:27:19<2:21:07, 159.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:09:06,945] Trial 46 finished with value: 0.5104579987913321 and parameters: {'d_model': 256, 'n_heads': 2, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.43970594105850236, 'weight_decay': 0.000607304965100166, 'label_smoothing': 0.019959550743934205, 'grad_clip_norm': 1.9954437042351767, 'batch_size': 16, 'lr': 0.00044713437366810743, 'warmup_epochs': 6, 'temperature': 0.10288960960721331, 'noise_std': 0.06516033626831375, 'mask_prob': 0.010272616217958294, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  47%|████▋     | 47/100 [2:30:04<2:21:07, 159.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:11:51,928] Trial 47 finished with value: 0.5457002766423056 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.32435259279332623, 'weight_decay': 1.8073405750029056e-05, 'label_smoothing': 0.09965344069268112, 'grad_clip_norm': 0.7052823116879536, 'batch_size': 8, 'lr': 0.00033805193102360074, 'warmup_epochs': 8, 'temperature': 0.08905079747917713, 'noise_std': 0.03783085031647454, 'mask_prob': 0.04726975169894775, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  48%|████▊     | 48/100 [2:30:04<2:19:49, 161.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  49%|████▉     | 49/100 [2:32:03<2:06:29, 148.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:13:51,556] Trial 48 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  49%|████▉     | 49/100 [2:33:57<2:06:29, 148.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:15:45,145] Trial 49 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  50%|█████     | 50/100 [2:33:57<1:55:12, 138.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  51%|█████     | 51/100 [2:37:26<2:10:15, 159.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:19:14,249] Trial 50 finished with value: 0.5314373475776984 and parameters: {'d_model': 256, 'n_heads': 2, 'n_layers': 3, 'fusion_dim': 256, 'dropout': 0.35684001940197224, 'weight_decay': 0.003143727247355676, 'label_smoothing': 0.01378062341867476, 'grad_clip_norm': 0.9088114479303832, 'batch_size': 32, 'lr': 0.0004965747094868234, 'warmup_epochs': 5, 'temperature': 0.1441805377841077, 'noise_std': 0.15101943174216279, 'mask_prob': 0.03753322647075045, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  52%|█████▏    | 52/100 [2:39:11<1:54:26, 143.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:20:58,897] Trial 51 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  53%|█████▎    | 53/100 [2:40:56<1:43:11, 131.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:22:44,217] Trial 52 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  54%|█████▍    | 54/100 [2:44:51<2:04:41, 162.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:26:38,998] Trial 53 finished with value: 0.5723418121457337 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3360198781796106, 'weight_decay': 0.0009622849755364431, 'label_smoothing': 0.009002983229825314, 'grad_clip_norm': 0.9364847296183094, 'batch_size': 32, 'lr': 0.00015025547502463198, 'warmup_epochs': 7, 'temperature': 0.07633055027302116, 'noise_std': 0.1090217308922487, 'mask_prob': 0.1256854550824144, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  55%|█████▌    | 55/100 [2:46:31<1:47:57, 143.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:28:19,309] Trial 54 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  56%|█████▌    | 56/100 [2:48:17<1:37:08, 132.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:30:04,963] Trial 55 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  57%|█████▋    | 57/100 [2:51:20<1:45:45, 147.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:33:07,773] Trial 56 finished with value: 0.5238007054673721 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.31893320730058144, 'weight_decay': 0.0006604523683589959, 'label_smoothing': 0.008857187964720263, 'grad_clip_norm': 0.6610521711562374, 'batch_size': 8, 'lr': 0.00021920869587313402, 'warmup_epochs': 8, 'temperature': 0.09584533774938046, 'noise_std': 0.1806226658552806, 'mask_prob': 0.12329438803788263, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  58%|█████▊    | 58/100 [2:53:06<1:34:42, 135.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:34:54,442] Trial 57 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  59%|█████▉    | 59/100 [2:55:40<1:36:13, 140.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:37:28,122] Trial 58 finished with value: 0.5234617744038034 and parameters: {'d_model': 256, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.3887040974840062, 'weight_decay': 1.559343861246598e-05, 'label_smoothing': 0.014135179450181307, 'grad_clip_norm': 1.6696235540761706, 'batch_size': 8, 'lr': 0.0003697710619715144, 'warmup_epochs': 14, 'temperature': 0.1727813906213071, 'noise_std': 0.1323662517064758, 'mask_prob': 0.07820383897973904, 'jitter': 0}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  60%|██████    | 60/100 [2:59:05<1:46:39, 160.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:40:52,884] Trial 59 finished with value: 0.5458221807241415 and parameters: {'d_model': 128, 'n_heads': 4, 'n_layers': 3, 'fusion_dim': 128, 'dropout': 0.43987212893136246, 'weight_decay': 0.00043971351763282283, 'label_smoothing': 0.0031563495795968533, 'grad_clip_norm': 1.079510292699057, 'batch_size': 16, 'lr': 0.0001491542480924514, 'warmup_epochs': 9, 'temperature': 0.06767265343259518, 'noise_std': 0.09936680422859259, 'mask_prob': 0.1027886400175175, 'jitter': 2}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  61%|██████    | 61/100 [3:00:48<1:32:58, 143.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:42:36,369] Trial 60 finished with value: 0.5294781852134793 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.13466741093197393, 'weight_decay': 2.330853721911466e-05, 'label_smoothing': 0.02443388811946955, 'grad_clip_norm': 1.1983467507303482, 'batch_size': 8, 'lr': 0.0002687653726717407, 'warmup_epochs': 6, 'temperature': 0.1021627593946521, 'noise_std': 0.10618898279946647, 'mask_prob': 0.0970527031137983, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  62%|██████▏   | 62/100 [3:04:29<1:45:24, 166.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:46:17,395] Trial 61 finished with value: 0.5661956870290203 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.33614391562651713, 'weight_decay': 0.0015470102880145453, 'label_smoothing': 0.005921821234436757, 'grad_clip_norm': 0.9451852923102726, 'batch_size': 32, 'lr': 0.00018206217372601608, 'warmup_epochs': 6, 'temperature': 0.07639890223182975, 'noise_std': 0.11211908381386287, 'mask_prob': 0.08544492060579362, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  63%|██████▎   | 63/100 [3:06:15<1:31:27, 148.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:48:03,386] Trial 62 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  64%|██████▍   | 64/100 [3:08:43<1:28:52, 148.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:50:31,128] Trial 63 finished with value: 0.5371172037838704 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.29617932975795586, 'weight_decay': 0.0026080520456896648, 'label_smoothing': 0.01459976412888812, 'grad_clip_norm': 0.7052990452535293, 'batch_size': 32, 'lr': 0.00033177348766200257, 'warmup_epochs': 7, 'temperature': 0.06387104248172408, 'noise_std': 0.09129864101394453, 'mask_prob': 0.11069356412398262, 'jitter': 3}. Best is trial 4 with value: 0.5811022927689594.\n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  65%|██████▌   | 65/100 [3:10:24<1:18:13, 134.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:52:12,503] Trial 64 pruned. \n",
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  65%|██████▌   | 65/100 [3:12:26<1:18:13, 134.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 00:54:14,085] Trial 65 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  66%|██████▌   | 66/100 [3:12:26<1:13:52, 130.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[Dataset] Cached 53 scenarios in memory\n",
            "[Dataset] Cached 45 scenarios in memory\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  66%|██████▌   | 66/100 [3:16:03<1:13:52, 130.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[W 2025-12-15 00:57:50,921] Trial 66 failed with parameters: {'d_model': 256, 'n_heads': 8, 'n_layers': 2, 'fusion_dim': 256, 'dropout': 0.409307297811371, 'weight_decay': 0.0017725276229324008, 'label_smoothing': 0.01781246010867884, 'grad_clip_norm': 0.7975406800204813, 'batch_size': 32, 'lr': 0.0003032311333761149, 'warmup_epochs': 7, 'temperature': 0.12563714828155803, 'noise_std': 0.14722279099708685, 'mask_prob': 0.08132007347742223, 'jitter': 0} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_14370/1721322844.py\", line 44, in objective\n",
            "    train_epoch(model, train_loader, optimizer, config)\n",
            "  File \"/tmp/ipykernel_14370/3145803616.py\", line 8, in train_epoch\n",
            "    logits = model(metrics, metrics_mask, batch['log_texts'],\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_14370/2619661231.py\", line 147, in forward\n",
            "    log_emb = self.log_encoder(log_texts, services)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_14370/2619661231.py\", line 50, in forward\n",
            "    embeddings = self.sentence_model.encode(all_texts, convert_to_tensor=True)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py\", line 1090, in encode\n",
            "    features = batch_to_device(features, device)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/sentence_transformers/util/tensor.py\", line 184, in batch_to_device\n",
            "    batch[key] = batch[key].to(target_device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-12-15 00:57:50,930] Trial 66 failed with value None.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Best trial: 4. Best value: 0.581102:  66%|██████▌   | 66/100 [3:16:03<1:40:59, 178.23s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Balanced: 100 trials for ~8 hours total\u001b[39;00m\n\u001b[32m      2\u001b[39m N_TRIALS = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study.trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m trials\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pg \u001b[38;5;129;01min\u001b[39;00m optimizer.param_groups:\n\u001b[32m     42\u001b[39m         pg[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m] = lr\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m results = evaluate(model, val_loader)\n\u001b[32m     46\u001b[39m mrr = results[\u001b[33m'\u001b[39m\u001b[33mmrr\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, config)\u001b[39m\n\u001b[32m      6\u001b[39m metrics_mask = batch[\u001b[33m'\u001b[39m\u001b[33mmetrics_mask\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)\n\u001b[32m      7\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlog_texts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m              \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpod_to_service_idx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mservices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m loss = F.cross_entropy(logits, labels, label_smoothing=config.get(\u001b[33m'\u001b[39m\u001b[33mlabel_smoothing\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m))\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mMultimodalRCA.forward\u001b[39m\u001b[34m(self, metrics, metrics_mask, log_texts, pod_to_service_idx, services)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, metrics, metrics_mask, log_texts, pod_to_service_idx, services):\n\u001b[32m    146\u001b[39m     metric_emb = \u001b[38;5;28mself\u001b[39m.metric_encoder(metrics, metrics_mask)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     log_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     fused = \u001b[38;5;28mself\u001b[39m.fusion(metric_emb, log_emb, pod_to_service_idx, services)\n\u001b[32m    149\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(fused, services)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mLogEncoder.forward\u001b[39m\u001b[34m(self, log_texts_batch, services_batch)\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentence_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Clone to allow gradients through proj (sentence_model is frozen)\u001b[39;00m\n\u001b[32m     52\u001b[39m embeddings = embeddings.clone().detach().requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1090\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m   1082\u001b[39m             features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m   1083\u001b[39m                 (\n\u001b[32m   1084\u001b[39m                     features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1087\u001b[39m                 -\u001b[32m1\u001b[39m,\n\u001b[32m   1088\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/sentence_transformers/util/tensor.py:184\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Celda para analizar importancia\n",
        "import optuna.importance\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ANÁLISIS DE IMPORTANCIA DE PARÁMETROS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Obtener importancias\n",
        "importances = optuna.importance.get_param_importances(study)\n",
        "\n",
        "print(\"\\nImportancia de cada parámetro (mayor = más impacto en MRR):\\n\")\n",
        "for param, importance in sorted(importances.items(), key=lambda x: x[1], reverse=True):\n",
        "    bar = \"█\" * int(importance * 50)\n",
        "    print(f\"{param:20s} {importance:.4f} {bar}\")\n",
        "\n",
        "# Ver parámetros del mejor trial\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MEJOR TRIAL (#4)\")\n",
        "print(\"=\"*70)\n",
        "best_trial = study.best_trial\n",
        "print(f\"\\nMRR: {best_trial.value:.4f}\")\n",
        "print(\"\\nParámetros:\")\n",
        "for k, v in best_trial.params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Guardar para LODOCV\n",
        "with open('/root/lemm/best_hyperparams.json', 'w') as f:\n",
        "    json.dump(best_trial.params, f, indent=2)\n",
        "print(\"\\n✓ Guardado en /root/lemm/best_hyperparams.json\")\n",
        "\n",
        "# Ver distribución de los mejores trials\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOP 10 TRIALS\")\n",
        "print(\"=\"*70)\n",
        "trials_df = study.trials_dataframe()\n",
        "top10 = trials_df.nsmallest(10, 'value') if 'value' in trials_df else trials_df.head(10)\n",
        "print(top10[['number', 'value', 'params_d_model', 'params_n_layers', 'params_dropout']].to_string())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ANÁLISIS DE IMPORTANCIA DE PARÁMETROS\n",
            "======================================================================\n",
            "\n",
            "Importancia de cada parámetro (mayor = más impacto en MRR):\n",
            "\n",
            "lr                   0.3165 ███████████████\n",
            "label_smoothing      0.2782 █████████████\n",
            "grad_clip_norm       0.1610 ████████\n",
            "n_heads              0.0544 ██\n",
            "temperature          0.0512 ██\n",
            "noise_std            0.0297 █\n",
            "dropout              0.0293 █\n",
            "jitter               0.0186 \n",
            "warmup_epochs        0.0179 \n",
            "mask_prob            0.0161 \n",
            "batch_size           0.0108 \n",
            "fusion_dim           0.0061 \n",
            "d_model              0.0048 \n",
            "n_layers             0.0033 \n",
            "weight_decay         0.0021 \n",
            "\n",
            "======================================================================\n",
            "MEJOR TRIAL (#4)\n",
            "======================================================================\n",
            "\n",
            "MRR: 0.5811\n",
            "\n",
            "Parámetros:\n",
            "  d_model: 64\n",
            "  n_heads: 4\n",
            "  n_layers: 3\n",
            "  fusion_dim: 256\n",
            "  dropout: 0.27101640734341986\n",
            "  weight_decay: 1.1919481947918725e-05\n",
            "  label_smoothing: 0.010789142699330446\n",
            "  grad_clip_norm: 0.5471437785301014\n",
            "  batch_size: 8\n",
            "  lr: 0.00034827974366176894\n",
            "  warmup_epochs: 6\n",
            "  temperature: 0.11155743845534447\n",
            "  noise_std: 0.15111022770860974\n",
            "  mask_prob: 0.045759633098324495\n",
            "  jitter: 0\n",
            "\n",
            "✓ Guardado en /root/lemm/best_hyperparams.json\n",
            "\n",
            "======================================================================\n",
            "TOP 10 TRIALS\n",
            "======================================================================\n",
            "    number     value  params_d_model  params_n_layers  params_dropout\n",
            "19      19  0.375806              64                3        0.101998\n",
            "57      57  0.406251              64                2        0.232112\n",
            "49      49  0.411786              64                2        0.400665\n",
            "39      39  0.452606             128                2        0.285281\n",
            "52      52  0.455718              64                2        0.302750\n",
            "51      51  0.462797              64                2        0.343348\n",
            "28      28  0.463461             128                2        0.219009\n",
            "2        2  0.465699             128                3        0.457931\n",
            "54      54  0.471912              64                2        0.375398\n",
            "1        1  0.478892             256                2        0.168210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('='*70)\n",
        "print('BEST TRIAL')\n",
        "print('='*70)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "print(f'\\nBest MRR: {best_trial.value:.4f}')\n",
        "print('\\nBest hyperparameters:')\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f'  {key}: {value}')\n",
        "\n",
        "pruned = len([t for t in study.trials if t.state == TrialState.PRUNED])\n",
        "complete = len([t for t in study.trials if t.state == TrialState.COMPLETE])\n",
        "print(f'\\nTrial statistics: Complete={complete}, Pruned={pruned}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_config = {**FIXED_CONFIG, **best_trial.params}\n",
        "with open('/root/lemm/best_hyperparams.json', 'w') as f:\n",
        "    json.dump(best_config, f, indent=2)\n",
        "print('Saved to /root/lemm/best_hyperparams.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Second Round: Focused Search\n",
        "\n",
        "Based on importance analysis:\n",
        "- **Explore deeply**: lr, label_smoothing, grad_clip_norm, d_model, n_layers, n_heads, temperature\n",
        "- **Fix**: dropout, noise_std, jitter, warmup_epochs, mask_prob, batch_size, fusion_dim, weight_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Second round objective with fixed low-importance params\n",
        "def objective_v2(trial):\n",
        "    \"\"\"Focused objective: fix low-importance params, explore important ones.\"\"\"\n",
        "    \n",
        "    # FIXED PARAMS (from trial #4, <3% importance)\n",
        "    dropout = 0.27\n",
        "    noise_std = 0.15\n",
        "    jitter = 0\n",
        "    warmup_epochs = 6\n",
        "    mask_prob = 0.05\n",
        "    batch_size = 8\n",
        "    fusion_dim = 256\n",
        "    weight_decay = 1e-5\n",
        "    \n",
        "    # EXPLORE: High importance params (fine-grained)\n",
        "    lr = trial.suggest_float('lr', 1e-4, 5e-4, step=2e-5)  # 21 values\n",
        "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.05, step=0.005)  # 11 values\n",
        "    grad_clip_norm = trial.suggest_float('grad_clip_norm', 0.3, 1.0, step=0.1)  # 8 values\n",
        "    \n",
        "    # EXPLORE: Architecture (we don't know if low importance is real)\n",
        "    d_model = trial.suggest_categorical('d_model', [32, 64])\n",
        "    n_layers = trial.suggest_int('n_layers', 3)\n",
        "    n_heads = trial.suggest_categorical('n_heads', [4])\n",
        "    \n",
        "    # EXPLORE: Medium importance\n",
        "    temperature = trial.suggest_float('temperature', 0.05, 0.20, step=0.025)  # 7 values\n",
        "    \n",
        "    config = {\n",
        "        **FIXED_CONFIG,\n",
        "        'd_model': d_model,\n",
        "        'n_heads': n_heads,\n",
        "        'n_layers': n_layers,\n",
        "        'fusion_dim': fusion_dim,\n",
        "        'dropout': dropout,\n",
        "        'weight_decay': weight_decay,\n",
        "        'label_smoothing': label_smoothing,\n",
        "        'grad_clip_norm': grad_clip_norm,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': lr,\n",
        "        'warmup_epochs': warmup_epochs,\n",
        "        'temperature': temperature,\n",
        "        'noise_std': noise_std,\n",
        "        'mask_prob': mask_prob,\n",
        "        'jitter': jitter,\n",
        "    }\n",
        "    \n",
        "    # Create datasets\n",
        "    train_scenarios, val_scenarios = [], []\n",
        "    for s in discover_scenarios():\n",
        "        if s not in EXCLUDED:\n",
        "            if get_split(s) == 'train':\n",
        "                train_scenarios.append(s)\n",
        "            else:\n",
        "                val_scenarios.append(s)\n",
        "    \n",
        "    train_ds = RCADataset(train_scenarios, config, mode='train')\n",
        "    val_ds = RCADataset(val_scenarios, config, mode='val')\n",
        "    \n",
        "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], \n",
        "                             shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config['batch_size'],\n",
        "                           shuffle=False, collate_fn=collate_fn)\n",
        "    \n",
        "    # Create model\n",
        "    model = MultimodalRCA(config).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                                  lr=config['learning_rate'],\n",
        "                                  weight_decay=config['weight_decay'])\n",
        "    \n",
        "    # Training with pruning\n",
        "    best_mrr = 0.0\n",
        "    for epoch in range(config['epochs']):\n",
        "        train_epoch(model, train_loader, optimizer, config)\n",
        "        metrics = evaluate(model, val_loader)\n",
        "        mrr = metrics['mrr']\n",
        "        \n",
        "        if mrr > best_mrr:\n",
        "            best_mrr = mrr\n",
        "        \n",
        "        # Report for pruning\n",
        "        trial.report(mrr, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "    \n",
        "    return best_mrr\n",
        "\n",
        "print(\"Second round objective defined\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Second round objective defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create new study for second round\n",
        "study_v2 = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(\n",
        "        seed=42,\n",
        "        n_startup_trials=10,  # 10 random trials first\n",
        "    ),\n",
        "    pruner=optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=10,\n",
        "        n_warmup_steps=12,\n",
        "        interval_steps=1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Seed with best trial from round 1\n",
        "study_v2.enqueue_trial({\n",
        "    'lr': 0.00035,  # Close to trial #4\n",
        "    'label_smoothing': 0.01,\n",
        "    'grad_clip_norm': 0.55,\n",
        "    'd_model': 64,\n",
        "    'n_layers': 3,\n",
        "    'n_heads': 4,\n",
        "    'temperature': 0.11,\n",
        "})\n",
        "\n",
        "print(\"Study v2 created, seeded with trial #4 values\")\n",
        "print(\"Starting second round: 50 trials (~2-3 hours)\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2025-12-15 13:38:56,368] A new study created in memory with name: no-name-bdfcaa8e-5eda-4835-8764-0c375b2f19a7\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Study v2 created, seeded with trial #4 values\n",
            "Starting second round: 50 trials (~2-3 hours)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run second round - 50 trials focused search\n",
        "N_TRIALS_V2 = 50\n",
        "study_v2.optimize(objective_v2, n_trials=N_TRIALS_V2, show_progress_bar=True, gc_after_trial=True)\n",
        "print(f'\\nCompleted {len(study_v2.trials)} trials')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[W 2025-12-15 13:39:00,118] Trial 0 failed with parameters: {'lr': 0.00035, 'label_smoothing': 0.01, 'grad_clip_norm': 0.55, 'd_model': 64} because of the following error: TypeError(\"Trial.suggest_int() missing 1 required positional argument: 'high'\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_18622/4235438134.py\", line 22, in objective_v2\n",
            "    n_layers = trial.suggest_int('n_layers', 3)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/lemm/.venv/lib/python3.12/site-packages/optuna/_convert_positional_args.py\", line 135, in converter_wrapper\n",
            "    return func(**kwargs)  # type: ignore[call-arg]\n",
            "           ^^^^^^^^^^^^^^\n",
            "TypeError: Trial.suggest_int() missing 1 required positional argument: 'high'\n",
            "[W 2025-12-15 13:39:00,124] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Trial.suggest_int() missing 1 required positional argument: 'high'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run second round - 50 trials focused search\u001b[39;00m\n\u001b[32m      2\u001b[39m N_TRIALS_V2 = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mstudy_v2\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_v2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS_V2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study_v2.trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m trials\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mobjective_v2\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# EXPLORE: Architecture (we don't know if low importance is real)\u001b[39;00m\n\u001b[32m     21\u001b[39m d_model = trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33md_model\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m n_layers = \u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest_int\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_layers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m n_heads = trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33mn_heads\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m4\u001b[39m])\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# EXPLORE: Medium importance\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/lemm/.venv/lib/python3.12/site-packages/optuna/_convert_positional_args.py:135\u001b[39m, in \u001b[36mconvert_positional_args.<locals>.converter_decorator.<locals>.converter_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() got multiple values for arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_kwds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n\u001b[32m    133\u001b[39m kwargs.update(inferred_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: Trial.suggest_int() missing 1 required positional argument: 'high'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze second round results\n",
        "print(\"=\"*70)\n",
        "print(\"SECOND ROUND RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Importance analysis\n",
        "importances_v2 = optuna.importance.get_param_importances(study_v2)\n",
        "\n",
        "print(\"\\nImportancia de parámetros (segunda ronda):\\n\")\n",
        "for param, importance in sorted(importances_v2.items(), key=lambda x: x[1], reverse=True):\n",
        "    bar = \"█\" * int(importance * 50)\n",
        "    print(f\"{param:20s} {importance:.4f} {bar}\")\n",
        "\n",
        "# Best trial\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MEJOR TRIAL - SEGUNDA RONDA\")\n",
        "print(\"=\"*70)\n",
        "best_v2 = study_v2.best_trial\n",
        "print(f\"\\nMRR: {best_v2.value:.4f}\")\n",
        "print(f\"Mejor que ronda 1? {best_v2.value:.4f} vs 0.5811\")\n",
        "print(\"\\nParámetros:\")\n",
        "for k, v in best_v2.params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Compare with round 1\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARACIÓN\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Ronda 1 mejor: 0.5811 (trial #4)\")\n",
        "print(f\"Ronda 2 mejor: {best_v2.value:.4f}\")\n",
        "if best_v2.value > 0.5811:\n",
        "    print(\"✓ MEJORAMOS!\")\n",
        "else:\n",
        "    print(\"✗ No mejoramos, usar trial #4 de ronda 1\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SECOND ROUND RESULTS\n",
            "======================================================================\n",
            "\n",
            "Importancia de parámetros (segunda ronda):\n",
            "\n",
            "temperature          0.3299 ████████████████\n",
            "label_smoothing      0.1622 ████████\n",
            "grad_clip_norm       0.1514 ███████\n",
            "lr                   0.1449 ███████\n",
            "d_model              0.1016 █████\n",
            "n_heads              0.0605 ███\n",
            "n_layers             0.0496 ██\n",
            "\n",
            "======================================================================\n",
            "MEJOR TRIAL - SEGUNDA RONDA\n",
            "======================================================================\n",
            "\n",
            "MRR: 0.5788\n",
            "Mejor que ronda 1? 0.5788 vs 0.5811\n",
            "\n",
            "Parámetros:\n",
            "  lr: 0.00036\n",
            "  label_smoothing: 0.0\n",
            "  grad_clip_norm: 0.7\n",
            "  d_model: 64\n",
            "  n_layers: 3\n",
            "  n_heads: 2\n",
            "  temperature: 0.175\n",
            "\n",
            "======================================================================\n",
            "COMPARACIÓN\n",
            "======================================================================\n",
            "Ronda 1 mejor: 0.5811 (trial #4)\n",
            "Ronda 2 mejor: 0.5788\n",
            "✗ No mejoramos, usar trial #4 de ronda 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save best hyperparameters (from whichever round was better)\n",
        "if best_v2.value > 0.5811:\n",
        "    # Use round 2 params + fixed params\n",
        "    final_params = {\n",
        "        **best_v2.params,\n",
        "        # Add fixed params back\n",
        "        'dropout': 0.27,\n",
        "        'noise_std': 0.15,\n",
        "        'jitter': 0,\n",
        "        'warmup_epochs': 6,\n",
        "        'mask_prob': 0.05,\n",
        "        'batch_size': 8,\n",
        "        'fusion_dim': 256,\n",
        "        'weight_decay': 1e-5,\n",
        "    }\n",
        "    source = \"round 2\"\n",
        "else:\n",
        "    # Use round 1 params (trial #4)\n",
        "    final_params = study.best_trial.params\n",
        "    source = \"round 1 (trial #4)\"\n",
        "\n",
        "# Add fixed config\n",
        "final_config = {**FIXED_CONFIG, **final_params}\n",
        "\n",
        "with open('/root/lemm/best_hyperparams.json', 'w') as f:\n",
        "    json.dump(final_config, f, indent=2)\n",
        "\n",
        "print(f\"✓ Saved best hyperparameters from {source} to /root/lemm/best_hyperparams.json\")\n",
        "print(\"\\nFinal config for LODOCV:\")\n",
        "for k, v in final_config.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_v2' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save best hyperparameters (from whichever round was better)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_v2\u001b[49m.value > \u001b[32m0.5811\u001b[39m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Use round 2 params + fixed params\u001b[39;00m\n\u001b[32m      4\u001b[39m     final_params = {\n\u001b[32m      5\u001b[39m         **best_v2.params,\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# Add fixed params back\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-5\u001b[39m,\n\u001b[32m     15\u001b[39m     }\n\u001b[32m     16\u001b[39m     source = \u001b[33m\"\u001b[39m\u001b[33mround 2\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'best_v2' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}