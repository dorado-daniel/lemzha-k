{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LODOCV – Leave-One-Day-Out Cross-Validation\n",
        "\n",
        "4 folds, one per Nezha collection day (20220822, 20220823, 20230129, 20230130). Nezha only, no LEMMA — zero temporal overlap between train and test.\n",
        "\n",
        "Reports Acc@1, Acc@3, MRR (mean +/- std).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/lemm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === PATHS ===\n",
        "BASE_DIR = Path(\"/root/lemm\")\n",
        "MULTIMODAL_DIR = BASE_DIR / \"core_multimodal_tmp\"\n",
        "METRICS_DIR = BASE_DIR / \"core_metrics_tmp\"\n",
        "LOGS_DIR = BASE_DIR / \"core_logs_tmp\"\n",
        "\n",
        "# === HYPERPARAMETERS ===\n",
        "# Base config (n_metrics will be determined dynamically from data)\n",
        "CONFIG = {\n",
        "    \"window_size\": 22,\n",
        "    \"n_metrics\": None,  # Will be set dynamically from manifest.json\n",
        "    \"bin_seconds\": 30,\n",
        "    \"log_embed_dim\": 384,\n",
        "    \"d_model\": 128,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 2,\n",
        "    \"fusion_dim\": 256,\n",
        "    \"dropout\": 0.35,\n",
        "    \"batch_size\": 8,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"epochs\": 50,\n",
        "    \"warmup_epochs\": 10,\n",
        "    \"temperature\": 0.1,\n",
        "    \"label_smoothing\": 0.02,\n",
        "    \"grad_clip_norm\": 1.0,\n",
        "    \"jitter\": 3,\n",
        "    \"noise_std\": 0.1,\n",
        "    \"mask_prob\": 0.1,\n",
        "}\n",
        "\n",
        "# Load Optuna results if available and merge\n",
        "OPTUNA_RESULTS_PATH = BASE_DIR / \"best_hyperparams.json\"\n",
        "if OPTUNA_RESULTS_PATH.exists():\n",
        "    with open(OPTUNA_RESULTS_PATH) as f:\n",
        "        optuna_params = json.load(f)\n",
        "    # Handle 'lr' vs 'learning_rate' naming convention\n",
        "    if \"lr\" in optuna_params:\n",
        "        optuna_params[\"learning_rate\"] = optuna_params.pop(\"lr\")\n",
        "    # Don't override n_metrics from Optuna (we'll set it dynamically)\n",
        "    optuna_params.pop(\"n_metrics\", None)\n",
        "    CONFIG.update(optuna_params)\n",
        "    print(\"Loaded and merged hyperparameters from Optuna\")\n",
        "else:\n",
        "    print(\"Using default hyperparameters\")\n",
        "\n",
        "EXCLUDED_SCENARIOS = {\n",
        "    \"20220822_nezha_22\", \"20220822_nezha_23\",\n",
        "    \"20230130_nezha_15\", \"20230130_nezha_16\",\n",
        "    \"20220822_nezha_14\", \"20220823_nezha_21\", \"20220823_nezha_24\",\n",
        "}\n",
        "\n",
        "NEZHA_DAYS = [\"20220822\", \"20220823\", \"20230129\", \"20230130\"]\n",
        "\n",
        "print(f\"\\nConfig: d_model={CONFIG.get('d_model')}, n_layers={CONFIG.get('n_layers')}, dropout={CONFIG.get('dropout'):.3f}\")\n",
        "print(f\"        lr={CONFIG.get('learning_rate'):.6f}, temperature={CONFIG.get('temperature'):.3f}\")\n",
        "print(f\"        n_metrics: will be determined dynamically from data\")\n",
        "print(f\"Excluded scenarios: {len(EXCLUDED_SCENARIOS)}\")\n",
        "print(f\"LODOCV folds: {len(NEZHA_DAYS)} (one per Nezha day)\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded and merged hyperparameters from Optuna\n",
            "\n",
            "Config: d_model=64, n_layers=3, dropout=0.271\n",
            "        lr=0.000348, temperature=0.112\n",
            "        n_metrics: will be determined dynamically from data\n",
            "Excluded scenarios: 7\n",
            "LODOCV folds: 4 (one per Nezha day)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === SCENARIO DISCOVERY ===\n",
        "NEZHA_PATTERN = re.compile(r\"^\\d{8}_nezha_\\d+$\")\n",
        "\n",
        "def get_nezha_day(scenario_id: str) -> Optional[str]:\n",
        "    if \"_nezha_\" in scenario_id: return scenario_id.split(\"_nezha_\")[0]\n",
        "    return None\n",
        "\n",
        "def discover_scenarios() -> List[Dict]:\n",
        "    \"\"\"Discover only Nezha scenarios (LEMMA excluded).\"\"\"\n",
        "    scenarios = []\n",
        "    for d in sorted(MULTIMODAL_DIR.iterdir()):\n",
        "        if not d.is_dir() or d.name in EXCLUDED_SCENARIOS: continue\n",
        "        if not (d / \"manifest.json\").exists() or not (d / \"ground_truth.json\").exists(): continue\n",
        "        # Only include Nezha scenarios\n",
        "        if not NEZHA_PATTERN.match(d.name): continue\n",
        "        scenarios.append({\"id\": d.name, \"dataset\": \"nezha\", \"day\": get_nezha_day(d.name), \"path\": d})\n",
        "    return scenarios\n",
        "\n",
        "NEZHA_SCENARIOS = discover_scenarios()\n",
        "\n",
        "print(f\"Total Nezha scenarios: {len(NEZHA_SCENARIOS)}\")\n",
        "for day in NEZHA_DAYS:\n",
        "    count = len([s for s in NEZHA_SCENARIOS if s[\"day\"] == day])\n",
        "    print(f\"  {day}: {count} scenarios\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Nezha scenarios: 94\n",
            "  20220822: 21 scenarios\n",
            "  20220823: 30 scenarios\n",
            "  20230129: 28 scenarios\n",
            "  20230130: 15 scenarios\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === DATA LOADING ===\n",
        "# METRIC_NAMES will be determined dynamically from manifest.json\n",
        "# This allows us to use ALL available metrics, not just 7 core ones\n",
        "\n",
        "def load_scenario_data(scenario_id: str) -> Dict:\n",
        "    \"\"\"Load scenario data, including ALL available metrics from manifest.json.\"\"\"\n",
        "    multimodal_path = MULTIMODAL_DIR / scenario_id\n",
        "    metrics_path = METRICS_DIR / scenario_id\n",
        "    logs_path = LOGS_DIR / scenario_id\n",
        "    \n",
        "    with open(multimodal_path / \"manifest.json\") as f: \n",
        "        manifest = json.load(f)\n",
        "    with open(multimodal_path / \"ground_truth.json\") as f: \n",
        "        gt = json.load(f)\n",
        "    \n",
        "    # Load ALL metrics listed in manifest.json (not just 7 core ones)\n",
        "    metrics = {}\n",
        "    metric_files = manifest.get(\"metrics_files\", [])\n",
        "    \n",
        "    for metric_file_path in metric_files:\n",
        "        # Extract metric name from path: /path/to/pod_metric_name.parquet -> metric_name\n",
        "        metric_file = Path(metric_file_path).name\n",
        "        metric_name = metric_file.replace(\"pod_\", \"\").replace(\".parquet\", \"\")\n",
        "        \n",
        "        # Try to load from metrics_path\n",
        "        metric_path = metrics_path / metric_file\n",
        "        if metric_path.exists():\n",
        "            metrics[metric_name] = pd.read_parquet(metric_path)\n",
        "        else:\n",
        "            # If not found, try to construct from metric_name\n",
        "            metric_path = metrics_path / f\"pod_{metric_name}.parquet\"\n",
        "            if metric_path.exists():\n",
        "                metrics[metric_name] = pd.read_parquet(metric_path)\n",
        "    \n",
        "    logs_file = logs_path / \"logs_service_texts.parquet\"\n",
        "    logs = pd.read_parquet(logs_file) if logs_file.exists() else None\n",
        "    \n",
        "    return {\n",
        "        \"metrics\": metrics, \n",
        "        \"logs\": logs, \n",
        "        \"manifest\": manifest, \n",
        "        \"ground_truth\": gt,\n",
        "        \"pods\": manifest[\"pods\"], \n",
        "        \"services\": manifest[\"services\"], \n",
        "        \"pod_to_service_idx\": manifest[\"pod_to_service_idx\"],\n",
        "        \"n_metrics\": manifest.get(\"n_metrics\", len(metrics))  # Store n_metrics from manifest\n",
        "    }\n",
        "\n",
        "# Determine n_metrics from first scenario (all should have same number)\n",
        "if NEZHA_SCENARIOS:\n",
        "    first_scenario_data = load_scenario_data(NEZHA_SCENARIOS[0][\"id\"])\n",
        "    CONFIG[\"n_metrics\"] = first_scenario_data[\"n_metrics\"]\n",
        "    print(f\"\\nDetected {CONFIG['n_metrics']} metrics from scenario {NEZHA_SCENARIOS[0]['id']}\")\n",
        "    print(f\"Available metrics: {sorted(first_scenario_data['metrics'].keys())}\")\n",
        "else:\n",
        "    print(\"WARNING: No Nezha scenarios found! n_metrics cannot be determined.\")\n",
        "\n",
        "print(\"\\nData loading functions defined\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Detected 19 metrics from scenario 20220822_nezha_0\n",
            "Available metrics: ['cpu_usage_m', 'cpu_usage_rate', 'latency_client_p90', 'latency_client_p95', 'latency_client_p99', 'latency_server_p90', 'latency_server_p95', 'latency_server_p99', 'memory_usage', 'memory_usage_rate', 'network_rx_bytes', 'network_tx_bytes', 'node_cpu_usage_rate', 'node_memory_usage_rate', 'node_network_rx_bytes', 'success_rate', 'syscall_read', 'syscall_write', 'workload_ops']\n",
            "\n",
            "Data loading functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === DATASET CLASS ===\n",
        "class RCAScenarioDataset(Dataset):\n",
        "    def __init__(self, scenario_ids, window_size=22, mode=\"train\", jitter=3, noise_std=0.0, mask_prob=0.0,\n",
        "                 norm_mode=\"per_pod\", log_token_mode=\"unique\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            norm_mode: \"global\" (normalize all pods together), \"per_pod\" (normalize each pod separately),\n",
        "                      \"hybrid\" (per pod + preserve relative magnitude), \"adaptive\" (detect structure)\n",
        "            log_token_mode: \"common\" (same token for all services without logs), \"unique\" (unique token per service)\n",
        "        \"\"\"\n",
        "        self.scenario_ids, self.window_size, self.mode = scenario_ids, window_size, mode\n",
        "        self.jitter = jitter if mode == \"train\" else 0\n",
        "        self.noise_std = noise_std if mode == \"train\" else 0.0\n",
        "        self.mask_prob = mask_prob if mode == \"train\" else 0.0\n",
        "        self.norm_mode = norm_mode\n",
        "        self.log_token_mode = log_token_mode\n",
        "        self.data = {sid: load_scenario_data(sid) for sid in scenario_ids}\n",
        "        self.all_services = sorted(set(s for d in self.data.values() for s in d[\"services\"]))\n",
        "        self.service_to_idx = {s: i for i, s in enumerate(self.all_services)}\n",
        "    \n",
        "    def __len__(self): return len(self.scenario_ids)\n",
        "    \n",
        "    def _crop_around_fault(self, n_timesteps, fault_idx):\n",
        "        if n_timesteps <= self.window_size: return 0, n_timesteps, fault_idx\n",
        "        half = self.window_size // 2\n",
        "        offset = random.randint(-self.jitter, self.jitter) if self.jitter > 0 else 0\n",
        "        start = fault_idx - half + offset\n",
        "        start = max(max(0, fault_idx - self.window_size + 1), min(min(fault_idx, n_timesteps - self.window_size), start))\n",
        "        return start, start + self.window_size, fault_idx - start\n",
        "    \n",
        "    def _get_metric_type(self, metric_name):\n",
        "        \"\"\"Classify metric by type for appropriate normalization.\n",
        "        \n",
        "        IMPORTANT: Order matters! Check more specific patterns first.\n",
        "        \"\"\"\n",
        "        name_lower = metric_name.lower()\n",
        "        \n",
        "        # CPU in milicores (check BEFORE 'usage' to avoid misclassification)\n",
        "        if 'cpu' in name_lower and ('m' in name_lower or '_m' in name_lower):\n",
        "            return 'milicores'\n",
        "        \n",
        "        # Latency metrics (log-normal distribution) - check BEFORE 'usage'\n",
        "        if any(x in name_lower for x in ['latency', 'delay']):\n",
        "            return 'latency'\n",
        "        \n",
        "        # Percentage metrics (0-100%) - must have 'rate' or be explicit percentage\n",
        "        if 'rate' in name_lower or 'success' in name_lower:\n",
        "            return 'percentage'\n",
        "        # Memory/CPU usage RATE (percentage)\n",
        "        if 'usage_rate' in name_lower or 'utilization' in name_lower:\n",
        "            return 'percentage'\n",
        "        \n",
        "        # Memory usage (MiB) - bytes but not percentage\n",
        "        if 'memory_usage' in name_lower and 'rate' not in name_lower:\n",
        "            return 'bytes'\n",
        "        \n",
        "        # Byte metrics (can be very large)\n",
        "        if any(x in name_lower for x in ['bytes', 'network']):\n",
        "            return 'bytes'\n",
        "        \n",
        "        # Count/ops metrics (may have many zeros, use log1p)\n",
        "        if any(x in name_lower for x in ['ops', 'count', 'workload', 'syscall']):\n",
        "            return 'count'\n",
        "        \n",
        "        # Default: z-score\n",
        "        return 'default'\n",
        "    \n",
        "    def _normalize_metrics(self, m, metric_names):\n",
        "        \"\"\"\n",
        "        Normalize metrics with type-aware normalization per metric.\n",
        "        Returns normalized metrics and 3D mask (time, pods, metrics).\n",
        "        \"\"\"\n",
        "        # Create 3D mask: (time, pods, metrics) - True if value is valid\n",
        "        valid_mask_3d = ~np.isnan(m)\n",
        "        \n",
        "        # Normalize each metric according to its type\n",
        "        for f, metric_name in enumerate(metric_names):\n",
        "            metric_type = self._get_metric_type(metric_name)\n",
        "            metric_data = m[:, :, f]\n",
        "            valid = valid_mask_3d[:, :, f]\n",
        "            \n",
        "            if valid.sum() == 0:\n",
        "                continue  # Skip if no valid values\n",
        "            \n",
        "            if self.norm_mode == \"per_pod\":\n",
        "                # Normalize each pod separately\n",
        "                for p in range(m.shape[1]):\n",
        "                    pod_data = metric_data[:, p]\n",
        "                    pod_valid = valid[:, p]\n",
        "                    if pod_valid.sum() > 0:\n",
        "                        if metric_type == 'percentage':\n",
        "                            # Min-Max to [0, 1] for percentages\n",
        "                            v = pod_data[pod_valid]\n",
        "                            v_min, v_max = v.min(), v.max()\n",
        "                            if v_max > v_min:\n",
        "                                metric_data[:, p] = (pod_data - v_min) / (v_max - v_min)\n",
        "                            else:\n",
        "                                metric_data[:, p] = pod_data - v_min\n",
        "                        elif metric_type == 'bytes':\n",
        "                            # Byte metrics - check if log is needed\n",
        "                            v = pod_data[pod_valid]\n",
        "                            # If values are very large (max/min > 100), use log\n",
        "                            # Otherwise, use z-score directly\n",
        "                            if len(v) > 0 and np.max(v) > 0:\n",
        "                                ratio = np.max(v) / (np.min(v[v > 0]) if np.any(v > 0) else 1)\n",
        "                                if ratio > 100:\n",
        "                                    # Large range - use log normalization\n",
        "                                    v_log = np.log1p(v)\n",
        "                                    mean, std = np.mean(v_log), np.std(v_log)\n",
        "                                    if std > 1e-8:\n",
        "                                        metric_data[:, p] = (np.log1p(pod_data) - mean) / std\n",
        "                                    else:\n",
        "                                        metric_data[:, p] = np.log1p(pod_data) - mean\n",
        "                                else:\n",
        "                                    # Small range - use z-score directly\n",
        "                                    mean, std = np.mean(v), np.std(v)\n",
        "                                    if std > 1e-8:\n",
        "                                        metric_data[:, p] = (pod_data - mean) / std\n",
        "                                    else:\n",
        "                                        metric_data[:, p] = pod_data - mean\n",
        "                        elif metric_type == 'latency':\n",
        "                            # Latency metrics - use log1p (handles zeros) + z-score\n",
        "                            # Many latency metrics have zeros (no requests in that period)\n",
        "                            v = pod_data[pod_valid]\n",
        "                            v_log = np.log1p(v)  # log1p handles zeros correctly\n",
        "                            mean, std = np.mean(v_log), np.std(v_log)\n",
        "                            if std > 1e-8:\n",
        "                                metric_data[:, p] = (np.log1p(pod_data) - mean) / std\n",
        "                            else:\n",
        "                                metric_data[:, p] = np.log1p(pod_data) - mean\n",
        "                        elif metric_type == 'count':\n",
        "                            # Count metrics may have many zeros - use log1p + z-score\n",
        "                            v = pod_data[pod_valid]\n",
        "                            # Check if there are many zeros (>10%)\n",
        "                            zeros_pct = 100 * np.sum(v == 0) / len(v) if len(v) > 0 else 0\n",
        "                            if zeros_pct > 10:\n",
        "                                # Use log1p for counts with many zeros\n",
        "                                v_log = np.log1p(v)\n",
        "                                mean, std = np.mean(v_log), np.std(v_log)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data[:, p] = (np.log1p(pod_data) - mean) / std\n",
        "                                else:\n",
        "                                    metric_data[:, p] = np.log1p(pod_data) - mean\n",
        "                            else:\n",
        "                                # Z-score for counts without many zeros\n",
        "                                mean, std = np.mean(v), np.std(v)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data[:, p] = (pod_data - mean) / std\n",
        "                                else:\n",
        "                                    metric_data[:, p] = pod_data - mean\n",
        "                        elif metric_type == 'milicores':\n",
        "                            # CPU in milicores - check if log-normal distribution\n",
        "                            # For now, use z-score (can be changed to log if needed)\n",
        "                            v = pod_data[pod_valid]\n",
        "                            mean, std = np.mean(v), np.std(v)\n",
        "                            if std > 1e-8:\n",
        "                                metric_data[:, p] = (pod_data - mean) / std\n",
        "                            else:\n",
        "                                metric_data[:, p] = pod_data - mean\n",
        "                        else:\n",
        "                            # Default: z-score\n",
        "                            v = pod_data[pod_valid]\n",
        "                            mean, std = np.mean(v), np.std(v)\n",
        "                            if std > 1e-8:\n",
        "                                metric_data[:, p] = (pod_data - mean) / std\n",
        "                            else:\n",
        "                                metric_data[:, p] = pod_data - mean\n",
        "            else:\n",
        "                # Global normalization (all pods together)\n",
        "                all_valid = metric_data[valid]\n",
        "                if len(all_valid) > 0:\n",
        "                    if metric_type == 'percentage':\n",
        "                        v_min, v_max = all_valid.min(), all_valid.max()\n",
        "                        if v_max > v_min:\n",
        "                            metric_data = (metric_data - v_min) / (v_max - v_min)\n",
        "                        else:\n",
        "                            metric_data = metric_data - v_min\n",
        "                    elif metric_type == 'bytes':\n",
        "                        # Bytes - check if log is needed\n",
        "                        if len(all_valid) > 0 and np.max(all_valid) > 0:\n",
        "                            ratio = np.max(all_valid) / (np.min(all_valid[all_valid > 0]) if np.any(all_valid > 0) else 1)\n",
        "                            if ratio > 100:\n",
        "                                v_log = np.log1p(all_valid)\n",
        "                                mean, std = np.mean(v_log), np.std(v_log)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data = (np.log1p(metric_data) - mean) / std\n",
        "                                else:\n",
        "                                    metric_data = np.log1p(metric_data) - mean\n",
        "                            else:\n",
        "                                mean, std = np.mean(all_valid), np.std(all_valid)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data = (metric_data - mean) / std\n",
        "                                else:\n",
        "                                    metric_data = metric_data - mean\n",
        "                    elif metric_type == 'latency':\n",
        "                        # Latency - use log1p + z-score\n",
        "                        v_log = np.log1p(all_valid)\n",
        "                        mean, std = np.mean(v_log), np.std(v_log)\n",
        "                        if std > 1e-8:\n",
        "                            metric_data = (np.log1p(metric_data) - mean) / std\n",
        "                        else:\n",
        "                            metric_data = np.log1p(metric_data) - mean\n",
        "                    elif metric_type == 'count':\n",
        "                        # Count metrics - check for zeros\n",
        "                        zeros_pct = 100 * np.sum(all_valid == 0) / len(all_valid) if len(all_valid) > 0 else 0\n",
        "                        if zeros_pct > 10:\n",
        "                            v_log = np.log1p(all_valid)\n",
        "                            mean, std = np.mean(v_log), np.std(v_log)\n",
        "                            if std > 1e-8:\n",
        "                                metric_data = (np.log1p(metric_data) - mean) / std\n",
        "                            else:\n",
        "                                metric_data = np.log1p(metric_data) - mean\n",
        "                        else:\n",
        "                            mean, std = np.mean(all_valid), np.std(all_valid)\n",
        "                            if std > 1e-8:\n",
        "                                metric_data = (metric_data - mean) / std\n",
        "                            else:\n",
        "                                metric_data = metric_data - mean\n",
        "                    elif metric_type == 'bytes':\n",
        "                        # Bytes - check if log is needed\n",
        "                        if len(all_valid) > 0 and np.max(all_valid) > 0:\n",
        "                            ratio = np.max(all_valid) / (np.min(all_valid[all_valid > 0]) if np.any(all_valid > 0) else 1)\n",
        "                            if ratio > 100:\n",
        "                                v_log = np.log1p(all_valid)\n",
        "                                mean, std = np.mean(v_log), np.std(v_log)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data = (np.log1p(metric_data) - mean) / std\n",
        "                                else:\n",
        "                                    metric_data = np.log1p(metric_data) - mean\n",
        "                            else:\n",
        "                                mean, std = np.mean(all_valid), np.std(all_valid)\n",
        "                                if std > 1e-8:\n",
        "                                    metric_data = (metric_data - mean) / std\n",
        "                                else:\n",
        "                                    metric_data = metric_data - mean\n",
        "                    elif metric_type == 'milicores':\n",
        "                        # Milicores - use z-score\n",
        "                        mean, std = np.mean(all_valid), np.std(all_valid)\n",
        "                        if std > 1e-8:\n",
        "                            metric_data = (metric_data - mean) / std\n",
        "                        else:\n",
        "                            metric_data = metric_data - mean\n",
        "                    else:\n",
        "                        # Default: z-score\n",
        "                        mean, std = np.mean(all_valid), np.std(all_valid)\n",
        "                        if std > 1e-8:\n",
        "                            metric_data = (metric_data - mean) / std\n",
        "                        else:\n",
        "                            metric_data = metric_data - mean\n",
        "                    m[:, :, f] = metric_data\n",
        "        \n",
        "        # Convert NaN to 0 only for invalid values (mask will handle them)\n",
        "        m = np.nan_to_num(m, nan=0.0)\n",
        "        \n",
        "        return m, valid_mask_3d\n",
        "    \n",
        "    def _apply_augmentation(self, m, mask_3d):\n",
        "        \"\"\"Apply augmentation with 3D mask (time, pods, metrics).\"\"\"\n",
        "        if self.mode != \"train\": return m, mask_3d\n",
        "        if self.noise_std > 0: \n",
        "            m = m + np.random.normal(0, self.noise_std, m.shape).astype(np.float32) * mask_3d\n",
        "        if self.mask_prob > 0:\n",
        "            r = np.random.random(mask_3d.shape) > self.mask_prob\n",
        "            mask_3d = mask_3d & r\n",
        "            m = m * mask_3d\n",
        "        return m, mask_3d\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sid, data = self.scenario_ids[idx], self.data[self.scenario_ids[idx]]\n",
        "        gt, fault_idx, rc = data[\"ground_truth\"], data[\"ground_truth\"][\"fault_time_idx\"], data[\"ground_truth\"][\"root_cause_service\"]\n",
        "        first_m = list(data[\"metrics\"].values())[0]\n",
        "        n_t, pods, n_pods = len(first_m), list(first_m.columns), len(first_m.columns)\n",
        "        start, end, new_fault = (self._crop_around_fault(n_t, fault_idx) if n_t > self.window_size else (0, n_t, fault_idx))\n",
        "        \n",
        "        # Use ALL available metrics (not just hardcoded METRIC_NAMES)\n",
        "        # Get metric names from the data itself, sorted for consistency\n",
        "        available_metric_names = sorted(data[\"metrics\"].keys())\n",
        "        m_list = [data[\"metrics\"][mn].iloc[start:end].values if mn in data[\"metrics\"] else np.full((end-start, n_pods), np.nan) for mn in available_metric_names]\n",
        "        m = np.stack(m_list, axis=-1).astype(np.float32)\n",
        "        m, mask_3d = self._normalize_metrics(m, available_metric_names)\n",
        "        m, mask_3d = self._apply_augmentation(m, mask_3d)\n",
        "        if m.shape[0] < self.window_size:\n",
        "            pad = self.window_size - m.shape[0]\n",
        "            n_metrics = m.shape[2]  # Use actual number of metrics from data\n",
        "            m = np.concatenate([m, np.zeros((pad, n_pods, n_metrics), dtype=np.float32)])\n",
        "            mask_3d = np.concatenate([mask_3d, np.zeros((pad, n_pods, n_metrics), dtype=bool)])\n",
        "        \n",
        "        # Get services and prepare for potential randomization\n",
        "        services_orig = data[\"services\"].copy()\n",
        "        services = services_orig.copy()\n",
        "        p2s_orig = data[\"pod_to_service_idx\"]\n",
        "        \n",
        "        # FIX: Randomize service order during training to prevent idx 0 bias\n",
        "        # This ensures the model doesn't learn positional bias (e.g., always predicting first service)\n",
        "        if self.mode == \"train\":\n",
        "            import random\n",
        "            random.shuffle(services)\n",
        "            # Create permutation mapping: original_idx -> new_idx\n",
        "            perm_map = {svc: new_idx for new_idx, svc in enumerate(services)}\n",
        "            # Update pod_to_service_idx to match new service order\n",
        "            p2s_perm = [perm_map.get(services_orig[old_idx], -1) if 0 <= old_idx < len(services_orig) else -1 for old_idx in p2s_orig]\n",
        "            p2s_orig = p2s_perm  # Use permuted mapping\n",
        "        \n",
        "        # Build log texts (one aggregated text per service)\n",
        "        # Support different token modes for ablation study\n",
        "        log_texts = []\n",
        "        if data[\"logs\"] is not None:\n",
        "            logs_df = data[\"logs\"].iloc[start:end]\n",
        "            for service in services_orig:  # Use original order to index logs\n",
        "                if service in logs_df.columns:\n",
        "                    service_logs = logs_df[service].fillna(\"\").tolist()\n",
        "                    combined = \" | \".join([l for l in service_logs if l.strip()])\n",
        "                    if combined.strip():\n",
        "                        log_texts.append(combined[:512])\n",
        "                    else:\n",
        "                        # Empty logs: use token based on mode\n",
        "                        if self.log_token_mode == \"common\":\n",
        "                            log_texts.append(\"[N_LGS_TKN-LEZSHA]\")\n",
        "                        else:  # unique\n",
        "                            log_texts.append(f\"[N_LGS_{service}]\")\n",
        "                else:\n",
        "                    # Service not in logs: use token based on mode\n",
        "                    if self.log_token_mode == \"common\":\n",
        "                        log_texts.append(\"[N_LGS_TKN-LEZSHA]\")\n",
        "                    else:  # unique\n",
        "                        log_texts.append(f\"[N_LGS_{service}]\")\n",
        "        else:\n",
        "            # No logs at all: use token based on mode\n",
        "            if self.log_token_mode == \"common\":\n",
        "                log_texts = [\"[N_LGS_TKN-LEZSHA]\"] * len(services_orig)\n",
        "            else:  # unique\n",
        "                log_texts = [f\"[N_LGS_{svc}]\" for svc in services_orig]\n",
        "        \n",
        "        # Reorder log_texts to match permuted services (if in train mode)\n",
        "        if self.mode == \"train\":\n",
        "            log_texts = [log_texts[services_orig.index(svc)] for svc in services]\n",
        "        \n",
        "        # Label: index of root cause service in current service list (after potential permutation)\n",
        "        # FIX: Raise error instead of defaulting to 0 to prevent bias towards idx 0\n",
        "        if rc not in services:\n",
        "            raise ValueError(f\"Root cause service '{rc}' not found in services list for scenario {sid}\")\n",
        "        label = services.index(rc)\n",
        "        \n",
        "        return {\"scenario_id\": sid, \"metrics\": torch.from_numpy(m), \"metrics_mask\": torch.from_numpy(mask_3d),\n",
        "                \"log_texts\": log_texts, \"pods\": pods, \"services\": services,\n",
        "                \"pod_to_service_idx\": p2s_orig, \"fault_idx\": new_fault, \"label\": label, \"rc_service\": rc}\n",
        "print(\"Dataset class defined\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === MODEL COMPONENTS ===\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos * div), torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return self.dropout(x + self.pe[:, :x.size(1), :])\n",
        "\n",
        "class MetricEncoder(nn.Module):\n",
        "    def __init__(self, n_metrics=4, d_model=128, n_heads=4, n_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.input_proj = nn.Linear(n_metrics, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4, dropout=dropout, activation=\"gelu\", batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass with 3D mask support (time, pods, metrics).\n",
        "        Args:\n",
        "            x: (B, W, P, M) - batch, window, pods, metrics\n",
        "            mask: (B, W, P, M) - 3D mask, True for valid values\n",
        "        \"\"\"\n",
        "        B, W, P, M = x.shape\n",
        "        x = x.permute(0,2,1,3).reshape(B*P, W, M)\n",
        "        \n",
        "        # Handle 3D mask: convert to 2D for transformer (time dimension)\n",
        "        # A timestep is valid if ANY metric is valid for that pod\n",
        "        kpm = None\n",
        "        if mask is not None:\n",
        "            # mask: (B, W, P, M) -> (B*P, W) - True if timestep has ANY valid metric\n",
        "            mask_2d = mask.permute(0,2,1,3).reshape(B*P, W, M)  # (B*P, W, M)\n",
        "            mask_2d = mask_2d.any(dim=-1)  # (B*P, W) - True if any metric is valid\n",
        "            kpm = ~mask_2d  # Key padding mask: True to mask out\n",
        "        \n",
        "        x = self.transformer(self.pos_encoder(self.input_proj(x)), src_key_padding_mask=kpm)\n",
        "        \n",
        "        # Weighted average pooling using 3D mask\n",
        "        if mask is not None:\n",
        "            # mask: (B, W, P, M) -> (B*P, W) - weight by number of valid metrics per timestep\n",
        "            mask_2d = mask.permute(0,2,1,3).reshape(B*P, W, M)  # (B*P, W, M)\n",
        "            weights = mask_2d.float().sum(dim=-1)  # (B*P, W) - number of valid metrics per timestep\n",
        "            weights = weights.unsqueeze(-1)  # (B*P, W, 1)\n",
        "            x = (x * weights).sum(dim=1) / weights.sum(dim=1).clamp(min=1)\n",
        "        else: \n",
        "            x = x.mean(dim=1)\n",
        "        return self.output_proj(x).reshape(B, P, self.d_model)\n",
        "\n",
        "class LogEncoder(nn.Module):\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        super().__init__()\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.embed_dim = 384\n",
        "        self._no_logs_emb = None\n",
        "    @property\n",
        "    def no_logs_embedding(self):\n",
        "        if self._no_logs_emb is None:\n",
        "            with torch.no_grad(): self._no_logs_emb = torch.from_numpy(self.model.encode(\"[N-LGS-DST-TKN-LEZHSA]\", convert_to_numpy=True))\n",
        "        return self._no_logs_emb\n",
        "    def encode_texts(self, texts): return self.model.encode(texts, convert_to_tensor=True)\n",
        "    def forward(self, batch):\n",
        "        embs, max_s = [], max(len(t) for t in batch)\n",
        "        for texts in batch:\n",
        "            e = self.encode_texts(texts)\n",
        "            if len(texts) < max_s: e = torch.cat([e, self.no_logs_embedding.unsqueeze(0).expand(max_s-len(texts),-1).to(e.device)])\n",
        "            embs.append(e)\n",
        "        return torch.stack(embs)\n",
        "\n",
        "class FusionLayer(nn.Module):\n",
        "    def __init__(self, metric_dim=128, log_dim=384, output_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fusion = nn.Sequential(nn.Linear(2*metric_dim+log_dim, output_dim*2), nn.LayerNorm(output_dim*2), nn.GELU(), nn.Dropout(dropout), nn.Linear(output_dim*2, output_dim), nn.LayerNorm(output_dim))\n",
        "        self.metric_dim = metric_dim\n",
        "    def forward(self, me, le, p2s):\n",
        "        B, S, dev = me.size(0), le.size(1), me.device\n",
        "        out = []\n",
        "        for b in range(B):\n",
        "            sme = []\n",
        "            for s in range(S):\n",
        "                pi = [i for i, si in enumerate(p2s[b]) if si == s]\n",
        "                if pi:\n",
        "                    pm = me[b, pi]\n",
        "                    sme.append(torch.cat([pm.max(0).values, pm.mean(0)], -1))\n",
        "                else: sme.append(torch.zeros(2*self.metric_dim, device=dev))\n",
        "            out.append(self.fusion(torch.cat([torch.stack(sme), le[b]], -1)))\n",
        "        return torch.stack(out)\n",
        "\n",
        "class SimilarityClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=256, embed_dim=384, temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(nn.Linear(input_dim, embed_dim), nn.LayerNorm(embed_dim))\n",
        "        self.temperature = temperature\n",
        "        self._cache = {}\n",
        "    def get_svc_emb(self, svcs, le):\n",
        "        k = tuple(svcs)\n",
        "        if k not in self._cache: self._cache[k] = F.normalize(le.encode_texts([f\"microservice: {s}\" for s in svcs]), dim=-1)\n",
        "        return self._cache[k]\n",
        "    def forward(self, fe, sb, le):\n",
        "        B, dev, ms = fe.size(0), fe.device, fe.size(1)\n",
        "        pe = F.normalize(self.proj(fe), dim=-1)\n",
        "        out = []\n",
        "        for b in range(B):\n",
        "            svcs, ns = sb[b], len(sb[b])\n",
        "            se = self.get_svc_emb(svcs, le).to(dev)\n",
        "            sim = (pe[b,:ns] * se).sum(-1) / self.temperature\n",
        "            if ns < ms: sim = torch.cat([sim, torch.full((ms-ns,), -100.0, device=dev)])\n",
        "            out.append(sim)\n",
        "        return torch.stack(out)\n",
        "\n",
        "class MultimodalRCAModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.metric_encoder = MetricEncoder(config[\"n_metrics\"], config[\"d_model\"], config[\"n_heads\"], config[\"n_layers\"], config[\"dropout\"])\n",
        "        self.log_encoder = LogEncoder()\n",
        "        for p in self.log_encoder.model.parameters(): p.requires_grad = False\n",
        "        self.fusion = FusionLayer(config[\"d_model\"], config[\"log_embed_dim\"], config[\"fusion_dim\"], config[\"dropout\"])\n",
        "        self.classifier = SimilarityClassifier(config[\"fusion_dim\"], config[\"log_embed_dim\"], config[\"temperature\"])\n",
        "    def forward(self, m, mm, lt, p2s, sb):\n",
        "        me = self.metric_encoder(m, mm)\n",
        "        le = self.log_encoder(lt).to(m.device)\n",
        "        return self.classifier(self.fusion(me, le, p2s), sb, self.log_encoder)\n",
        "\n",
        "print(\"Model components defined\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model components defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === TRAINING UTILITIES ===\n",
        "def rca_collate_fn(batch):\n",
        "    \"\"\"Collate function with 3D mask support (time, pods, metrics).\"\"\"\n",
        "    mp, mm, ws, nm = max(s[\"metrics\"].size(1) for s in batch), max(len(s[\"services\"]) for s in batch), batch[0][\"metrics\"].size(0), batch[0][\"metrics\"].size(2)\n",
        "    metrics_p, masks_p, p2s_p = [], [], []\n",
        "    for s in batch:\n",
        "        m, mask_3d, p2s, np_ = s[\"metrics\"], s[\"metrics_mask\"], s[\"pod_to_service_idx\"], s[\"metrics\"].size(1)\n",
        "        if np_ < mp:\n",
        "            # Pad metrics and mask: (W, P, M) -> (W, max_pods, M)\n",
        "            m = torch.cat([m, torch.zeros(ws, mp-np_, nm)], 1)\n",
        "            mask_3d = torch.cat([mask_3d, torch.zeros(ws, mp-np_, nm, dtype=torch.bool)], 1)\n",
        "            p2s = p2s + [-1]*(mp-np_)\n",
        "        metrics_p.append(m); masks_p.append(mask_3d); p2s_p.append(p2s)\n",
        "    return {\"scenario_ids\": [s[\"scenario_id\"] for s in batch], \"metrics\": torch.stack(metrics_p), \"metrics_mask\": torch.stack(masks_p),\n",
        "            \"log_texts\": [s[\"log_texts\"] for s in batch], \"pods\": [s[\"pods\"] for s in batch], \"services\": [s[\"services\"] for s in batch],\n",
        "            \"pod_to_service_idx\": p2s_p, \"labels\": torch.tensor([s[\"label\"] for s in batch], dtype=torch.long), \"rc_services\": [s[\"rc_service\"] for s in batch]}\n",
        "\n",
        "def train_epoch(model, dl, opt, cfg, dev):\n",
        "    model.train()\n",
        "    tl, nb = 0, 0\n",
        "    for b in dl:\n",
        "        m, mm, l = b[\"metrics\"].to(dev), b[\"metrics_mask\"].to(dev), b[\"labels\"].to(dev)\n",
        "        logits = model(m, mm, b[\"log_texts\"], b[\"pod_to_service_idx\"], b[\"services\"])\n",
        "        loss = F.cross_entropy(logits, l, label_smoothing=cfg.get(\"label_smoothing\", 0.0))\n",
        "        opt.zero_grad(); loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.get(\"grad_clip_norm\", 1.0))\n",
        "        opt.step(); tl += loss.item(); nb += 1\n",
        "    return tl / nb\n",
        "\n",
        "def evaluate(model, dl, dev):\n",
        "    model.eval()\n",
        "    preds, labels, scores, sids, rcs = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            m, mm = b[\"metrics\"].to(dev), b[\"metrics_mask\"].to(dev)\n",
        "            logits = model(m, mm, b[\"log_texts\"], b[\"pod_to_service_idx\"], b[\"services\"])\n",
        "            preds.extend(logits.argmax(-1).cpu().tolist()); labels.extend(b[\"labels\"].tolist())\n",
        "            scores.extend(logits.cpu().tolist()); sids.extend(b[\"scenario_ids\"]); rcs.extend(b[\"rc_services\"])\n",
        "    acc1 = sum(p==l for p,l in zip(preds, labels)) / len(preds)\n",
        "    acc3 = sum(l in sorted(range(len(s)), key=lambda i: s[i], reverse=True)[:3] for s,l in zip(scores, labels)) / len(labels)\n",
        "    mrr = sum(1.0/(sorted(range(len(s)), key=lambda i: s[i], reverse=True).index(l)+1) for s,l in zip(scores, labels)) / len(labels)\n",
        "    return {\"acc@1\": acc1, \"acc@3\": acc3, \"mrr\": mrr, \"predictions\": list(zip(sids, preds, labels, rcs))}\n",
        "\n",
        "print(\"Training utilities defined\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training utilities defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === LODOCV MAIN LOOP ===\n",
        "def run_lodocv(config, nezha_days, nezha_scenarios, device):\n",
        "    \"\"\"Leave-One-Day-Out Cross-Validation.\n",
        "    \n",
        "    Only uses Nezha scenarios (LEMMA excluded).\n",
        "    Uses type-aware normalization and 3D masking.\n",
        "    \"\"\"\n",
        "    results_per_fold, all_predictions = [], []\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"LEAVE-ONE-DAY-OUT CROSS-VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nFolds: {len(nezha_days)}\")\n",
        "    print(f\"Using only Nezha scenarios (LEMMA excluded)\")\n",
        "    print(f\"Metrics: {config.get('n_metrics', 'unknown')}\")\n",
        "    print(f\"Normalization: Type-aware (per metric type)\")\n",
        "    print(f\"Masking: 3D (time, pods, metrics)\")\n",
        "    print(f\"Epochs per fold: {config['epochs']}\\n\")\n",
        "    \n",
        "    for fold_idx, test_day in enumerate(nezha_days):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FOLD {fold_idx + 1}/{len(nezha_days)}: Test on {test_day}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        test_ids = [s[\"id\"] for s in nezha_scenarios if s[\"day\"] == test_day]\n",
        "        train_ids = [s[\"id\"] for s in nezha_scenarios if s[\"day\"] != test_day]\n",
        "        print(f\"Train: {len(train_ids)} scenarios, Test: {len(test_ids)} scenarios\")\n",
        "        \n",
        "        train_ds = RCAScenarioDataset(train_ids, window_size=config[\"window_size\"], mode=\"train\",\n",
        "                                      jitter=config.get(\"jitter\", 3), noise_std=config.get(\"noise_std\", 0.1),\n",
        "                                      mask_prob=config.get(\"mask_prob\", 0.1),\n",
        "                                      norm_mode=\"per_pod\", log_token_mode=\"unique\")\n",
        "        test_ds = RCAScenarioDataset(test_ids, window_size=config[\"window_size\"], mode=\"eval\",\n",
        "                                     norm_mode=\"per_pod\", log_token_mode=\"unique\")\n",
        "        \n",
        "        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=rca_collate_fn)\n",
        "        test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=rca_collate_fn)\n",
        "        \n",
        "        model = MultimodalRCAModel(config).to(device)\n",
        "        optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "        \n",
        "        best_mrr, best_state = 0.0, None\n",
        "        for epoch in range(config[\"epochs\"]):\n",
        "            if epoch < config.get(\"warmup_epochs\", 5):\n",
        "                for pg in optimizer.param_groups: pg['lr'] = config[\"learning_rate\"] * (epoch + 1) / config.get(\"warmup_epochs\", 5)\n",
        "            train_loss = train_epoch(model, train_loader, optimizer, config, device)\n",
        "            if (epoch + 1) % 10 == 0 or epoch == config[\"epochs\"] - 1:\n",
        "                results = evaluate(model, test_loader, device)\n",
        "                if results[\"mrr\"] > best_mrr: best_mrr, best_state = results[\"mrr\"], {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "                print(f\"  Epoch {epoch+1:2d}: loss={train_loss:.4f}, Acc@1={results['acc@1']:.3f}, Acc@3={results['acc@3']:.3f}, MRR={results['mrr']:.3f}\")\n",
        "        \n",
        "        if best_state: model.load_state_dict(best_state)\n",
        "        final_results = evaluate(model, test_loader, device)\n",
        "        print(f\"\\n  Final: Acc@1={final_results['acc@1']:.3f}, Acc@3={final_results['acc@3']:.3f}, MRR={final_results['mrr']:.3f}\")\n",
        "        \n",
        "        results_per_fold.append({\"fold\": fold_idx+1, \"test_day\": test_day, \"train_size\": len(train_ids), \"test_size\": len(test_ids),\n",
        "                                 \"acc@1\": final_results[\"acc@1\"], \"acc@3\": final_results[\"acc@3\"], \"mrr\": final_results[\"mrr\"]})\n",
        "        all_predictions.extend(final_results[\"predictions\"])\n",
        "        \n",
        "        del model, optimizer, train_ds, test_ds\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    \n",
        "    return results_per_fold, all_predictions\n",
        "\n",
        "print(\"LODOCV function defined\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LODOCV function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === RUN LODOCV ===\n",
        "start_time = datetime.now()\n",
        "print(f\"Starting LODOCV at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "fold_results, all_predictions = run_lodocv(\n",
        "    config=CONFIG, nezha_days=NEZHA_DAYS, nezha_scenarios=NEZHA_SCENARIOS,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f\"\\nCompleted at {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Total duration: {end_time - start_time}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting LODOCV at 2026-02-07 15:08:22\n",
            "======================================================================\n",
            "LEAVE-ONE-DAY-OUT CROSS-VALIDATION\n",
            "======================================================================\n",
            "\n",
            "Folds: 4\n",
            "Using only Nezha scenarios (LEMMA excluded)\n",
            "Metrics: 19\n",
            "Normalization: Type-aware (per metric type)\n",
            "Masking: 3D (time, pods, metrics)\n",
            "Epochs per fold: 50\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FOLD 1/4: Test on 20220822\n",
            "======================================================================\n",
            "Train: 73 scenarios, Test: 21 scenarios\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/root/lemm/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  Epoch 10: loss=2.2682, Acc@1=0.190, Acc@3=0.714, MRR=0.487\n",
            "  Epoch 20: loss=1.4513, Acc@1=0.238, Acc@3=0.810, MRR=0.537\n",
            "  Epoch 30: loss=0.9374, Acc@1=0.333, Acc@3=0.810, MRR=0.581\n",
            "  Epoch 40: loss=0.8043, Acc@1=0.286, Acc@3=0.762, MRR=0.557\n",
            "  Epoch 50: loss=0.7697, Acc@1=0.381, Acc@3=0.762, MRR=0.601\n",
            "\n",
            "  Final: Acc@1=0.381, Acc@3=0.762, MRR=0.601\n",
            "\n",
            "======================================================================\n",
            "FOLD 2/4: Test on 20220823\n",
            "======================================================================\n",
            "Train: 64 scenarios, Test: 30 scenarios\n",
            "  Epoch 10: loss=2.0754, Acc@1=0.633, Acc@3=0.733, MRR=0.728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === RESULTS SUMMARY ===\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LODOCV RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nPer-Fold Results:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Fold':<6} {'Test Day':<12} {'Train':<8} {'Test':<8} {'Acc@1':<8} {'Acc@3':<8} {'MRR':<8}\")\n",
        "print(\"-\"*70)\n",
        "for r in fold_results:\n",
        "    print(f\"{r['fold']:<6} {r['test_day']:<12} {r['train_size']:<8} {r['test_size']:<8} \"\n",
        "          f\"{r['acc@1']:<8.3f} {r['acc@3']:<8.3f} {r['mrr']:<8.3f}\")\n",
        "\n",
        "acc1_vals = [r[\"acc@1\"] for r in fold_results]\n",
        "acc3_vals = [r[\"acc@3\"] for r in fold_results]\n",
        "mrr_vals = [r[\"mrr\"] for r in fold_results]\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Mean':<6} {'':<12} {'':<8} {'':<8} {np.mean(acc1_vals):<8.3f} {np.mean(acc3_vals):<8.3f} {np.mean(mrr_vals):<8.3f}\")\n",
        "print(f\"{'Std':<6} {'':<12} {'':<8} {'':<8} {np.std(acc1_vals):<8.3f} {np.std(acc3_vals):<8.3f} {np.std(mrr_vals):<8.3f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PUBLICATION-READY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nAcc@1: {np.mean(acc1_vals):.3f} +/- {np.std(acc1_vals):.3f}\")\n",
        "print(f\"Acc@3: {np.mean(acc3_vals):.3f} +/- {np.std(acc3_vals):.3f}\")\n",
        "print(f\"MRR:   {np.mean(mrr_vals):.3f} +/- {np.std(mrr_vals):.3f}\")\n",
        "\n",
        "# Save results\n",
        "results_to_save = {\n",
        "    \"config\": {k: v for k, v in CONFIG.items() if not callable(v)},\n",
        "    \"fold_results\": fold_results,\n",
        "    \"summary\": {\"acc@1_mean\": float(np.mean(acc1_vals)), \"acc@1_std\": float(np.std(acc1_vals)),\n",
        "                \"acc@3_mean\": float(np.mean(acc3_vals)), \"acc@3_std\": float(np.std(acc3_vals)),\n",
        "                \"mrr_mean\": float(np.mean(mrr_vals)), \"mrr_std\": float(np.std(mrr_vals))},\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "}\n",
        "with open(BASE_DIR / \"lodocv_results.json\", \"w\") as f:\n",
        "    json.dump(results_to_save, f, indent=2, default=str)\n",
        "print(f\"\\nResults saved to {BASE_DIR / 'lodocv_results.json'}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LODOCV RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Per-Fold Results:\n",
            "----------------------------------------------------------------------\n",
            "Fold   Test Day     Train    Test     Acc@1    Acc@3    MRR     \n",
            "----------------------------------------------------------------------\n",
            "1      20220822     73       21       0.381    0.762    0.601   \n",
            "2      20220823     64       30       0.667    0.767    0.746   \n",
            "3      20230129     66       28       0.357    0.500    0.494   \n",
            "4      20230130     79       15       0.267    0.400    0.400   \n",
            "----------------------------------------------------------------------\n",
            "Mean                                  0.418    0.607    0.560   \n",
            "Std                                   0.150    0.161    0.129   \n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "PUBLICATION-READY RESULTS\n",
            "======================================================================\n",
            "\n",
            "Acc@1: 0.418 +/- 0.150\n",
            "Acc@3: 0.607 +/- 0.161\n",
            "MRR:   0.560 +/- 0.129\n",
            "\n",
            "Results saved to /root/lemm/lodocv_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === ERROR ANALYSIS ===\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "correct = [(sid, pred, label, rc) for sid, pred, label, rc in all_predictions if pred == label]\n",
        "wrong = [(sid, pred, label, rc) for sid, pred, label, rc in all_predictions if pred != label]\n",
        "\n",
        "print(f\"\\nTotal predictions: {len(all_predictions)}\")\n",
        "print(f\"Correct: {len(correct)} ({100*len(correct)/len(all_predictions):.1f}%)\")\n",
        "print(f\"Wrong:   {len(wrong)} ({100*len(wrong)/len(all_predictions):.1f}%)\")\n",
        "\n",
        "if wrong:\n",
        "    print(\"\\nWrong predictions (first 20):\")\n",
        "    for sid, pred, label, rc in wrong[:20]:\n",
        "        print(f\"  {sid}: predicted idx {pred}, actual {rc} (idx {label})\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ERROR ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Total predictions: 94\n",
            "Correct: 42 (44.7%)\n",
            "Wrong:   52 (55.3%)\n",
            "\n",
            "Wrong predictions (first 20):\n",
            "  20220822_nezha_0: predicted idx 0, actual frontend (idx 5)\n",
            "  20220822_nezha_11: predicted idx 2, actual currencyservice (idx 3)\n",
            "  20220822_nezha_12: predicted idx 0, actual emailservice (idx 4)\n",
            "  20220822_nezha_13: predicted idx 0, actual emailservice (idx 4)\n",
            "  20220822_nezha_16: predicted idx 0, actual productcatalogservice (idx 7)\n",
            "  20220822_nezha_17: predicted idx 0, actual recommendationservice (idx 8)\n",
            "  20220822_nezha_19: predicted idx 0, actual recommendationservice (idx 8)\n",
            "  20220822_nezha_20: predicted idx 0, actual shippingservice (idx 9)\n",
            "  20220822_nezha_21: predicted idx 0, actual shippingservice (idx 9)\n",
            "  20220822_nezha_3: predicted idx 0, actual frontend (idx 5)\n",
            "  20220822_nezha_4: predicted idx 7, actual cartservice (idx 1)\n",
            "  20220822_nezha_7: predicted idx 0, actual checkoutservice (idx 2)\n",
            "  20220822_nezha_9: predicted idx 0, actual checkoutservice (idx 2)\n",
            "  20220823_nezha_1: predicted idx 3, actual frontend (idx 5)\n",
            "  20220823_nezha_11: predicted idx 2, actual currencyservice (idx 3)\n",
            "  20220823_nezha_13: predicted idx 0, actual emailservice (idx 4)\n",
            "  20220823_nezha_17: predicted idx 1, actual productcatalogservice (idx 7)\n",
            "  20220823_nezha_18: predicted idx 2, actual productcatalogservice (idx 7)\n",
            "  20220823_nezha_20: predicted idx 2, actual productcatalogservice (idx 7)\n",
            "  20220823_nezha_27: predicted idx 4, actual adservice (idx 0)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}