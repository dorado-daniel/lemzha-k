{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMMA-RCA Format Converter\n",
        "\n",
        "LEMMA scenarios have inconsistent directory structures across dates. Normalizes everything to the `20240215` layout (renames files, reorganizes folders). Data content is not modified.\n",
        "\n",
        "| Aspect | Source | Target |\n",
        "|--------|--------|--------|\n",
        "| Metrics | `cpu_usage.npy`, various subfolders | `pod_cpu_usage_total.npy` in `metrics_data/{scenario}/` |\n",
        "| Logs | `pod_message/`, `pod_removed/` | `log_data/{scenario}/log_data/pod/` |\n",
        "| Log frequency | various paths | `{MMDD}_log_frequency_pod_level_removed.npy` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Directory Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import subprocess\n",
        "import gc\n",
        "\n",
        "# Number of parallel workers for file extraction \n",
        "NUM_WORKERS = 20\n",
        "\n",
        "# Prefer native unzip command (faster)\n",
        "USE_NATIVE_UNZIP = True\n",
        "\n",
        "# Base directory containing the LEMMA-RCA dataset\n",
        "BASE_DIR = Path(\"/root/lemm\")\n",
        "\n",
        "# Source directories (original lemma-RCA preprocessed data)\n",
        "ZIPS_METRICS = BASE_DIR / \"Cloud_Computing_Preprocessed\" / \"Metrics Data\"\n",
        "ZIPS_LOGS = BASE_DIR / \"Cloud_Computing_Preprocessed\" / \"Log Data\"\n",
        "\n",
        "# Target directories (unified format)\n",
        "OUT_METRICS = BASE_DIR / \"metrics_data\"\n",
        "OUT_LOGS = BASE_DIR / \"log_data\"\n",
        "\n",
        "print(f\"Source metrics: {ZIPS_METRICS}\")\n",
        "print(f\"Source logs:    {ZIPS_LOGS}\")\n",
        "print(f\"Output metrics: {OUT_METRICS}\")\n",
        "print(f\"Output logs:    {OUT_LOGS}\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source metrics: /root/lemm/Cloud_Computing_Preprocessed/Metrics Data\n",
            "Source logs:    /root/lemm/Cloud_Computing_Preprocessed/Log Data\n",
            "Output metrics: /root/lemm/metrics_data\n",
            "Output logs:    /root/lemm/log_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Scenario Configuration\n",
        "\n",
        "ZIP structures vary per scenario â€” paths below found by manual inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metric filename mapping: source name -> target name (20240215 format)\n",
        "METRIC_MAPPING = {\n",
        "    \"pod_level_data_cpu_usage.npy\": \"pod_level_data_pod_cpu_usage_total.npy\",\n",
        "    \"pod_level_data_memory_usage.npy\": \"pod_level_data_pod_memory_working_set.npy\",\n",
        "    \"pod_level_data_received_bandwidth.npy\": \"pod_level_data_pod_network_rx_bytes.npy\",\n",
        "    \"pod_level_data_transmit_bandwidth.npy\": \"pod_level_data_pod_network_tx_bytes.npy\",\n",
        "}\n",
        "\n",
        "# Scenario-specific configuration with EXACT paths (verified via unzip -l)\n",
        "SCENARIOS = {\n",
        "    \"20231207\": {\n",
        "        # Metrics: 20231207.zip -> 20231207/pod_level_data_*.npy\n",
        "        \"zip_metrics\": \"20231207.zip\",\n",
        "        \"metrics_prefix\": \"20231207/\",\n",
        "        # Logs: 20231207.zip -> log_data/pod_removed/*_structured.csv\n",
        "        \"zip_logs\": \"20231207.zip\",\n",
        "        \"logs_prefix\": \"log_data/pod_removed/\",\n",
        "        # log_frequency in Logs ZIP\n",
        "        \"log_frequency_zip\": \"logs\",\n",
        "        \"log_frequency_path\": \"log_data/pod_level_log_frequency.npy\",\n",
        "        # Metadata\n",
        "    },\n",
        "    \"20231221\": {\n",
        "        # Metrics: 20231221.zip -> 20231221/metric_error/pod_level_data_*.npy\n",
        "        \"zip_metrics\": \"20231221.zip\",\n",
        "        \"metrics_prefix\": \"20231221/metric_error/\",\n",
        "        # Logs: 20231221.zip -> 20231221/20231221/log_data/pod_message/*_structured.csv\n",
        "        \"zip_logs\": \"20231221.zip\",\n",
        "        \"logs_prefix\": \"20231221/20231221/log_data/pod_message/\",\n",
        "        # log_frequency in Logs ZIP\n",
        "        \"log_frequency_zip\": \"logs\",\n",
        "        \"log_frequency_path\": \"20231221/20231221/log_data/pod_level_log_frequency.npy\",\n",
        "        # Metadata\n",
        "    },\n",
        "    \"20240115\": {\n",
        "        # Metrics: 20240115.zip -> 20240115/latency/pod_level_data_*.npy\n",
        "        \"zip_metrics\": \"20240115.zip\",\n",
        "        \"metrics_prefix\": \"20240115/latency/\",\n",
        "        # Logs: 20240115.zip -> 20240115/log_data/pod_message/*_structured.csv\n",
        "        \"zip_logs\": \"20240115.zip\",\n",
        "        \"logs_prefix\": \"20240115/log_data/pod_message/\",\n",
        "        # log_frequency in Logs ZIP\n",
        "        \"log_frequency_zip\": \"logs\",\n",
        "        \"log_frequency_path\": \"20240115/pod_level_log_frequency.npy\",\n",
        "        # Metadata\n",
        "    },\n",
        "\n",
        "    #THIS SCENARIO IS THE TARGET FORMAT, NO NEED TO CONVERT METRICS BUT WE NEED TO CONVERT LOG DATA\n",
        "    \"20240215\": {\n",
        "        # Metrics: 20240215.zip -> 20240215/pod_level_data_pod_*.npy (already correct format)\n",
        "        \"zip_metrics\": \"20240215.zip\",\n",
        "        \"metrics_prefix\": \"20240215/\",\n",
        "        # Logs: 20240215.zip -> log_data/pod/*_structured.csv\n",
        "        \"zip_logs\": \"20240215.zip\",\n",
        "        \"logs_prefix\": \"log_data/pod/\",\n",
        "        # log_frequency in Logs ZIP (not Metrics ZIP!)\n",
        "        \"log_frequency_zip\": \"logs\",\n",
        "        \"log_frequency_path\": \"log_data/pod_level_log_frequency.npy\",\n",
        "        # Metadata\n",
        "        # Flag: metrics filenames already in target format\n",
        "        \"skip_metric_mapping\": True,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"Scenarios configured: {list(SCENARIOS.keys())}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scenarios configured: ['20231207', '20231221', '20240115', '20240215']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Metric Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_metrics(scenario_id: str, config: dict) -> Path:\n",
        "    \"\"\"\n",
        "    Extract and rename metric files from source ZIP to target format.\n",
        "    \"\"\"\n",
        "    zip_path = ZIPS_METRICS / config[\"zip_metrics\"]\n",
        "    out_dir = OUT_METRICS / scenario_id\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Extracting metrics: {scenario_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Source ZIP: {zip_path.name}\")\n",
        "    print(f\"Prefix: {config['metrics_prefix']}\")\n",
        "    \n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        all_files = zf.namelist()\n",
        "        prefix = config['metrics_prefix']\n",
        "        \n",
        "        skip_mapping = config.get('skip_metric_mapping', False)\n",
        "        \n",
        "        if skip_mapping:\n",
        "            # Extract files with original names (already in target format)\n",
        "            target_files = [\n",
        "                \"pod_level_data_pod_cpu_usage_total.npy\",\n",
        "                \"pod_level_data_pod_memory_working_set.npy\",\n",
        "                \"pod_level_data_pod_network_rx_bytes.npy\",\n",
        "                \"pod_level_data_pod_network_tx_bytes.npy\",\n",
        "            ]\n",
        "            for target_name in target_files:\n",
        "                src_path = prefix + target_name\n",
        "                if src_path in all_files:\n",
        "                    print(f\"  [OK] {target_name} (no rename needed)\")\n",
        "                    with zf.open(src_path) as f:\n",
        "                        data = f.read()\n",
        "                    with open(out_dir / target_name, 'wb') as f:\n",
        "                        f.write(data)\n",
        "                else:\n",
        "                    print(f\"  [ERROR] {target_name} not found at {src_path}\")\n",
        "        else:\n",
        "            # Extract and rename core metrics using METRIC_MAPPING\n",
        "            for src_name, dst_name in METRIC_MAPPING.items():\n",
        "                src_path = prefix + src_name\n",
        "                if src_path not in all_files:\n",
        "                    print(f\"  [ERROR] {src_name} not found at {src_path}\")\n",
        "                    continue\n",
        "                print(f\"  [OK] {src_name} -> {dst_name}\")\n",
        "                with zf.open(src_path) as f:\n",
        "                    data = f.read()\n",
        "                with open(out_dir / dst_name, 'wb') as f:\n",
        "                    f.write(data)\n",
        "        \n",
        "        # Extract log_frequency if it's in the metrics ZIP\n",
        "        log_freq_zip = config.get('log_frequency_zip', 'metrics')\n",
        "        if log_freq_zip == 'metrics':\n",
        "            log_freq_path = config.get('log_frequency_path')\n",
        "            if log_freq_path and log_freq_path in all_files:\n",
        "                log_freq_dst = f\"{scenario_id[-4:]}_log_frequency_pod_level_removed.npy\"\n",
        "                print(f\"  [OK] log_frequency -> {log_freq_dst}\")\n",
        "                with zf.open(log_freq_path) as f:\n",
        "                    data = f.read()\n",
        "                with open(out_dir / log_freq_dst, 'wb') as f:\n",
        "                    f.write(data)\n",
        "    \n",
        "    return out_dir\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Log Extraction\n",
        "\n",
        "Streams CSVs in 8MB chunks to keep memory low. Also extracts log_frequency NPY.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_logs_native(scenario_id: str, config: dict, zip_path: Path, out_dir: Path, metrics_out_dir: Path) -> int:\n",
        "    \"\"\"\n",
        "    Extract logs using native unzip command (fastest method).\n",
        "    Returns number of files extracted.\n",
        "    \"\"\"\n",
        "    prefix = config['logs_prefix']\n",
        "    \n",
        "    # Count files before extraction\n",
        "    files_before = set(out_dir.glob(\"*_structured.csv\"))\n",
        "    \n",
        "    # Extract directly to output directory using unzip\n",
        "    # -j: junk paths (extract flat, ignore directory structure in ZIP)\n",
        "    # -q: quiet\n",
        "    # -o: overwrite existing files\n",
        "    # -d: destination directory\n",
        "    pattern = f\"{prefix}*_structured.csv\"\n",
        "    cmd = [\"unzip\", \"-j\", \"-q\", \"-o\", str(zip_path), pattern, \"-d\", str(out_dir)]\n",
        "    \n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if result.returncode not in (0, 11):  # 11 = no files matched (not an error for us)\n",
        "        print(f\"  [WARNING] unzip returned {result.returncode}: {result.stderr}\")\n",
        "    \n",
        "    # Count files after extraction\n",
        "    files_after = set(out_dir.glob(\"*_structured.csv\"))\n",
        "    count = len(files_after - files_before) if files_before else len(files_after)\n",
        "    \n",
        "    # Extract log_frequency separately (single file)\n",
        "    log_freq_zip = config.get('log_frequency_zip', 'metrics')\n",
        "    if log_freq_zip == 'logs':\n",
        "        log_freq_path = config.get('log_frequency_path')\n",
        "        if log_freq_path:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "                if log_freq_path in zf.namelist():\n",
        "                    log_freq_dst = f\"{scenario_id[-4:]}_log_frequency_pod_level_removed.npy\"\n",
        "                    print(f\"  [OK] log_frequency -> {log_freq_dst}\")\n",
        "                    with zf.open(log_freq_path) as f:\n",
        "                        data = f.read()\n",
        "                    with open(metrics_out_dir / log_freq_dst, 'wb') as f:\n",
        "                        f.write(data)\n",
        "                else:\n",
        "                    print(f\"  [ERROR] log_frequency not found at {log_freq_path}\")\n",
        "    \n",
        "    return count\n",
        "\n",
        "\n",
        "def extract_logs_python(scenario_id: str, config: dict, zip_path: Path, out_dir: Path, metrics_out_dir: Path) -> int:\n",
        "    \"\"\"\n",
        "    Extract logs using Python zipfile with ThreadPoolExecutor (fallback method).\n",
        "    Returns number of files extracted.\n",
        "    \"\"\"\n",
        "    prefix = config['logs_prefix']\n",
        "    \n",
        "    # First pass: collect list of files to extract\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        all_files = zf.namelist()\n",
        "        csv_files = [f for f in all_files if f.startswith(prefix) and f.endswith('_structured.csv')]\n",
        "    \n",
        "    total_files = len(csv_files)\n",
        "    print(f\"  Found {total_files} CSV files (using {NUM_WORKERS} Python workers)\")\n",
        "    \n",
        "    # Thread-safe counter for progress reporting\n",
        "    extracted_count = [0]\n",
        "    counter_lock = threading.Lock()\n",
        "    \n",
        "    def extract_batch(batch: list) -> int:\n",
        "        \"\"\"Extract a batch of CSV files. Each thread opens its own ZIP handle.\"\"\"\n",
        "        count = 0\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            for src_path in batch:\n",
        "                try:\n",
        "                    filename = Path(src_path).name\n",
        "                    with zf.open(src_path) as f:\n",
        "                        data = f.read()\n",
        "                    with open(out_dir / filename, 'wb') as f:\n",
        "                        f.write(data)\n",
        "                    count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  [ERROR] Failed to extract {src_path}: {e}\")\n",
        "        \n",
        "        with counter_lock:\n",
        "            extracted_count[0] += count\n",
        "            current = extracted_count[0]\n",
        "            if current % 500 < count:\n",
        "                print(f\"  Progress: {current}/{total_files} CSVs extracted\")\n",
        "        return count\n",
        "    \n",
        "    # Split files into batches\n",
        "    batch_size = max(1, len(csv_files) // NUM_WORKERS)\n",
        "    batches = [csv_files[i:i + batch_size] for i in range(0, len(csv_files), batch_size)]\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "        futures = [executor.submit(extract_batch, batch) for batch in batches]\n",
        "        for future in as_completed(futures):\n",
        "            pass\n",
        "    \n",
        "    # Extract log_frequency\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        all_files = zf.namelist()\n",
        "        log_freq_zip = config.get('log_frequency_zip', 'metrics')\n",
        "        if log_freq_zip == 'logs':\n",
        "            log_freq_path = config.get('log_frequency_path')\n",
        "            if log_freq_path and log_freq_path in all_files:\n",
        "                log_freq_dst = f\"{scenario_id[-4:]}_log_frequency_pod_level_removed.npy\"\n",
        "                print(f\"  [OK] log_frequency -> {log_freq_dst}\")\n",
        "                with zf.open(log_freq_path) as f:\n",
        "                    data = f.read()\n",
        "                with open(metrics_out_dir / log_freq_dst, 'wb') as f:\n",
        "                    f.write(data)\n",
        "            else:\n",
        "                print(f\"  [ERROR] log_frequency not found at {log_freq_path}\")\n",
        "    \n",
        "    return extracted_count[0]\n",
        "\n",
        "\n",
        "def extract_logs(scenario_id: str, config: dict) -> Path:\n",
        "    \"\"\"\n",
        "    Extract log CSV files from source ZIP to target format.\n",
        "    Uses native unzip if available (faster), falls back to Python threading.\n",
        "    \"\"\"\n",
        "    zip_path = ZIPS_LOGS / config[\"zip_logs\"]\n",
        "    out_dir = OUT_LOGS / scenario_id / \"log_data\" / \"pod\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    metrics_out_dir = OUT_METRICS / scenario_id\n",
        "    metrics_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Extracting logs: {scenario_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Source ZIP: {zip_path.name}\")\n",
        "    print(f\"Prefix: {config['logs_prefix']}\")\n",
        "    \n",
        "    # Check if native unzip is available and preferred\n",
        "    use_native = USE_NATIVE_UNZIP\n",
        "    if use_native:\n",
        "        try:\n",
        "            subprocess.run([\"unzip\", \"-v\"], capture_output=True, check=True)\n",
        "            print(f\"  Using native unzip (fastest)\")\n",
        "            count = extract_logs_native(scenario_id, config, zip_path, out_dir, metrics_out_dir)\n",
        "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "            print(f\"  Native unzip not available, using Python fallback\")\n",
        "            use_native = False\n",
        "    \n",
        "    if not use_native:\n",
        "        count = extract_logs_python(scenario_id, config, zip_path, out_dir, metrics_out_dir)\n",
        "    \n",
        "    print(f\"  Total CSVs extracted: {count}\")\n",
        "    return out_dir\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verification\n",
        "\n",
        "Checks that all expected files exist and NPYs load without corruption.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def verify_extraction(scenario_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Verify that extraction produced all required files.\n",
        "    Uses memory-efficient verification without keeping data in RAM.\n",
        "    \n",
        "    Args:\n",
        "        scenario_id: Scenario identifier\n",
        "    \n",
        "    Returns:\n",
        "        True if all files present and valid, False otherwise\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Verifying: {scenario_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    metrics_dir = OUT_METRICS / scenario_id\n",
        "    logs_dir = OUT_LOGS / scenario_id / \"log_data\" / \"pod\"\n",
        "    \n",
        "    required_metrics = [\n",
        "        \"pod_level_data_pod_cpu_usage_total.npy\",\n",
        "        \"pod_level_data_pod_memory_working_set.npy\",\n",
        "        \"pod_level_data_pod_network_rx_bytes.npy\",\n",
        "        \"pod_level_data_pod_network_tx_bytes.npy\",\n",
        "        f\"{scenario_id[-4:]}_log_frequency_pod_level_removed.npy\",\n",
        "    ]\n",
        "    \n",
        "    all_ok = True\n",
        "    \n",
        "    print(\"\\nMetrics:\")\n",
        "    for fname in required_metrics:\n",
        "        fpath = metrics_dir / fname\n",
        "        if not fpath.exists():\n",
        "            print(f\"  [MISSING] {fname}\")\n",
        "            all_ok = False\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Load, extract info, and immediately free memory\n",
        "            data = np.load(fpath, allow_pickle=True).item()\n",
        "            key = list(data.keys())[0]\n",
        "            n_entities = len(data[key].get('Pod_Name', data[key].get('Node_Name', [])))\n",
        "            n_time = len(data[key]['time'])\n",
        "            del data  # Explicit delete to allow GC\n",
        "            print(f\"  [OK] {fname}: {n_entities} entities, {n_time} timestamps\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [CORRUPT] {fname}: {e}\")\n",
        "            all_ok = False\n",
        "    \n",
        "    print(\"\\nLogs:\")\n",
        "    if not logs_dir.exists():\n",
        "        print(f\"  [MISSING] Directory: {logs_dir}\")\n",
        "        all_ok = False\n",
        "    else:\n",
        "        csv_files = list(logs_dir.glob(\"*_structured.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            print(f\"  [WARNING] No structured CSV files found\")\n",
        "            all_ok = False\n",
        "        else:\n",
        "            print(f\"  [OK] Directory exists: {len(csv_files)} structured.csv files\")\n",
        "    \n",
        "    return all_ok\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run\n",
        "\n",
        "Processes all scenarios. Log extraction is parallelized with ThreadPoolExecutor.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Process each scenario\n",
        "results = {}\n",
        "\n",
        "for scenario_id, config in SCENARIOS.items():\n",
        "    try:\n",
        "        extract_metrics(scenario_id, config)\n",
        "        extract_logs(scenario_id, config)\n",
        "        ok = verify_extraction(scenario_id)\n",
        "        results[scenario_id] = \"OK\" if ok else \"PARTIAL\"\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] {scenario_id}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results[scenario_id] = \"ERROR\"\n",
        "    finally:\n",
        "        # Force garbage collection to free RAM between scenarios\n",
        "        gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "for sid, status in results.items():\n",
        "    print(f\"  {sid}: {status}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Extracting metrics: 20231207\n",
            "============================================================\n",
            "Source ZIP: 20231207.zip\n",
            "Prefix: 20231207/\n",
            "  [OK] pod_level_data_cpu_usage.npy -> pod_level_data_pod_cpu_usage_total.npy\n",
            "  [OK] pod_level_data_memory_usage.npy -> pod_level_data_pod_memory_working_set.npy\n",
            "  [OK] pod_level_data_received_bandwidth.npy -> pod_level_data_pod_network_rx_bytes.npy\n",
            "  [OK] pod_level_data_transmit_bandwidth.npy -> pod_level_data_pod_network_tx_bytes.npy\n",
            "\n",
            "============================================================\n",
            "Extracting logs: 20231207\n",
            "============================================================\n",
            "Source ZIP: 20231207.zip\n",
            "Prefix: log_data/pod_removed/\n",
            "  Using native unzip (fastest)\n",
            "  [OK] log_frequency -> 1207_log_frequency_pod_level_removed.npy\n",
            "  Total CSVs extracted: 195\n",
            "\n",
            "============================================================\n",
            "Verifying: 20231207\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  [OK] pod_level_data_pod_cpu_usage_total.npy: 196 entities, 79288 timestamps\n",
            "  [OK] pod_level_data_pod_memory_working_set.npy: 196 entities, 79299 timestamps\n",
            "  [OK] pod_level_data_pod_network_rx_bytes.npy: 196 entities, 79509 timestamps\n",
            "  [OK] pod_level_data_pod_network_tx_bytes.npy: 196 entities, 79509 timestamps\n",
            "  [OK] 1207_log_frequency_pod_level_removed.npy: 98 entities, 5535 timestamps\n",
            "\n",
            "Logs:\n",
            "  [OK] Directory exists: 195 structured.csv files\n",
            "\n",
            "============================================================\n",
            "Extracting metrics: 20231221\n",
            "============================================================\n",
            "Source ZIP: 20231221.zip\n",
            "Prefix: 20231221/metric_error/\n",
            "  [OK] pod_level_data_cpu_usage.npy -> pod_level_data_pod_cpu_usage_total.npy\n",
            "  [OK] pod_level_data_memory_usage.npy -> pod_level_data_pod_memory_working_set.npy\n",
            "  [OK] pod_level_data_received_bandwidth.npy -> pod_level_data_pod_network_rx_bytes.npy\n",
            "  [OK] pod_level_data_transmit_bandwidth.npy -> pod_level_data_pod_network_tx_bytes.npy\n",
            "\n",
            "============================================================\n",
            "Extracting logs: 20231221\n",
            "============================================================\n",
            "Source ZIP: 20231221.zip\n",
            "Prefix: 20231221/20231221/log_data/pod_message/\n",
            "  Using native unzip (fastest)\n",
            "  [OK] log_frequency -> 1221_log_frequency_pod_level_removed.npy\n",
            "  Total CSVs extracted: 370\n",
            "\n",
            "============================================================\n",
            "Verifying: 20231221\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  [OK] pod_level_data_pod_cpu_usage_total.npy: 159 entities, 107820 timestamps\n",
            "  [OK] pod_level_data_pod_memory_working_set.npy: 159 entities, 107850 timestamps\n",
            "  [OK] pod_level_data_pod_network_rx_bytes.npy: 159 entities, 107850 timestamps\n",
            "  [OK] pod_level_data_pod_network_tx_bytes.npy: 159 entities, 107850 timestamps\n",
            "  [OK] 1221_log_frequency_pod_level_removed.npy: 193 entities, 40228 timestamps\n",
            "\n",
            "Logs:\n",
            "  [OK] Directory exists: 370 structured.csv files\n",
            "\n",
            "============================================================\n",
            "Extracting metrics: 20240115\n",
            "============================================================\n",
            "Source ZIP: 20240115.zip\n",
            "Prefix: 20240115/latency/\n",
            "  [OK] pod_level_data_cpu_usage.npy -> pod_level_data_pod_cpu_usage_total.npy\n",
            "  [OK] pod_level_data_memory_usage.npy -> pod_level_data_pod_memory_working_set.npy\n",
            "  [OK] pod_level_data_received_bandwidth.npy -> pod_level_data_pod_network_rx_bytes.npy\n",
            "  [OK] pod_level_data_transmit_bandwidth.npy -> pod_level_data_pod_network_tx_bytes.npy\n",
            "\n",
            "============================================================\n",
            "Extracting logs: 20240115\n",
            "============================================================\n",
            "Source ZIP: 20240115.zip\n",
            "Prefix: 20240115/log_data/pod_message/\n",
            "  Using native unzip (fastest)\n",
            "  [OK] log_frequency -> 0115_log_frequency_pod_level_removed.npy\n",
            "  Total CSVs extracted: 131\n",
            "\n",
            "============================================================\n",
            "Verifying: 20240115\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  [OK] pod_level_data_pod_cpu_usage_total.npy: 257 entities, 159856 timestamps\n",
            "  [OK] pod_level_data_pod_memory_working_set.npy: 257 entities, 159612 timestamps\n",
            "  [OK] pod_level_data_pod_network_rx_bytes.npy: 257 entities, 159852 timestamps\n",
            "  [OK] pod_level_data_pod_network_tx_bytes.npy: 257 entities, 159852 timestamps\n",
            "  [OK] 0115_log_frequency_pod_level_removed.npy: 78 entities, 5439 timestamps\n",
            "\n",
            "Logs:\n",
            "  [OK] Directory exists: 131 structured.csv files\n",
            "\n",
            "============================================================\n",
            "Extracting metrics: 20240215\n",
            "============================================================\n",
            "Source ZIP: 20240215.zip\n",
            "Prefix: 20240215/\n",
            "  [OK] pod_level_data_pod_cpu_usage_total.npy (no rename needed)\n",
            "  [OK] pod_level_data_pod_memory_working_set.npy (no rename needed)\n",
            "  [OK] pod_level_data_pod_network_rx_bytes.npy (no rename needed)\n",
            "  [OK] pod_level_data_pod_network_tx_bytes.npy (no rename needed)\n",
            "\n",
            "============================================================\n",
            "Extracting logs: 20240215\n",
            "============================================================\n",
            "Source ZIP: 20240215.zip\n",
            "Prefix: log_data/pod/\n",
            "  Using native unzip (fastest)\n",
            "  [OK] log_frequency -> 0215_log_frequency_pod_level_removed.npy\n",
            "  Total CSVs extracted: 2678\n",
            "\n",
            "============================================================\n",
            "Verifying: 20240215\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  [OK] pod_level_data_pod_cpu_usage_total.npy: 59 entities, 128 timestamps\n",
            "  [OK] pod_level_data_pod_memory_working_set.npy: 59 entities, 128 timestamps\n",
            "  [OK] pod_level_data_pod_network_rx_bytes.npy: 59 entities, 115 timestamps\n",
            "  [OK] pod_level_data_pod_network_tx_bytes.npy: 59 entities, 115 timestamps\n",
            "  [OK] 0215_log_frequency_pod_level_removed.npy: 749 entities, 5748 timestamps\n",
            "\n",
            "Logs:\n",
            "  [OK] Directory exists: 2678 structured.csv files\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "  20231207: OK\n",
            "  20231221: OK\n",
            "  20240115: OK\n",
            "  20240215: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# %reset -f \n",
        "# if you are in a linux machine, you can run this to free up memory: \n",
        "# sync && echo 3 | sudo tee /proc/sys/vm/drop_caches"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}