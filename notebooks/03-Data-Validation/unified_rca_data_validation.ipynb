{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Validation (LEMMA + Nezha)\n",
        "\n",
        "Checks all 98 converted scenarios for structural correctness, temporal alignment, and signal quality.\n",
        "\n",
        "Validation tiers:\n",
        "1. Structure — manifest/ground_truth JSON coherence\n",
        "2. Temporal — monotonic indices, no gaps, fault_bin in range\n",
        "3. Metrics — mean shift (shift_sigma) around fault\n",
        "4. Logs — textual pattern/volume changes\n",
        "\n",
        "Each scenario gets an A/B/C/D grade based on signal strength (A = strong metrics + logs, D = both weak).\n",
        "\n",
        "LEMMA scenarios match `YYYYMMDD`, Nezha match `YYYYMMDD_nezha_N`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    HAS_SBERT = True\n",
        "    print(\"[INFO] sentence_transformers available - using MiniLM embeddings\")\n",
        "except ImportError:\n",
        "    HAS_SBERT = False\n",
        "    print(\"[INFO] sentence_transformers NOT available - falling back to TF-IDF\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/lemm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] sentence_transformers available - using MiniLM embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === CONFIGURATION ===\n",
        "ROOT = Path(\"/root/lemm/core_multimodal_tmp\")\n",
        "METRICS_BASE = Path(\"/root/lemm/core_metrics_tmp\")\n",
        "LOGS_BASE = Path(\"/root/lemm/core_logs_tmp\")\n",
        "\n",
        "WINDOW_BEFORE = pd.Timedelta(\"10min\")\n",
        "WINDOW_AFTER = pd.Timedelta(\"10min\")\n",
        "\n",
        "OUTDIR = Path(\"./unified_rca_reports\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "(OUTDIR / \"figs\").mkdir(exist_ok=True)\n",
        "\n",
        "# Patterns to identify datasets\n",
        "NEZHA_PATTERN = re.compile(r\"^\\d{8}_nezha_\\d+$\")\n",
        "LEMMA_PATTERN = re.compile(r\"^\\d{8}$\")\n",
        "\n",
        "# Excluded scenarios (known bad data)\n",
        "EXCLUDED_SCENARIOS = {\n",
        "    # Corrupted data\n",
        "    \"20220822_nezha_22\",  # adservice timestamps corrupted (year 2028)\n",
        "    \"20220822_nezha_23\",  # adservice timestamps corrupted (year 2028)\n",
        "    \"20230130_nezha_15\",  # ts-security-service CPU=0 in original dataset\n",
        "    \"20230130_nezha_16\",  # ts-security-service CPU=0 in original dataset\n",
        "    # No signal\n",
        "    \"20220822_nezha_14\",  # network_delay no signal\n",
        "    \"20220823_nezha_21\",  # network_delay no signal\n",
        "    \"20220823_nezha_24\",  # network_delay no signal\n",
        "}\n",
        "\n",
        "print(f\"ROOT: {ROOT}\")\n",
        "print(f\"OUTDIR: {OUTDIR.absolute()}\")\n",
        "print(f\"Excluded scenarios: {len(EXCLUDED_SCENARIOS)}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROOT: /root/lemm/core_multimodal_tmp\n",
            "OUTDIR: /root/lemm/notebooks/03-Data-Validation/unified_rca_reports\n",
            "Excluded scenarios: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Detection & Scenario Discovery\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def detect_dataset(scenario_id: str) -> str:\n",
        "    \"\"\"Detect if scenario is from LEMMA or Nezha.\"\"\"\n",
        "    if NEZHA_PATTERN.match(scenario_id):\n",
        "        return \"nezha\"\n",
        "    elif LEMMA_PATTERN.match(scenario_id):\n",
        "        return \"lemma\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "def get_metric_files_map(dataset: str) -> dict:\n",
        "    \"\"\"Get the correct metric file names based on dataset.\"\"\"\n",
        "    if dataset == \"lemma\":\n",
        "        return {\n",
        "            \"cpu\": \"pod_level_data_pod_cpu_usage_total.parquet\",\n",
        "            \"memory\": \"pod_level_data_pod_memory_working_set.parquet\",\n",
        "            \"rx_bytes\": \"pod_level_data_pod_network_rx_bytes.parquet\",\n",
        "            \"tx_bytes\": \"pod_level_data_pod_network_tx_bytes.parquet\",\n",
        "        }\n",
        "    else:  # nezha\n",
        "        return {\n",
        "            \"cpu\": \"pod_cpu_usage_total.parquet\",\n",
        "            \"memory\": \"pod_memory_working_set.parquet\",\n",
        "            \"rx_bytes\": \"pod_network_rx_bytes.parquet\",\n",
        "            \"tx_bytes\": \"pod_network_tx_bytes.parquet\",\n",
        "            # Extra metrics only in Nezha\n",
        "            \"latency_server\": \"pod_latency_server_p95.parquet\",\n",
        "            \"latency_client\": \"pod_latency_client_p95.parquet\",\n",
        "            \"workload\": \"pod_workload_ops.parquet\",\n",
        "        }\n",
        "\n",
        "def get_metrics_dir(scenario_id: str, dataset: str) -> Path:\n",
        "    \"\"\"Get the correct metrics directory based on dataset.\"\"\"\n",
        "    if dataset == \"lemma\":\n",
        "        return ROOT / scenario_id  # LEMMA has metrics in multimodal dir\n",
        "    else:\n",
        "        return METRICS_BASE / scenario_id  # Nezha has separate metrics dir\n",
        "\n",
        "def get_logs_path(scenario_id: str, dataset: str) -> Path:\n",
        "    \"\"\"Get the correct logs file path based on dataset.\"\"\"\n",
        "    if dataset == \"lemma\":\n",
        "        return None  # Will be read from manifest\n",
        "    else:\n",
        "        return LOGS_BASE / scenario_id / \"logs_service_texts.parquet\"\n",
        "\n",
        "# Discover all scenarios\n",
        "all_scenarios = []\n",
        "lemma_count = 0\n",
        "nezha_count = 0\n",
        "excluded_count = 0\n",
        "\n",
        "for d in sorted(ROOT.iterdir()):\n",
        "    if d.is_dir():\n",
        "        has_manifest = (d / \"manifest.json\").exists()\n",
        "        has_gt = (d / \"ground_truth.json\").exists()\n",
        "        \n",
        "        if has_manifest and has_gt:\n",
        "            dataset = detect_dataset(d.name)\n",
        "            \n",
        "            if d.name in EXCLUDED_SCENARIOS:\n",
        "                excluded_count += 1\n",
        "                print(f\"[EXCLUDED] {d.name}\")\n",
        "                continue\n",
        "            \n",
        "            if dataset == \"lemma\":\n",
        "                lemma_count += 1\n",
        "            elif dataset == \"nezha\":\n",
        "                nezha_count += 1\n",
        "            \n",
        "            all_scenarios.append({\"dir\": d, \"id\": d.name, \"dataset\": dataset})\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"SCENARIO DISCOVERY SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"LEMMA scenarios:    {lemma_count}\")\n",
        "print(f\"Nezha scenarios:    {nezha_count}\")\n",
        "print(f\"Excluded:           {excluded_count}\")\n",
        "print(f\"TOTAL valid:        {len(all_scenarios)}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SCENARIO DISCOVERY SUMMARY\n",
            "============================================================\n",
            "LEMMA scenarios:    4\n",
            "Nezha scenarios:    94\n",
            "Excluded:           0\n",
            "TOTAL valid:        98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_json(path):\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_manifest_and_gt(scen_dir):\n",
        "    manifest = load_json(scen_dir / \"manifest.json\")\n",
        "    gt = load_json(scen_dir / \"ground_truth.json\")\n",
        "    return manifest, gt\n",
        "\n",
        "def reconstruct_ts_index(time_start, time_end, n_timesteps):\n",
        "    start = pd.Timestamp(time_start)\n",
        "    end = pd.Timestamp(time_end)\n",
        "    return pd.date_range(start=start, end=end, periods=n_timesteps)\n",
        "\n",
        "def safe_std(arr, min_samples=3):\n",
        "    \"\"\"Calculate std safely. Returns None if not enough data.\n",
        "    Note: min_samples=3 for Nezha which has short PRE windows (~3 min = 6 points at 30s).\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    valid = arr[~np.isnan(arr)]\n",
        "    if len(valid) < min_samples:\n",
        "        return None\n",
        "    s = np.std(valid)\n",
        "    if s == 0 or s < 1e-10:\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "def safe_mad(arr, min_samples=3):\n",
        "    \"\"\"Calculate MAD (Median Absolute Deviation) safely.\n",
        "    Note: min_samples=3 for Nezha which has short PRE windows.\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    valid = arr[~np.isnan(arr)]\n",
        "    if len(valid) < min_samples:\n",
        "        return None\n",
        "    median = np.median(valid)\n",
        "    mad = np.median(np.abs(valid - median))\n",
        "    if mad == 0 or mad < 1e-10:\n",
        "        return None\n",
        "    return mad\n",
        "\n",
        "def robust_zscore(value, median, mad):\n",
        "    \"\"\"Calculate robust z-score using MAD. Scale factor = 1.4826 for normal dist.\"\"\"\n",
        "    if mad is None or mad == 0:\n",
        "        return np.nan\n",
        "    return (value - median) / (1.4826 * mad)\n",
        "\n",
        "def cap_zscore(z, cap=25.0):\n",
        "    \"\"\"Cap z-score to reasonable range for display.\"\"\"\n",
        "    if np.isnan(z):\n",
        "        return np.nan\n",
        "    return np.clip(z, -cap, cap)\n",
        "\n",
        "def is_valid_metric(value, max_reasonable=1000):\n",
        "    \"\"\"Verify that a sigma/z-score value is reasonable.\"\"\"\n",
        "    if value is None or np.isnan(value):\n",
        "        return False\n",
        "    return abs(value) < max_reasonable\n",
        "\n",
        "def detect_root_service_log_col(logs_df, service_name):\n",
        "    \"\"\"Find the log column for a service.\"\"\"\n",
        "    if service_name in logs_df.columns:\n",
        "        return service_name\n",
        "    for col in logs_df.columns:\n",
        "        if service_name.lower() in col.lower():\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "print(\"✓ Utilities loaded\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "✓ Utilities loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TIER 1: Structural Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Keys required in both datasets\n",
        "COMMON_MANIFEST_KEYS = [\"scenario_id\", \"time_start\", \"time_end\", \"n_timesteps\", \"n_pods\", \"n_services\", \"pods\", \"services\"]\n",
        "COMMON_GT_KEYS = [\"fault_timestamp_raw\", \"fault_time_idx\", \"root_cause_service\", \"root_cause_pods\"]\n",
        "\n",
        "# Additional keys by dataset\n",
        "LEMMA_MANIFEST_KEYS = [\"metrics_files\", \"logs_texts_file\"]\n",
        "NEZHA_MANIFEST_KEYS = [\"metrics_files\", \"logs_texts_file\"]\n",
        "NEZHA_GT_KEYS = [\"fault_type\"]\n",
        "\n",
        "struct_results = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "    errors = []\n",
        "    \n",
        "    # Check common keys\n",
        "    for k in COMMON_MANIFEST_KEYS:\n",
        "        if k not in manifest:\n",
        "            errors.append(f\"manifest missing {k}\")\n",
        "    \n",
        "    for k in COMMON_GT_KEYS:\n",
        "        if k not in gt:\n",
        "            errors.append(f\"gt missing {k}\")\n",
        "    \n",
        "    # Check dataset-specific keys\n",
        "    if dataset == \"nezha\":\n",
        "        for k in NEZHA_GT_KEYS:\n",
        "            if k not in gt:\n",
        "                errors.append(f\"gt missing {k}\")\n",
        "    \n",
        "    # Coherence checks\n",
        "    if not errors:\n",
        "        if len(manifest[\"pods\"]) != manifest[\"n_pods\"]:\n",
        "            errors.append(f\"len(pods)={len(manifest['pods'])} != n_pods={manifest['n_pods']}\")\n",
        "        \n",
        "        if len(manifest[\"services\"]) != manifest[\"n_services\"]:\n",
        "            errors.append(f\"len(services)={len(manifest['services'])} != n_services={manifest['n_services']}\")\n",
        "        \n",
        "        # Check fault timestamp in range\n",
        "        fault_ts = pd.Timestamp(gt[\"fault_timestamp_raw\"])\n",
        "        ts_start = pd.Timestamp(manifest[\"time_start\"])\n",
        "        ts_end = pd.Timestamp(manifest[\"time_end\"])\n",
        "        if not (ts_start <= fault_ts <= ts_end):\n",
        "            errors.append(f\"fault_ts outside window\")\n",
        "        \n",
        "        # Check root cause pod exists\n",
        "        for rc_pod in gt[\"root_cause_pods\"]:\n",
        "            if rc_pod not in manifest[\"pods\"]:\n",
        "                errors.append(f\"rc_pod {rc_pod[:20]}... not in pods\")\n",
        "    \n",
        "    status = \"OK\" if not errors else \"; \".join(errors[:3])\n",
        "    struct_results.append({\n",
        "        \"scenario\": sid,\n",
        "        \"dataset\": dataset,\n",
        "        \"fault_type\": gt.get(\"fault_type\", \"N/A\"),\n",
        "        \"n_pods\": manifest.get(\"n_pods\", 0),\n",
        "        \"n_services\": manifest.get(\"n_services\", 0),\n",
        "        \"status\": status,\n",
        "        \"n_errors\": len(errors)\n",
        "    })\n",
        "\n",
        "struct_df = pd.DataFrame(struct_results)\n",
        "\n",
        "# Summary\n",
        "print(f\"{'='*70}\")\n",
        "print(\"STRUCTURAL VALIDATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for ds in [\"lemma\", \"nezha\"]:\n",
        "    ds_df = struct_df[struct_df[\"dataset\"] == ds]\n",
        "    if len(ds_df) == 0:\n",
        "        continue\n",
        "    n_ok = (ds_df[\"status\"] == \"OK\").sum()\n",
        "    print(f\"\\n{ds.upper()}: {n_ok}/{len(ds_df)} OK\")\n",
        "    if n_ok < len(ds_df):\n",
        "        print(\"  Errors:\")\n",
        "        for _, row in ds_df[ds_df[\"status\"] != \"OK\"].iterrows():\n",
        "            print(f\"    {row['scenario']}: {row['status'][:50]}\")\n",
        "\n",
        "struct_df.to_csv(OUTDIR / \"struct_validation.csv\", index=False)\n",
        "print(f\"\\nSaved: {OUTDIR / 'struct_validation.csv'}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STRUCTURAL VALIDATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "LEMMA: 4/4 OK\n",
            "\n",
            "NEZHA: 94/94 OK\n",
            "\n",
            "Saved: unified_rca_reports/struct_validation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TIER 1: Temporal Alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "temporal_results = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "    errors = []\n",
        "    \n",
        "    try:\n",
        "        ts_start = pd.Timestamp(manifest[\"time_start\"])\n",
        "        ts_end = pd.Timestamp(manifest[\"time_end\"])\n",
        "        n_ts = manifest[\"n_timesteps\"]\n",
        "        \n",
        "        ts_index = reconstruct_ts_index(manifest[\"time_start\"], manifest[\"time_end\"], n_ts)\n",
        "        \n",
        "        # Check monotonicity\n",
        "        if not ts_index.is_monotonic_increasing:\n",
        "            errors.append(\"index not monotonic\")\n",
        "        \n",
        "        # Check step consistency (should be 30s)\n",
        "        diffs = ts_index.to_series().diff().dropna()\n",
        "        expected_step = pd.Timedelta(\"30s\")\n",
        "        max_deviation = (diffs - expected_step).abs().max()\n",
        "        if max_deviation > pd.Timedelta(\"1s\"):\n",
        "            errors.append(f\"step deviation: {max_deviation}\")\n",
        "        \n",
        "        # Check fault timestamp in window\n",
        "        fault_ts = pd.Timestamp(gt[\"fault_timestamp_raw\"])\n",
        "        if not (ts_start <= fault_ts <= ts_end):\n",
        "            errors.append(\"fault_ts outside window\")\n",
        "        \n",
        "        # Check fault_time_idx validity\n",
        "        fault_idx = gt[\"fault_time_idx\"]\n",
        "        if not (0 <= fault_idx < n_ts):\n",
        "            errors.append(f\"fault_idx={fault_idx} out of range [0,{n_ts})\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        errors.append(f\"exception: {str(e)[:30]}\")\n",
        "    \n",
        "    status = \"OK\" if not errors else \"; \".join(errors)\n",
        "    temporal_results.append({\n",
        "        \"scenario\": sid,\n",
        "        \"dataset\": dataset,\n",
        "        \"n_timesteps\": manifest.get(\"n_timesteps\", 0),\n",
        "        \"duration_min\": (ts_end - ts_start).total_seconds() / 60 if 'ts_end' in dir() else 0,\n",
        "        \"fault_idx\": gt.get(\"fault_time_idx\", -1),\n",
        "        \"status\": status\n",
        "    })\n",
        "\n",
        "temporal_df = pd.DataFrame(temporal_results)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"TEMPORAL ALIGNMENT SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for ds in [\"lemma\", \"nezha\"]:\n",
        "    ds_df = temporal_df[temporal_df[\"dataset\"] == ds]\n",
        "    if len(ds_df) == 0:\n",
        "        continue\n",
        "    n_ok = (ds_df[\"status\"] == \"OK\").sum()\n",
        "    print(f\"\\n{ds.upper()}: {n_ok}/{len(ds_df)} OK\")\n",
        "    if n_ok < len(ds_df):\n",
        "        print(\"  Errors:\")\n",
        "        for _, row in ds_df[ds_df[\"status\"] != \"OK\"].iterrows():\n",
        "            print(f\"    {row['scenario']}: {row['status'][:50]}\")\n",
        "\n",
        "temporal_df.to_csv(OUTDIR / \"temporal_validation.csv\", index=False)\n",
        "print(f\"\\nSaved: {OUTDIR / 'temporal_validation.csv'}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TEMPORAL ALIGNMENT SUMMARY\n",
            "======================================================================\n",
            "\n",
            "LEMMA: 4/4 OK\n",
            "\n",
            "NEZHA: 94/94 OK\n",
            "\n",
            "Saved: unified_rca_reports/temporal_validation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TIER 2: Metric Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_eval_results = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "    fault_type = gt.get(\"fault_type\", \"N/A\")\n",
        "    \n",
        "    fault_ts = pd.Timestamp(gt[\"fault_timestamp_raw\"])\n",
        "    fault_bin = fault_ts.floor(\"30s\")\n",
        "    \n",
        "    # Get actual scenario time bounds\n",
        "    ts_start = pd.Timestamp(manifest[\"time_start\"])\n",
        "    ts_end = pd.Timestamp(manifest[\"time_end\"])\n",
        "    \n",
        "    rc_pods = gt[\"root_cause_pods\"]\n",
        "    metrics_dir = get_metrics_dir(sid, dataset)\n",
        "    metric_files = get_metric_files_map(dataset)\n",
        "    \n",
        "    # Windows based on dataset - USE AVAILABLE DATA\n",
        "    if dataset == \"lemma\":\n",
        "        # LEMMA has long windows, use fixed offsets\n",
        "        pre_start = fault_bin - 2 * WINDOW_BEFORE\n",
        "        pre_end = fault_bin - WINDOW_BEFORE\n",
        "        near_start = fault_bin - WINDOW_BEFORE\n",
        "        near_end = fault_bin + WINDOW_AFTER\n",
        "    else:\n",
        "        # Nezha has short windows (PRE~3min, POST~7min)\n",
        "        # Use all available PRE data as baseline, excluding 30s before fault\n",
        "        pre_start = ts_start\n",
        "        pre_end = fault_bin - pd.Timedelta(\"30s\")\n",
        "        # NEAR = from fault_bin to end of scenario\n",
        "        near_start = fault_bin\n",
        "        near_end = ts_end\n",
        "    \n",
        "    for metric_key in [\"cpu\", \"memory\", \"rx_bytes\", \"tx_bytes\"]:\n",
        "        metric_file = metric_files.get(metric_key)\n",
        "        if not metric_file:\n",
        "            continue\n",
        "        \n",
        "        metric_path = metrics_dir / metric_file\n",
        "        if not metric_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_parquet(metric_path)\n",
        "            \n",
        "            for rc_pod in rc_pods:\n",
        "                if rc_pod not in df.columns:\n",
        "                    continue\n",
        "                \n",
        "                series = df[rc_pod]\n",
        "                \n",
        "                pre_vals = series.loc[pre_start:pre_end].values\n",
        "                near_vals = series.loc[near_start:near_end].values\n",
        "                \n",
        "                pre_mean = np.nanmean(pre_vals) if len(pre_vals) > 0 else np.nan\n",
        "                near_mean = np.nanmean(near_vals) if len(near_vals) > 0 else np.nan\n",
        "                \n",
        "                std = safe_std(pre_vals)\n",
        "                shift_sigma = (near_mean - pre_mean) / std if std else np.nan\n",
        "                \n",
        "                # z-score in window\n",
        "                mad = safe_mad(pre_vals)\n",
        "                median = np.nanmedian(pre_vals) if len(pre_vals) > 0 else np.nan\n",
        "                \n",
        "                z_scores = []\n",
        "                for v in near_vals:\n",
        "                    if not np.isnan(v):\n",
        "                        z = robust_zscore(v, median, mad)\n",
        "                        if not np.isnan(z):\n",
        "                            z_scores.append(abs(z))\n",
        "                \n",
        "                max_z = max(z_scores) if z_scores else np.nan\n",
        "                \n",
        "                metrics_eval_results.append({\n",
        "                    \"scenario\": sid,\n",
        "                    \"dataset\": dataset,\n",
        "                    \"fault_type\": fault_type,\n",
        "                    \"metric\": metric_key,\n",
        "                    \"pod\": rc_pod[:40],\n",
        "                    \"pre_mean\": pre_mean,\n",
        "                    \"near_mean\": near_mean,\n",
        "                    \"shift_sigma\": shift_sigma if is_valid_metric(shift_sigma) else np.nan,\n",
        "                    \"max_z_window\": cap_zscore(max_z) if is_valid_metric(max_z) else np.nan,\n",
        "                })\n",
        "        \n",
        "        except Exception as e:\n",
        "            pass  # Skip metrics with errors\n",
        "\n",
        "metrics_eval_df = pd.DataFrame(metrics_eval_results)\n",
        "metrics_eval_df.to_csv(OUTDIR / \"metrics_window_eval.csv\", index=False)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"METRIC EVALUATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for ds in [\"lemma\", \"nezha\"]:\n",
        "    ds_df = metrics_eval_df[metrics_eval_df[\"dataset\"] == ds]\n",
        "    if len(ds_df) == 0:\n",
        "        continue\n",
        "    print(f\"\\n{ds.upper()}:\")\n",
        "    print(f\"  Metrics evaluated: {len(ds_df)}\")\n",
        "    \n",
        "    valid_shifts = ds_df[\"shift_sigma\"].dropna()\n",
        "    if len(valid_shifts) > 0:\n",
        "        strong = (valid_shifts.abs() >= 3.0).sum()\n",
        "        print(f\"  Strong shift (|σ| ≥ 3): {strong} ({100*strong/len(valid_shifts):.0f}%)\")\n",
        "    \n",
        "    valid_z = ds_df[\"max_z_window\"].dropna()\n",
        "    if len(valid_z) > 0:\n",
        "        high_z = (valid_z >= 5.0).sum()\n",
        "        print(f\"  High z-score (z ≥ 5): {high_z} ({100*high_z/len(valid_z):.0f}%)\")\n",
        "\n",
        "print(f\"\\nSaved: {OUTDIR / 'metrics_window_eval.csv'}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "METRIC EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "NEZHA:\n",
            "  Metrics evaluated: 376\n",
            "  Strong shift (|σ| ≥ 3): 101 (30%)\n",
            "  High z-score (z ≥ 5): 181 (59%)\n",
            "\n",
            "Saved: unified_rca_reports/metrics_window_eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TIER 2: Log Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STRICT_ERROR_KEYWORDS = [\n",
        "    'error', 'exception', 'failed', 'failure', 'timeout', 'refused',\n",
        "    'unavailable', 'unreachable', 'denied', 'rejected', 'crash',\n",
        "    '500', '502', '503', '504', 'critical', 'fatal', 'panic', 'traceback'\n",
        "]\n",
        "EXCLUDE_PATTERNS = ['deprecated', 'health', 'kube-probe', 'liveness', 'readiness']\n",
        "\n",
        "def count_strict_errors(texts):\n",
        "    count = 0\n",
        "    for t in texts:\n",
        "        t_lower = str(t).lower()\n",
        "        if any(ex in t_lower for ex in EXCLUDE_PATTERNS):\n",
        "            continue\n",
        "        if any(kw in t_lower for kw in STRICT_ERROR_KEYWORDS):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "_LOG_NORM_PATTERNS = [\n",
        "    (re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'), '<IP>'),\n",
        "    (re.compile(r'\\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\b', re.I), '<UUID>'),\n",
        "    (re.compile(r'\\b\\d+\\b'), '<NUM>'),\n",
        "    (re.compile(r'-[a-z0-9]{5,10}-[a-z0-9]{5}'), '<POD>'),\n",
        "]\n",
        "\n",
        "def normalize_log_template(text):\n",
        "    t = str(text)\n",
        "    for pattern, replacement in _LOG_NORM_PATTERNS:\n",
        "        t = pattern.sub(replacement, t)\n",
        "    return t[:100].strip()\n",
        "\n",
        "def count_unique_templates(texts):\n",
        "    templates = set()\n",
        "    for t in texts:\n",
        "        template = normalize_log_template(t)\n",
        "        if template:\n",
        "            templates.add(template)\n",
        "    return len(templates)\n",
        "\n",
        "logs_eval_results = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "    fault_type = gt.get(\"fault_type\", \"N/A\")\n",
        "    \n",
        "    fault_ts = pd.Timestamp(gt[\"fault_timestamp_raw\"])\n",
        "    fault_bin = fault_ts.floor(\"30s\")\n",
        "    \n",
        "    # Get actual scenario time bounds\n",
        "    ts_start = pd.Timestamp(manifest[\"time_start\"])\n",
        "    ts_end = pd.Timestamp(manifest[\"time_end\"])\n",
        "    \n",
        "    # Windows - USE AVAILABLE DATA\n",
        "    if dataset == \"lemma\":\n",
        "        pre_start = fault_bin - 2 * WINDOW_BEFORE\n",
        "        pre_end = fault_bin - WINDOW_BEFORE\n",
        "        near_start = fault_bin - WINDOW_BEFORE\n",
        "        near_end = fault_bin + WINDOW_AFTER\n",
        "    else:\n",
        "        # Nezha: use all available PRE data, excluding 30s before fault\n",
        "        pre_start = ts_start\n",
        "        pre_end = fault_bin - pd.Timedelta(\"30s\")\n",
        "        near_start = fault_bin\n",
        "        near_end = ts_end\n",
        "    \n",
        "    rc_service = gt[\"root_cause_service\"]\n",
        "    \n",
        "    # Get logs path\n",
        "    if dataset == \"lemma\":\n",
        "        logs_path = Path(manifest.get(\"logs_texts_file\", \"\"))\n",
        "    else:\n",
        "        logs_path = LOGS_BASE / sid / \"logs_service_texts.parquet\"\n",
        "    \n",
        "    if not logs_path or not logs_path.exists():\n",
        "        logs_eval_results.append({\n",
        "            \"scenario\": sid, \"dataset\": dataset, \"fault_type\": fault_type,\n",
        "            \"status\": \"logs file not found\"\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    logs_df = pd.read_parquet(logs_path)\n",
        "    log_col = detect_root_service_log_col(logs_df, rc_service)\n",
        "    \n",
        "    if not log_col:\n",
        "        logs_eval_results.append({\n",
        "            \"scenario\": sid, \"dataset\": dataset, \"fault_type\": fault_type,\n",
        "            \"status\": f\"service {rc_service} not in logs\"\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    pre_logs = logs_df.loc[pre_start:pre_end, log_col].dropna()\n",
        "    near_logs = logs_df.loc[near_start:near_end, log_col].dropna()\n",
        "    \n",
        "    pre_texts = [str(t) for t in pre_logs.values if str(t).strip()]\n",
        "    near_texts = [str(t) for t in near_logs.values if str(t).strip()]\n",
        "    \n",
        "    vol_pre = sum(len(t) for t in pre_texts)\n",
        "    vol_near = sum(len(t) for t in near_texts)\n",
        "    delta_vol = (vol_near - vol_pre) / max(vol_pre, 1) * 100\n",
        "    \n",
        "    errors_pre = count_strict_errors(pre_texts)\n",
        "    errors_near = count_strict_errors(near_texts)\n",
        "    \n",
        "    templates_pre = count_unique_templates(pre_texts)\n",
        "    templates_near = count_unique_templates(near_texts)\n",
        "    new_templates = max(0, templates_near - templates_pre)\n",
        "    \n",
        "    has_log_change = (errors_near > errors_pre or new_templates > 2 or abs(delta_vol) > 100)\n",
        "    \n",
        "    logs_eval_results.append({\n",
        "        \"scenario\": sid,\n",
        "        \"dataset\": dataset,\n",
        "        \"fault_type\": fault_type,\n",
        "        \"log_col\": log_col,\n",
        "        \"vol_pre\": vol_pre,\n",
        "        \"vol_near\": vol_near,\n",
        "        \"delta_vol_pct\": delta_vol,\n",
        "        \"errors_pre\": errors_pre,\n",
        "        \"errors_near\": errors_near,\n",
        "        \"templates_pre\": templates_pre,\n",
        "        \"templates_near\": templates_near,\n",
        "        \"new_templates\": new_templates,\n",
        "        \"has_log_change\": has_log_change,\n",
        "        \"status\": \"OK\"\n",
        "    })\n",
        "\n",
        "logs_eval_df = pd.DataFrame(logs_eval_results)\n",
        "logs_eval_df.to_csv(OUTDIR / \"logs_window_eval.csv\", index=False)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"LOG EVALUATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for ds in [\"lemma\", \"nezha\"]:\n",
        "    ds_df = logs_eval_df[logs_eval_df[\"dataset\"] == ds]\n",
        "    if len(ds_df) == 0:\n",
        "        continue\n",
        "    ok_df = ds_df[ds_df[\"status\"] == \"OK\"]\n",
        "    print(f\"\\n{ds.upper()}:\")\n",
        "    print(f\"  Processed: {len(ok_df)}/{len(ds_df)}\")\n",
        "    if len(ok_df) > 0:\n",
        "        print(f\"  With log change: {ok_df['has_log_change'].sum()} ({100*ok_df['has_log_change'].mean():.0f}%)\")\n",
        "\n",
        "print(f\"\\nSaved: {OUTDIR / 'logs_window_eval.csv'}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LOG EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "LEMMA:\n",
            "  Processed: 4/4\n",
            "  With log change: 4 (100%)\n",
            "\n",
            "NEZHA:\n",
            "  Processed: 94/94\n",
            "  With log change: 73 (78%)\n",
            "\n",
            "Saved: unified_rca_reports/logs_window_eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A/B/C/D Classification\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "METRIC_THRESHOLD_SHIFT = 3.0\n",
        "METRIC_THRESHOLD_Z = 5.0\n",
        "\n",
        "quality_results = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "    fault_type = gt.get(\"fault_type\", \"N/A\")\n",
        "    \n",
        "    # Metrics strength\n",
        "    scen_metrics = metrics_eval_df[metrics_eval_df[\"scenario\"] == sid]\n",
        "    metrics_strong = False\n",
        "    metrics_reasons = []\n",
        "    \n",
        "    if len(scen_metrics) > 0:\n",
        "        valid_metrics = scen_metrics[scen_metrics[\"shift_sigma\"].notna()]\n",
        "        if len(valid_metrics) > 0:\n",
        "            shifts = valid_metrics[\"shift_sigma\"].abs()\n",
        "            has_shift = (shifts >= METRIC_THRESHOLD_SHIFT).any()\n",
        "            if has_shift:\n",
        "                max_shift = shifts.max()\n",
        "                metrics_reasons.append(f\"shift={max_shift:.1f}σ\")\n",
        "            \n",
        "            z_vals = valid_metrics[\"max_z_window\"].dropna()\n",
        "            has_z = (z_vals >= METRIC_THRESHOLD_Z).any() if len(z_vals) > 0 else False\n",
        "            if has_z:\n",
        "                max_z = z_vals.max()\n",
        "                metrics_reasons.append(f\"z={max_z:.1f}\")\n",
        "            \n",
        "            metrics_strong = has_shift or has_z\n",
        "    \n",
        "    # Logs strength\n",
        "    scen_logs = logs_eval_df[logs_eval_df[\"scenario\"] == sid]\n",
        "    logs_strong = False\n",
        "    logs_reasons = []\n",
        "    \n",
        "    if len(scen_logs) > 0 and scen_logs.iloc[0].get(\"status\") == \"OK\":\n",
        "        row = scen_logs.iloc[0]\n",
        "        logs_strong = row.get(\"has_log_change\", False)\n",
        "        \n",
        "        if row.get(\"errors_near\", 0) > row.get(\"errors_pre\", 0):\n",
        "            logs_reasons.append(f\"errors:{row['errors_pre']}->{row['errors_near']}\")\n",
        "        if row.get(\"new_templates\", 0) > 2:\n",
        "            logs_reasons.append(f\"new_tmpl={row['new_templates']}\")\n",
        "    \n",
        "    # Classification\n",
        "    if metrics_strong and logs_strong:\n",
        "        classification = \"A\"\n",
        "    elif metrics_strong and not logs_strong:\n",
        "        classification = \"B\"\n",
        "    elif not metrics_strong and logs_strong:\n",
        "        classification = \"C\"\n",
        "    else:\n",
        "        classification = \"D\"\n",
        "    \n",
        "    quality_results.append({\n",
        "        \"scenario\": sid,\n",
        "        \"dataset\": dataset,\n",
        "        \"fault_type\": fault_type,\n",
        "        \"metrics_strong\": bool(metrics_strong),\n",
        "        \"metrics_reasons\": \", \".join(metrics_reasons) if metrics_reasons else \"none\",\n",
        "        \"logs_strong\": bool(logs_strong),\n",
        "        \"logs_reasons\": \", \".join(logs_reasons) if logs_reasons else \"none\",\n",
        "        \"classification\": classification\n",
        "    })\n",
        "\n",
        "quality_df = pd.DataFrame(quality_results)\n",
        "quality_df.to_csv(OUTDIR / \"scenario_quality.csv\", index=False)\n",
        "\n",
        "# Save as JSON\n",
        "quality_dict = {r[\"scenario\"]: r for r in quality_results}\n",
        "with open(OUTDIR / \"scenario_quality.json\", \"w\") as f:\n",
        "    json.dump(quality_dict, f, indent=2)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"A/B/C/D CLASSIFICATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for ds in [\"lemma\", \"nezha\"]:\n",
        "    ds_df = quality_df[quality_df[\"dataset\"] == ds]\n",
        "    if len(ds_df) == 0:\n",
        "        continue\n",
        "    print(f\"\\n{ds.upper()} ({len(ds_df)} scenarios):\")\n",
        "    for cls in ['A', 'B', 'C', 'D']:\n",
        "        n = (ds_df['classification'] == cls).sum()\n",
        "        pct = 100 * n / len(ds_df)\n",
        "        print(f\"  {cls}: {n:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"COMBINED SUMMARY:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(quality_df[\"classification\"].value_counts().sort_index())\n",
        "\n",
        "print(f\"\\nSaved: {OUTDIR / 'scenario_quality.csv'} and {OUTDIR / 'scenario_quality.json'}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "A/B/C/D CLASSIFICATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "LEMMA (4 scenarios):\n",
            "  A:   0 (  0.0%)\n",
            "  B:   0 (  0.0%)\n",
            "  C:   4 (100.0%)\n",
            "  D:   0 (  0.0%)\n",
            "\n",
            "NEZHA (94 scenarios):\n",
            "  A:  69 ( 73.4%)\n",
            "  B:  20 ( 21.3%)\n",
            "  C:   4 (  4.3%)\n",
            "  D:   1 (  1.1%)\n",
            "\n",
            "======================================================================\n",
            "COMBINED SUMMARY:\n",
            "======================================================================\n",
            "classification\n",
            "A    69\n",
            "B    20\n",
            "C     8\n",
            "D     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Saved: unified_rca_reports/scenario_quality.csv and unified_rca_reports/scenario_quality.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combine all results\n",
        "summary_df = quality_df[[\"scenario\", \"dataset\", \"fault_type\", \"classification\", \"metrics_strong\", \"logs_strong\"]].copy()\n",
        "\n",
        "# Add validation status\n",
        "summary_df = summary_df.merge(\n",
        "    struct_df[[\"scenario\", \"status\"]].rename(columns={\"status\": \"struct_status\"}),\n",
        "    on=\"scenario\", how=\"left\"\n",
        ")\n",
        "summary_df = summary_df.merge(\n",
        "    temporal_df[[\"scenario\", \"n_timesteps\", \"status\"]].rename(columns={\"status\": \"temporal_status\"}),\n",
        "    on=\"scenario\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Add max shift\n",
        "max_shifts = metrics_eval_df.groupby(\"scenario\")[\"shift_sigma\"].apply(\n",
        "    lambda x: x.abs().max()\n",
        ").reset_index()\n",
        "max_shifts.columns = [\"scenario\", \"max_shift_sigma\"]\n",
        "summary_df = summary_df.merge(max_shifts, on=\"scenario\", how=\"left\")\n",
        "\n",
        "summary_df.to_csv(OUTDIR / \"report_summary.csv\", index=False)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"FINAL UNIFIED VALIDATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nTotal scenarios validated: {len(summary_df)}\")\n",
        "print(f\"  LEMMA:  {len(summary_df[summary_df['dataset'] == 'lemma'])}\")\n",
        "print(f\"  Nezha:  {len(summary_df[summary_df['dataset'] == 'nezha'])}\")\n",
        "\n",
        "print(f\"\\nStructural validation:\")\n",
        "print(f\"  OK: {(summary_df['struct_status'] == 'OK').sum()}\")\n",
        "\n",
        "print(f\"\\nTemporal validation:\")\n",
        "print(f\"  OK: {(summary_df['temporal_status'] == 'OK').sum()}\")\n",
        "\n",
        "print(f\"\\nClassification (all datasets):\")\n",
        "for cls in ['A', 'B', 'C', 'D']:\n",
        "    n = (summary_df['classification'] == cls).sum()\n",
        "    pct = 100 * n / len(summary_df)\n",
        "    print(f\"  {cls}: {n:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"TRAINING RECOMMENDATIONS\")\n",
        "print(f\"{'='*70}\")\n",
        "n_a = (summary_df['classification'] == 'A').sum()\n",
        "n_b = (summary_df['classification'] == 'B').sum()\n",
        "n_c = (summary_df['classification'] == 'C').sum()\n",
        "n_d = (summary_df['classification'] == 'D').sum()\n",
        "\n",
        "print(f\"\\n✅ Class A (ideal multimodal): {n_a} scenarios\")\n",
        "print(f\"✅ Class B (metrics focus):    {n_b} scenarios\")\n",
        "print(f\"✅ Class C (logs focus):       {n_c} scenarios\")\n",
        "print(f\"⚠️  Class D (consider exclude): {n_d} scenarios\")\n",
        "print(f\"\\nTotal trainable (A+B+C): {n_a + n_b + n_c} scenarios\")\n",
        "\n",
        "print(f\"\\nSaved: {OUTDIR / 'report_summary.csv'}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FINAL UNIFIED VALIDATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Total scenarios validated: 98\n",
            "  LEMMA:  4\n",
            "  Nezha:  94\n",
            "\n",
            "Structural validation:\n",
            "  OK: 98\n",
            "\n",
            "Temporal validation:\n",
            "  OK: 98\n",
            "\n",
            "Classification (all datasets):\n",
            "  A:  69 ( 70.4%)\n",
            "  B:  20 ( 20.4%)\n",
            "  C:   8 (  8.2%)\n",
            "  D:   1 (  1.0%)\n",
            "\n",
            "======================================================================\n",
            "TRAINING RECOMMENDATIONS\n",
            "======================================================================\n",
            "\n",
            "✅ Class A (ideal multimodal): 69 scenarios\n",
            "✅ Class B (metrics focus):    20 scenarios\n",
            "✅ Class C (logs focus):       8 scenarios\n",
            "⚠️  Class D (consider exclude): 1 scenarios\n",
            "\n",
            "Total trainable (A+B+C): 97 scenarios\n",
            "\n",
            "Saved: unified_rca_reports/report_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sanity Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_ok = True\n",
        "errors = []\n",
        "\n",
        "for scen in all_scenarios:\n",
        "    scen_dir = scen[\"dir\"]\n",
        "    sid = scen[\"id\"]\n",
        "    dataset = scen[\"dataset\"]\n",
        "    \n",
        "    try:\n",
        "        manifest, gt = load_manifest_and_gt(scen_dir)\n",
        "        \n",
        "        ts_index = reconstruct_ts_index(manifest[\"time_start\"], manifest[\"time_end\"], manifest[\"n_timesteps\"])\n",
        "        assert ts_index.is_monotonic_increasing, f\"{sid}: ts_index not monotonic\"\n",
        "        \n",
        "        fault_ts = pd.Timestamp(gt[\"fault_timestamp_raw\"])\n",
        "        fault_bin = fault_ts.floor(\"30s\")\n",
        "        window_start = fault_bin - WINDOW_BEFORE\n",
        "        window_end = fault_bin + WINDOW_AFTER\n",
        "        window_mask = (ts_index >= window_start) & (ts_index <= window_end)\n",
        "        assert window_mask.sum() > 0, f\"{sid}: window empty\"\n",
        "        \n",
        "        rc_pods = gt[\"root_cause_pods\"]\n",
        "        metrics_dir = get_metrics_dir(sid, dataset)\n",
        "        metric_files = get_metric_files_map(dataset)\n",
        "        \n",
        "        cpu_path = metrics_dir / metric_files[\"cpu\"]\n",
        "        if cpu_path.exists():\n",
        "            cpu_df = pd.read_parquet(cpu_path)\n",
        "            for pod in rc_pods:\n",
        "                assert pod in cpu_df.columns, f\"{sid}: rc_pod {pod[:20]}... not in metrics\"\n",
        "    \n",
        "    except AssertionError as e:\n",
        "        all_ok = False\n",
        "        errors.append(str(e))\n",
        "\n",
        "if all_ok:\n",
        "    print(\"✅ All sanity checks passed successfully\")\n",
        "else:\n",
        "    print(f\"❌ {len(errors)} checks failed:\")\n",
        "    for e in errors[:10]:\n",
        "        print(f\"  - {e}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "✅ All sanity checks passed successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}