{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMMA-RCA – Data Transformation\n",
        "\n",
        "Converts raw LEMMA data into the unified multimodal format: align metrics to 30s bins, crop to a 45-min fault window (PRE=15min, POST=30min), aggregate logs at service level, generate manifest + ground_truth JSONs.\n",
        "\n",
        "Cropping takes LEMMA from ~2600-5500 timesteps down to ~90. PPTX timestamps are JST, output is UTC.\n",
        "\n",
        "```\n",
        "core_metrics_tmp/{scenario}/     -> 4 metric parquets\n",
        "core_logs_tmp/{scenario}/        -> service-level log parquet\n",
        "core_multimodal_tmp/{scenario}/  -> manifest.json + ground_truth.json\n",
        "```\n",
        "\n",
        "**Ground truth note:** the pipeline writes ALL pods matching `root_cause_deployment`, but for training only the pod with the actual anomaly signal should be kept. Fix `ground_truth.json` manually after running:\n",
        "\n",
        "| Scenario | Default | Keep |\n",
        "|----------|---------|------|\n",
        "| 20231207 | both productpage pods | `productpage-v1-94d68db49-vc8ct` (10x CPU) |\n",
        "| 20231221 | all ratings pods | `ratings-aggregated` (252x CPU) |\n",
        "| 20240115 | single pod already | no change needed |\n",
        "| 20240215 | single pod already | no change needed |\n"
      ],
      "id": "3a71abd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Scenario Configuration\n",
        "\n",
        "Ground truth from PPTX/README files:\n",
        "\n",
        "| Scenario | Fault | Root Cause | Notes |\n",
        "|----------|-------|------------|-------|\n",
        "| 20231207 | CPU stress | productpage | PPTX time is JST |\n",
        "| 20231221 | Pod migration | ratings | PPTX time is JST |\n",
        "| 20240115 | Malware injection | scenario10 | unique service |\n",
        "| 20240215 | Network issue | details | reference format |\n"
      ],
      "id": "94058062"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = Path(\"/root/lemm\")\n",
        "\n",
        "# ============================================================================\n",
        "# CROPPING CONFIGURATION - Similar to Nezha for consistency\n",
        "# ============================================================================\n",
        "# LEMMA raw data has 2600-5500 timesteps (22-46 hours), but the fault signal\n",
        "# is concentrated near the fault time. Cropping to a smaller window:\n",
        "# - Dramatically reduces processing time (especially logs)\n",
        "# - Creates more consistent sequence lengths with Nezha (22 timesteps)\n",
        "# - Removes irrelevant \"baseline\" data far from the fault\n",
        "# ============================================================================\n",
        "PRE_FAULT_MINUTES = 15   # 30 bins before fault\n",
        "POST_FAULT_MINUTES = 30  # 60 bins after fault\n",
        "# Total: ~90 bins (45 minutes) vs original 2600-5500 bins\n",
        "\n",
        "SCENARIO_CONFIG = {\n",
        "    \"20240215\": {\n",
        "        \"fault_timestamp_raw\": pd.Timestamp(\"2024-02-14 06:11:29\", tz=\"UTC\"),\n",
        "        \"root_cause_deployment\": \"details-v1\",\n",
        "        \"root_cause_service\": \"details\",\n",
        "    },\n",
        "    \"20240115\": {               \n",
        "        \"fault_timestamp_raw\": pd.Timestamp(\"2024-01-14 07:26:39\", tz=\"UTC\"),\n",
        "        \"root_cause_deployment\": \"scenario10-malware-deployment\",\n",
        "        \"root_cause_service\": \"scenario10\",\n",
        "    },\n",
        "    \"20231221\": {\n",
        "        # PPTX states 11:32:05 JST -> converted to UTC = 02:32:05\n",
        "        \"fault_timestamp_raw\": pd.Timestamp(\"2023-12-20 02:32:05\", tz=\"UTC\"),\n",
        "        \"root_cause_deployment\": \"ratings\",\n",
        "        \"root_cause_service\": \"ratings\",\n",
        "    },\n",
        "    \"20231207\": {\n",
        "        # PPTX states 16:53:03 JST -> converted to UTC = 07:53:03\n",
        "        \"fault_timestamp_raw\": pd.Timestamp(\"2023-12-07 07:53:03\", tz=\"UTC\"),\n",
        "        \"root_cause_deployment\": \"productpage-v1\",\n",
        "        \"root_cause_service\": \"productpage\",\n",
        "    },\n",
        "}\n"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "1bbabd0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Processing Functions\n",
        "\n",
        "Metric loading/alignment, log cleaning + aggregation, and manifest generation.\n",
        "\n",
        "Log cleaner tokenizes variable content (timestamps → `<TS>`, UUIDs → `<UUID>`, IPs → `<HOST>`, etc.) for better model generalization. Known ports (`:8080`, `:443`) are preserved.\n"
      ],
      "id": "73f1b45e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "BASE_DIR = Path(\"/root/lemm\")\n",
        "\n",
        "\n",
        "def load_metric_npy(path: Path) -> tuple[pd.DatetimeIndex, pd.DataFrame]:\n",
        "    \"\"\"Load a metric NPY file and return a DataFrame with temporal index.\"\"\"\n",
        "    obj = np.load(path, allow_pickle=True).item()\n",
        "    root_key = next(iter(obj.keys()))\n",
        "    info = obj[root_key]\n",
        "\n",
        "    pod_names = info.get(\"Pod_Name\", info.get(\"Node_Name\"))\n",
        "    time_arr = info[\"time\"]\n",
        "    seq = info[\"Sequence\"]\n",
        "\n",
        "    if seq.ndim == 3:\n",
        "        seq = np.squeeze(seq, -1)\n",
        "\n",
        "    if seq.shape[0] == len(pod_names) and seq.shape[1] != len(pod_names):\n",
        "        seq = seq.T\n",
        "\n",
        "    T, D = seq.shape\n",
        "\n",
        "    if D == len(pod_names) + 1:\n",
        "        # FIX: Extra column is the LAST one (contains indices/timestamps, not CPU)\n",
        "        # Verified: col[-1] has anomalous values (4-1019) vs col[0] has normal CPU\n",
        "        seq = seq[:, :-1]\n",
        "        D = seq.shape[1]\n",
        "\n",
        "    if D != len(pod_names):\n",
        "        raise ValueError(\n",
        "            f\"Mismatch after adjustments: seq has {D} columns, pod_names has {len(pod_names)}\"\n",
        "        )\n",
        "\n",
        "    time_index = pd.to_datetime(time_arr, unit=\"s\", utc=True)\n",
        "    df = pd.DataFrame(seq, index=time_index, columns=pod_names)\n",
        "    return time_index, df\n",
        "\n",
        "\n",
        "METRIC_TYPES = {\n",
        "    \"pod_level_data_pod_cpu_usage_total\": \"gauge\",\n",
        "    \"pod_level_data_pod_memory_working_set\": \"gauge\",\n",
        "    \"pod_level_data_pod_network_rx_bytes\": \"rate\",\n",
        "    \"pod_level_data_pod_network_tx_bytes\": \"rate\",\n",
        "}\n",
        "\n",
        "def align_and_trim_metrics(scenario_id: str) -> tuple[Path, dict]:\n",
        "    \"\"\"\n",
        "    Align metrics to log_frequency grid with tolerance and validation.\n",
        "    \n",
        "    Returns:\n",
        "        trimmed_dir: Path to directory with aligned metrics\n",
        "        validation: dict with validation statistics\n",
        "    \"\"\"\n",
        "    metrics_dir = BASE_DIR / \"metrics_data\" / scenario_id\n",
        "    aligned_dir = BASE_DIR / \"aligned_metrics_tmp\" / scenario_id\n",
        "    trimmed_dir = BASE_DIR / \"trimmed_metrics_tmp\" / scenario_id\n",
        "    aligned_dir.mkdir(parents=True, exist_ok=True)\n",
        "    trimmed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if scenario_id == \"20240215\":\n",
        "        lf_name = \"0215_log_frequency_pod_level_removed.npy\"\n",
        "    else:\n",
        "        lf_name = f\"{scenario_id[-4:]}_log_frequency_pod_level_removed.npy\"\n",
        "    \n",
        "    lf_time, lf_df = load_metric_npy(metrics_dir / lf_name)\n",
        "    ref_index_full = lf_df.index\n",
        "    ref_step = (ref_index_full[1] - ref_index_full[0]).total_seconds() if len(ref_index_full) > 1 else 30\n",
        "\n",
        "    metric_files = [\n",
        "        \"pod_level_data_pod_cpu_usage_total.npy\",\n",
        "        \"pod_level_data_pod_memory_working_set.npy\",\n",
        "        \"pod_level_data_pod_network_rx_bytes.npy\",\n",
        "        \"pod_level_data_pod_network_tx_bytes.npy\",\n",
        "    ]\n",
        "    \n",
        "    all_starts = []\n",
        "    all_ends = []\n",
        "    for fname in metric_files:\n",
        "        _, df = load_metric_npy(metrics_dir / fname)\n",
        "        all_starts.append(df.index.min())\n",
        "        all_ends.append(df.index.max())\n",
        "    \n",
        "    overlap_start = max(all_starts)\n",
        "    overlap_end = min(all_ends)\n",
        "    ref_index = ref_index_full[(ref_index_full >= overlap_start) & (ref_index_full <= overlap_end)]\n",
        "    \n",
        "    lf_df_trimmed = lf_df.loc[ref_index]\n",
        "    lf_df_trimmed.to_parquet(aligned_dir / \"log_frequency.parquet\")\n",
        "    \n",
        "    validation = {\"scenario\": scenario_id, \"ref_step_s\": ref_step, \"n_ref_pts\": len(ref_index), \"metrics\": {}}\n",
        "\n",
        "    for fname in metric_files:\n",
        "        metric_name = fname.replace(\".npy\", \"\")\n",
        "        metric_type = METRIC_TYPES.get(metric_name, \"gauge\")\n",
        "        \n",
        "        _, df = load_metric_npy(metrics_dir / fname)\n",
        "        df = df.sort_index()\n",
        "        \n",
        "        if len(df) > 1:\n",
        "            steps = np.diff(df.index.astype(np.int64) // 1e9)\n",
        "            src_step = float(np.median(steps))\n",
        "        else:\n",
        "            src_step = ref_step\n",
        "        tolerance = pd.Timedelta(seconds=max(src_step, ref_step) / 2)\n",
        "        \n",
        "        if metric_type == \"rate\" and src_step < ref_step:\n",
        "            df_resampled = df.resample(f\"{int(ref_step)}s\").mean()\n",
        "            df_aligned = df_resampled.reindex(ref_index, method='nearest', tolerance=tolerance)\n",
        "        else:\n",
        "            df_aligned = df.reindex(ref_index, method='nearest', tolerance=tolerance)\n",
        "        \n",
        "        nan_before = df_aligned.isna().sum().sum()\n",
        "        total_cells = df_aligned.size\n",
        "        \n",
        "        df_aligned = df_aligned.ffill(limit=2).bfill(limit=2)\n",
        "        \n",
        "        nan_after = df_aligned.isna().sum().sum()\n",
        "        nan_ratio = nan_after / total_cells if total_cells > 0 else 0\n",
        "        imputed_ratio = (nan_before - nan_after) / total_cells if total_cells > 0 else 0\n",
        "        \n",
        "        orig_times = df.index\n",
        "        aligned_times = ref_index[ref_index.isin(df_aligned.dropna(how='all').index)]\n",
        "        if len(aligned_times) > 0 and len(orig_times) > 0:\n",
        "            drifts = []\n",
        "            for t in aligned_times[:100]:\n",
        "                idx = orig_times.get_indexer([t], method='nearest')[0]\n",
        "                if idx >= 0 and idx < len(orig_times):\n",
        "                    drift = abs((t - orig_times[idx]).total_seconds())\n",
        "                    drifts.append(drift)\n",
        "            max_drift = max(drifts) if drifts else 0\n",
        "        else:\n",
        "            max_drift = 0\n",
        "        \n",
        "        validation[\"metrics\"][metric_name] = {\n",
        "            \"src_step_s\": src_step,\n",
        "            \"tolerance_s\": tolerance.total_seconds(),\n",
        "            \"max_drift_s\": max_drift,\n",
        "            \"nan_ratio\": nan_ratio,\n",
        "            \"imputed_ratio\": imputed_ratio,\n",
        "            \"type\": metric_type,\n",
        "        }\n",
        "        \n",
        "        out_name = fname.replace(\".npy\", \".parquet\")\n",
        "        df_aligned.to_parquet(aligned_dir / out_name)\n",
        "\n",
        "    parquet_files = sorted(aligned_dir.glob(\"*.parquet\"))\n",
        "    dfs = {f.stem: pd.read_parquet(f) for f in parquet_files}\n",
        "    starts = {name: df.first_valid_index() for name, df in dfs.items() if df.first_valid_index() is not None}\n",
        "    global_start = max(starts.values()) if starts else ref_index[0]\n",
        "\n",
        "    for name, df in dfs.items():\n",
        "        trimmed_df = df.loc[global_start:]\n",
        "        trimmed_df.to_parquet(trimmed_dir / f\"{name}.parquet\")\n",
        "\n",
        "    return trimmed_dir, validation\n",
        "\n",
        "\n",
        "def select_core_metrics(trimmed_dir: Path, scenario_id: str) -> Path:\n",
        "    \"\"\"\n",
        "    Copy 4 core metrics and create 3 empty extras for unified schema.\n",
        "    \n",
        "    Output files (7 metrics + mask):\n",
        "        - pod_cpu_usage_total.parquet         [CORE - with data]\n",
        "        - pod_memory_working_set.parquet      [CORE - with data]\n",
        "        - pod_network_rx_bytes.parquet        [CORE - with data]\n",
        "        - pod_network_tx_bytes.parquet        [CORE - with data]\n",
        "        - pod_latency_server_p95.parquet      [EXTRA - NaN for LEMMA]\n",
        "        - pod_latency_client_p95.parquet      [EXTRA - NaN for LEMMA]\n",
        "        - pod_workload_ops.parquet            [EXTRA - NaN for LEMMA]\n",
        "        - metrics_mask.parquet                [1,1,1,1,0,0,0]\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "    \n",
        "    core_dir = BASE_DIR / \"core_metrics_tmp\" / scenario_id\n",
        "    core_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Mapping: old LEMMA names -> new unified names\n",
        "    core_metric_mapping = {\n",
        "        \"pod_level_data_pod_cpu_usage_total.parquet\": \"pod_cpu_usage_total.parquet\",\n",
        "        \"pod_level_data_pod_memory_working_set.parquet\": \"pod_memory_working_set.parquet\",\n",
        "        \"pod_level_data_pod_network_rx_bytes.parquet\": \"pod_network_rx_bytes.parquet\",\n",
        "        \"pod_level_data_pod_network_tx_bytes.parquet\": \"pod_network_tx_bytes.parquet\",\n",
        "    }\n",
        "\n",
        "    # Copy and rename CORE metrics\n",
        "    for old_name, new_name in core_metric_mapping.items():\n",
        "        src = trimmed_dir / old_name\n",
        "        dst = core_dir / new_name\n",
        "        shutil.copy2(src, dst)\n",
        "    \n",
        "    # Read one core metric to get shape (time_index, pods)\n",
        "    ref_df = pd.read_parquet(core_dir / \"pod_cpu_usage_total.parquet\")\n",
        "    \n",
        "    # Create EXTRA metrics (empty - all NaN) with same shape\n",
        "    extra_metrics = [\n",
        "        \"pod_latency_server_p95.parquet\",\n",
        "        \"pod_latency_client_p95.parquet\", \n",
        "        \"pod_workload_ops.parquet\",\n",
        "    ]\n",
        "    \n",
        "    for fname in extra_metrics:\n",
        "        empty_df = pd.DataFrame(\n",
        "            np.nan, \n",
        "            index=ref_df.index, \n",
        "            columns=ref_df.columns\n",
        "        )\n",
        "        empty_df.to_parquet(core_dir / fname)\n",
        "    \n",
        "    # NOTE: metrics_mask se generará después en el pipeline de entrenamiento\n",
        "    # basándose en los valores reales (NaN vs datos) de cada timestep/pod\n",
        "    \n",
        "    print(f\"  [metrics] Created 7 parquets in {core_dir}\")\n",
        "\n",
        "    return core_dir\n",
        "\n",
        "\n",
        "def build_pod_to_service_from_pods(pods: list[str]) -> dict[str, str]:\n",
        "    \"\"\"Extract service name from pod name.\"\"\"\n",
        "    pod_to_service = {}\n",
        "    for pod in pods:\n",
        "        parts = pod.split(\"-\")\n",
        "        if len(parts) >= 1:\n",
        "            pod_to_service[pod] = parts[0]\n",
        "    return pod_to_service\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# Whitelist of common ports to preserve (not replaced by <NUM>)\n",
        "_WHITELIST_PORTS = r'80|443|8080|8443|3000|3306|5432|5631|5672|5900|6379|7001|7070|8081|9000|9001|9090|9200|9300|9411|9999'\n",
        "\n",
        "\n",
        "def clean_log_content_vectorized(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Vectorized version of clean_log_content using pandas str methods.\n",
        "    \"\"\"\n",
        "    s = series.fillna('')\n",
        "    s = s.str.replace(r'\\x1b\\[[0-9;]*m', '', regex=True)\n",
        "    s = s.str.replace(r'\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z?', '<TS>', regex=True)\n",
        "    s = s.str.replace(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', '<UUID>', regex=True)\n",
        "    s = s.str.replace(r'\\b[a-fA-F0-9]{24,}\\b', '<HEX>', regex=True)\n",
        "    s = s.str.replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d{2,5})?', '<HOST>', regex=True)\n",
        "    s = s.str.replace(r'(?<=[a-zA-Z0-9])-[a-z0-9]{6,12}-[a-z0-9]{4,6}\\b', '-<RAND>', regex=True)\n",
        "    s = s.str.replace(r'(?<=/)\\d{4,}(?=/|$|\\s|\")', ':id', regex=True)\n",
        "    s = s.str.replace(r'(?i)(\\bport\\b|listen(?:ing)?(?:\\s+on)?|bind(?:ing)?(?:\\s+to)?|socket)[:\\s]+(\\d{2,5})', r'\\1 __PRT_\\2__', regex=True)\n",
        "    s = s.str.replace(rf':({_WHITELIST_PORTS})\\b', r':__PRT_\\1__', regex=True)\n",
        "    s = s.str.replace(r'(?i)(https?|status|code)[:\\s]*([1-5]\\d\\d)\\b', r'\\1 __HTTP_\\2__', regex=True)\n",
        "    s = s.str.replace(r'\\bv?(\\d+(?:\\.\\d+){1,3})\\b', r'__VER_\\1__', regex=True)\n",
        "    s = s.str.replace(r'\\b\\d{4,}\\b', '<NUM>', regex=True)\n",
        "    s = s.str.replace(r'__PRT_(\\d+)__', r'\\1', regex=True)\n",
        "    s = s.str.replace(r'__HTTP_(\\d+)__', r'\\1', regex=True)\n",
        "    s = s.str.replace(r'__VER_([^_]+)__', r'\\1', regex=True)\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def _process_chunk(chunk_data):\n",
        "    \"\"\"\n",
        "    Process a data chunk with early exit optimization.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (results_dict, status)\n",
        "        - results_dict: {(time_bin, content): count}\n",
        "        - status: 'ok', 'skip' (chunk before window), 'stop' (chunk after window)\n",
        "    \"\"\"\n",
        "    chunk, time_start, time_end = chunk_data\n",
        "    \n",
        "    # Parse timestamps first (fast check for early exit)\n",
        "    chunk[\"Time\"] = pd.to_datetime(chunk[\"Time\"], format='ISO8601', utc=True, errors='coerce')\n",
        "    chunk = chunk.dropna(subset=[\"Time\"])\n",
        "    \n",
        "    if len(chunk) == 0:\n",
        "        return {}, 'skip'\n",
        "    \n",
        "    # Early exit: if entire chunk is AFTER time_end, signal to stop reading\n",
        "    chunk_min_time = chunk[\"Time\"].min()\n",
        "    chunk_max_time = chunk[\"Time\"].max()\n",
        "    \n",
        "    if chunk_min_time > time_end:\n",
        "        return {}, 'stop'  # All subsequent chunks will also be after time_end\n",
        "    \n",
        "    # Early exit: if entire chunk is BEFORE time_start, skip it\n",
        "    if chunk_max_time < time_start:\n",
        "        return {}, 'skip'\n",
        "    \n",
        "    # Filter to window\n",
        "    chunk = chunk[(chunk[\"Time\"] >= time_start) & (chunk[\"Time\"] <= time_end)]\n",
        "    if len(chunk) == 0:\n",
        "        return {}, 'ok'\n",
        "    \n",
        "    chunk[\"TimeBin\"] = chunk[\"Time\"].dt.floor(\"30s\")\n",
        "    chunk[\"CleanContent\"] = clean_log_content_vectorized(chunk[\"Content\"])\n",
        "    grouped = chunk.groupby([\"TimeBin\", \"CleanContent\"]).size()\n",
        "    return {(t_bin, content): count for (t_bin, content), count in grouped.items() if content}, 'ok'\n",
        "\n",
        "\n",
        "def _process_single_log_file(args):\n",
        "    \"\"\"\n",
        "    Process a CSV file with early termination optimization.\n",
        "    \n",
        "    If logs are sorted by time (common), we stop reading when we pass time_end.\n",
        "    This can reduce read time by 95%+ when cropping to a small window.\n",
        "    \"\"\"\n",
        "    csv_path, time_start, time_end = args\n",
        "    CHUNK_SIZE = 500_000  # Smaller chunks for faster early termination check\n",
        "    \n",
        "    pod_name = csv_path.name.replace(\"_messages_structured.csv\", \"\").replace(\"_structured.csv\", \"\")\n",
        "    parts = pod_name.split(\"-\")\n",
        "    service = parts[0] if parts else pod_name\n",
        "    \n",
        "    local_bins = {}\n",
        "    chunks_read = 0\n",
        "    chunks_skipped = 0\n",
        "    \n",
        "    try:\n",
        "        chunk_iter = pd.read_csv(\n",
        "            csv_path,\n",
        "            encoding_errors='ignore',\n",
        "            chunksize=CHUNK_SIZE,\n",
        "            usecols=['Time', 'Content'],\n",
        "            dtype={'Time': str, 'Content': str},\n",
        "            engine='c',\n",
        "            low_memory=True,\n",
        "        )\n",
        "    except Exception:\n",
        "        return {}\n",
        "    \n",
        "    for chunk in chunk_iter:\n",
        "        chunks_read += 1\n",
        "        result, status = _process_chunk((chunk, time_start, time_end))\n",
        "        \n",
        "        if status == 'stop':\n",
        "            # Early termination: all remaining data is after our window\n",
        "            break\n",
        "        elif status == 'skip':\n",
        "            chunks_skipped += 1\n",
        "            continue\n",
        "        \n",
        "        # Merge results\n",
        "        for (t_bin, content), count in result.items():\n",
        "            key = (service, t_bin, content)\n",
        "            local_bins[key] = local_bins.get(key, 0) + count\n",
        "    \n",
        "    return local_bins\n",
        "\n",
        "\n",
        "def aggregate_logs_to_services(scenario_id: str, time_index: pd.DatetimeIndex, services: list[str]) -> Path:\n",
        "    \"\"\"Aggregate pod logs to service level using 20 CPU workers.\"\"\"\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    import os\n",
        "    \n",
        "    log_base = BASE_DIR / \"log_data\" / scenario_id / \"log_data\" / \"pod\"\n",
        "    out_dir = BASE_DIR / \"core_logs_tmp\" / scenario_id\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Sort by size DESCENDING: large files first for better parallelization\n",
        "    all_structured = sorted(log_base.glob(\"*_structured.csv\"), \n",
        "                            key=lambda p: p.stat().st_size, \n",
        "                            reverse=True)\n",
        "    total_files = len(all_structured)\n",
        "    print(f\"[INFO] Largest: {all_structured[0].name} ({all_structured[0].stat().st_size / 1e9:.1f} GB)\")\n",
        "    time_start = time_index[0]\n",
        "    time_end = time_index[-1]\n",
        "    \n",
        "    # Prepare arguments for each file\n",
        "    args_list = [(csv_path, time_start, time_end) for csv_path in all_structured]\n",
        "    \n",
        "    # Process in parallel (optimized for Ryzen 9 5900X: 24 threads)\n",
        "    n_workers = min(os.cpu_count() or 4, 20)\n",
        "    print(f\"[INFO] Processing {total_files} files with {n_workers} CPU workers...\")\n",
        "    \n",
        "    service_bins = defaultdict(lambda: defaultdict(Counter))\n",
        "    completed = 0\n",
        "    \n",
        "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
        "        futures = {executor.submit(_process_single_log_file, args): args[0] for args in args_list}\n",
        "        \n",
        "        for future in as_completed(futures):\n",
        "            completed += 1\n",
        "            if completed % 20 == 0:\n",
        "                print(f\"  Processing {completed}/{total_files} files...\")\n",
        "            \n",
        "            result = future.result()\n",
        "            # Combine results\n",
        "            for (service, t_bin, tmpl), count in result.items():\n",
        "                service_bins[service][t_bin][tmpl] += count\n",
        "\n",
        "    print(f\"[OK] Processed {total_files} files\")\n",
        "\n",
        "    # Build DataFrame of texts per service (column by column, more efficient)\n",
        "    time_bins = time_index.floor(\"30s\")  # Vectorized\n",
        "    \n",
        "    # Pre-build columns dict\n",
        "    columns_data = {}\n",
        "    for svc in services:\n",
        "        svc_bins = service_bins[svc]\n",
        "        col = []\n",
        "        for ts_bin in time_bins:\n",
        "            templates = svc_bins.get(ts_bin, {})\n",
        "            if templates:\n",
        "                text = \" | \".join(f\"{t}:x{c}\" for t, c in templates.most_common(10))\n",
        "            else:\n",
        "                text = \"\"\n",
        "            col.append(text)\n",
        "        columns_data[svc] = col\n",
        "    \n",
        "    logs_df = pd.DataFrame(columns_data, index=time_index)\n",
        "    out_path = out_dir / \"logs_service_texts.parquet\"\n",
        "    logs_df.to_parquet(out_path)\n",
        "\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def build_manifest_and_ground_truth(scenario_id: str, core_metrics_dir: Path, logs_texts_path: Path) -> Path:\n",
        "    \"\"\"Generate manifest.json and ground_truth.json for the scenario.\"\"\"\n",
        "    out_dir = BASE_DIR / \"core_multimodal_tmp\" / scenario_id\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cfg = SCENARIO_CONFIG[scenario_id]\n",
        "    fault_ts = cfg[\"fault_timestamp_raw\"]\n",
        "    rc_deployment = cfg[\"root_cause_deployment\"]\n",
        "    rc_service = cfg[\"root_cause_service\"]\n",
        "\n",
        "    cpu_df = pd.read_parquet(core_metrics_dir / \"pod_cpu_usage_total.parquet\")\n",
        "    pods = list(cpu_df.columns)\n",
        "    time_index = cpu_df.index\n",
        "\n",
        "    pod_to_service = build_pod_to_service_from_pods(pods)\n",
        "    services = sorted(set(pod_to_service.values()))\n",
        "    service_to_idx = {s: i for i, s in enumerate(services)}\n",
        "    pod_to_idx = {p: i for i, p in enumerate(pods)}\n",
        "\n",
        "    # Find root cause pods\n",
        "    rc_pods = [p for p in pods if rc_deployment in p]\n",
        "    rc_pod_indices = [pod_to_idx[p] for p in rc_pods]\n",
        "    rc_service_idx = service_to_idx.get(rc_service, -1)\n",
        "\n",
        "    # Calculate fault_bin and fault_time_idx\n",
        "    fault_bin = fault_ts.floor(\"30s\")\n",
        "    time_index_floored = time_index.floor(\"30s\")\n",
        "    \n",
        "    if fault_bin in time_index_floored:\n",
        "        fault_time_idx = int(time_index_floored.get_loc(fault_bin))\n",
        "    else:\n",
        "        fault_time_idx = int(np.searchsorted(time_index_floored, fault_bin))\n",
        "        fault_time_idx = min(fault_time_idx, len(time_index) - 1)\n",
        "\n",
        "    # Build pod_to_service_idx: array where pod_to_service_idx[pod_i] = service_idx\n",
        "    # This enables per-pod log fusion in the model\n",
        "    pod_to_service_idx = []\n",
        "    for pod in pods:\n",
        "        svc = pod_to_service.get(pod, pods[0].split(\"-\")[0])  # fallback to first part\n",
        "        svc_idx = service_to_idx.get(svc, 0)\n",
        "        pod_to_service_idx.append(svc_idx)\n",
        "\n",
        "    # Unified 7-metric schema (LEMMA + Nezha compatible)\n",
        "    all_metric_files = [\n",
        "        \"pod_cpu_usage_total.parquet\",\n",
        "        \"pod_memory_working_set.parquet\",\n",
        "        \"pod_network_rx_bytes.parquet\",\n",
        "        \"pod_network_tx_bytes.parquet\",\n",
        "        \"pod_latency_server_p95.parquet\",\n",
        "        \"pod_latency_client_p95.parquet\",\n",
        "        \"pod_workload_ops.parquet\",\n",
        "    ]\n",
        "    \n",
        "    manifest = {\n",
        "        \"scenario_id\": scenario_id,\n",
        "        \"dataset\": \"lemma\",  # Identifies source dataset\n",
        "        \"time_start\": str(time_index[0]),\n",
        "        \"time_end\": str(time_index[-1]),\n",
        "        \"n_timesteps\": len(time_index),\n",
        "        \"n_pods\": len(pods),\n",
        "        \"n_services\": len(services),\n",
        "        \"n_metrics\": 7,  # Unified schema: 4 core + 3 extras\n",
        "        \"window_T\": \"30s\",\n",
        "        \"metrics_files\": [str(core_metrics_dir / f) for f in all_metric_files],\n",
        "        # metrics_mask se genera después en el pipeline de entrenamiento\n",
        "        \"logs_texts_file\": str(logs_texts_path),\n",
        "        \"pods\": pods,\n",
        "        \"services\": services,\n",
        "        \"service_to_idx\": service_to_idx,\n",
        "        \"pod_to_service_idx\": pod_to_service_idx,  \n",
        "    }\n",
        "\n",
        "    ground_truth = {\n",
        "        \"scenario_id\": scenario_id,\n",
        "        \"fault_timestamp_raw\": str(fault_ts),\n",
        "        \"fault_bin\": str(fault_bin),\n",
        "        \"fault_time_idx\": fault_time_idx,\n",
        "        \"root_cause_service\": rc_service,\n",
        "        \"root_cause_service_idx\": rc_service_idx,\n",
        "        \"root_cause_deployment\": rc_deployment,\n",
        "        \"root_cause_pods\": rc_pods,\n",
        "        \"root_cause_pod_indices\": rc_pod_indices,\n",
        "        \"pod_to_idx\": pod_to_idx,\n",
        "    }\n",
        "\n",
        "    with open(out_dir / \"manifest.json\", \"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "    with open(out_dir / \"ground_truth.json\", \"w\") as f:\n",
        "        json.dump(ground_truth, f, indent=2)\n",
        "\n",
        "    return out_dir\n",
        "\n",
        "\n",
        "def crop_metrics_to_fault_window(\n",
        "    core_metrics_dir: Path, \n",
        "    scenario_id: str,\n",
        "    pre_minutes: int = PRE_FAULT_MINUTES,\n",
        "    post_minutes: int = POST_FAULT_MINUTES\n",
        ") -> tuple[pd.DatetimeIndex, int]:\n",
        "    \"\"\"\n",
        "    Crop all metric parquets to a window around the fault time.\n",
        "    \n",
        "    This dramatically reduces:\n",
        "    - Data size: from 2600-5500 to ~90 timesteps\n",
        "    - Log processing time: only processes logs in the cropped window\n",
        "    - Memory usage during training\n",
        "    \n",
        "    Args:\n",
        "        core_metrics_dir: Path to directory with metric parquets\n",
        "        scenario_id: Scenario identifier\n",
        "        pre_minutes: Minutes before fault to include\n",
        "        post_minutes: Minutes after fault to include\n",
        "        \n",
        "    Returns:\n",
        "        cropped_time_index: The new time index after cropping\n",
        "        fault_time_idx: Index of fault bin in cropped window\n",
        "    \"\"\"\n",
        "    cfg = SCENARIO_CONFIG[scenario_id]\n",
        "    fault_ts = cfg[\"fault_timestamp_raw\"]\n",
        "    fault_bin = fault_ts.floor(\"30s\")\n",
        "    \n",
        "    # Calculate window bounds\n",
        "    window_start = fault_bin - pd.Timedelta(minutes=pre_minutes)\n",
        "    window_end = fault_bin + pd.Timedelta(minutes=post_minutes)\n",
        "    \n",
        "    # Get all metric parquet files\n",
        "    metric_files = list(core_metrics_dir.glob(\"*.parquet\"))\n",
        "    \n",
        "    # Read reference to get original shape\n",
        "    ref_df = pd.read_parquet(core_metrics_dir / \"pod_cpu_usage_total.parquet\")\n",
        "    original_len = len(ref_df)\n",
        "    \n",
        "    # Crop each parquet file\n",
        "    cropped_time_index = None\n",
        "    for parquet_file in metric_files:\n",
        "        df = pd.read_parquet(parquet_file)\n",
        "        \n",
        "        # Crop to window\n",
        "        mask = (df.index >= window_start) & (df.index <= window_end)\n",
        "        df_cropped = df.loc[mask]\n",
        "        \n",
        "        if cropped_time_index is None:\n",
        "            cropped_time_index = df_cropped.index\n",
        "        \n",
        "        # Overwrite with cropped version\n",
        "        df_cropped.to_parquet(parquet_file)\n",
        "    \n",
        "    # Calculate fault_time_idx in cropped window\n",
        "    time_index_floored = cropped_time_index.floor(\"30s\")\n",
        "    if fault_bin in time_index_floored:\n",
        "        fault_time_idx = int(time_index_floored.get_loc(fault_bin))\n",
        "    else:\n",
        "        fault_time_idx = int(np.searchsorted(time_index_floored, fault_bin))\n",
        "        fault_time_idx = min(fault_time_idx, len(cropped_time_index) - 1)\n",
        "    \n",
        "    cropped_len = len(cropped_time_index)\n",
        "    print(f\"  [crop] {original_len} -> {cropped_len} timesteps ({cropped_len/original_len*100:.1f}%)\")\n",
        "    print(f\"  [crop] Window: {window_start} to {window_end}\")\n",
        "    print(f\"  [crop] Fault at index {fault_time_idx}/{cropped_len}\")\n",
        "    \n",
        "    return cropped_time_index, fault_time_idx\n"
      ],
      "execution_count": 2,
      "outputs": [],
      "id": "e7be729a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Execute Pipeline\n",
        "\n",
        "Runs align → crop → log aggregation → manifest generation for each scenario.\n",
        "\n",
        "Raw LEMMA has 2600-5500 timesteps (~22-46h). Cropped to 45 min around fault (15 pre + 30 post → ~90 bins). Log processing parallelized with 20 workers, early-terminates past `time_end` — combined speedup ~95-99%.\n",
        ""
      ],
      "id": "8810c70a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_scenario(scenario_id: str) -> tuple[Path, dict]:\n",
        "    \"\"\"\n",
        "    Prepare a complete scenario and return alignment validation.\n",
        "    \n",
        "    Pipeline:\n",
        "    1. align_and_trim_metrics() - Align to 30s grid\n",
        "    2. select_core_metrics() - Copy 4 core metrics + create 3 empty extras\n",
        "    3. crop_metrics_to_fault_window() - Crop to PRE/POST window around fault\n",
        "    4. aggregate_logs_to_services() - Process logs (only cropped window)\n",
        "    5. build_manifest_and_ground_truth() - Generate JSON files\n",
        "    \"\"\"\n",
        "    print(f\"  [1/5] Aligning metrics to 30s grid...\")\n",
        "    trimmed_dir, validation = align_and_trim_metrics(scenario_id)\n",
        "    \n",
        "    print(f\"  [2/5] Selecting core metrics...\")\n",
        "    core_metrics_dir = select_core_metrics(trimmed_dir, scenario_id)\n",
        "\n",
        "    # CROP metrics to fault window BEFORE processing logs\n",
        "    # This dramatically reduces log processing time\n",
        "    print(f\"  [3/5] Cropping to fault window...\")\n",
        "    cropped_time_index, fault_time_idx = crop_metrics_to_fault_window(\n",
        "        core_metrics_dir, scenario_id\n",
        "    )\n",
        "    \n",
        "    # Use cropped time_index for subsequent processing\n",
        "    metrics_ref = pd.read_parquet(core_metrics_dir / \"pod_cpu_usage_total.parquet\")\n",
        "    time_index = metrics_ref.index  # Now cropped\n",
        "    pods = list(metrics_ref.columns)\n",
        "    pod_to_service = build_pod_to_service_from_pods(pods)\n",
        "    services = sorted(set(pod_to_service.values()))\n",
        "\n",
        "    print(f\"  [4/5] Aggregating logs to services...\")\n",
        "    logs_texts_path = aggregate_logs_to_services(scenario_id, time_index, services)\n",
        "    \n",
        "    print(f\"  [5/5] Building manifest and ground truth...\")\n",
        "    multimodal_dir = build_manifest_and_ground_truth(scenario_id, core_metrics_dir, logs_texts_path)\n",
        "    \n",
        "    # Save validation info\n",
        "    val_path = multimodal_dir / \"alignment_validation.json\"\n",
        "    with open(val_path, \"w\") as f:\n",
        "        json.dump(validation, f, indent=2)\n",
        "\n",
        "    return multimodal_dir, validation\n",
        "\n",
        "\n",
        "def print_validation_report(validations: list[dict]):\n",
        "    \"\"\"Print alignment validation report.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"METRIC ALIGNMENT VALIDATION REPORT\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"{'Scenario':<12} {'Metric':<35} {'src_s':>6} {'ref_s':>6} {'tol_s':>6} {'drift':>6} {'nan%':>6} {'imp%':>6}\")\n",
        "    print(\"-\"*90)\n",
        "    for v in validations:\n",
        "        sid = v[\"scenario\"]\n",
        "        ref_step = v[\"ref_step_s\"]\n",
        "        for mname, m in v[\"metrics\"].items():\n",
        "            mshort = mname.replace(\"pod_level_data_\", \"\")[:30]\n",
        "            print(f\"{sid:<12} {mshort:<35} {m['src_step_s']:>6.0f} {ref_step:>6.0f} {m['tolerance_s']:>6.0f} {m['max_drift_s']:>6.1f} {m['nan_ratio']*100:>5.1f}% {m['imputed_ratio']*100:>5.1f}%\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "\n",
        "all_validations = []\n",
        "for sid in [\"20231207\", \"20231221\", \"20240115\", \"20240215\"]:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {sid}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    out_dir, validation = prepare_scenario(sid)\n",
        "    all_validations.append(validation)\n",
        "    print(f\"[OK] {sid} -> {out_dir}\")\n",
        "\n",
        "print_validation_report(all_validations)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Processing 20231207...\n",
            "============================================================\n",
            "  [1/5] Aligning metrics to 30s grid...\n",
            "  [2/5] Selecting core metrics...\n",
            "  [metrics] Created 7 parquets in /root/lemm/core_metrics_tmp/20231207\n",
            "  [3/5] Cropping to fault window...\n",
            "  [crop] 2642 -> 91 timesteps (3.4%)\n",
            "  [crop] Window: 2023-12-07 07:38:00+00:00 to 2023-12-07 08:23:00+00:00\n",
            "  [crop] Fault at index 30/91\n",
            "  [4/5] Aggregating logs to services...\n",
            "[INFO] Largest: currencyservice-696d6df76c-xdz9b_messages_structured.csv (1.2 GB)\n",
            "[INFO] Processing 195 files with 20 CPU workers...\n",
            "  Processing 20/195 files...\n",
            "  Processing 40/195 files...\n",
            "  Processing 60/195 files...\n",
            "  Processing 80/195 files...\n",
            "  Processing 100/195 files...\n",
            "  Processing 120/195 files...\n",
            "  Processing 140/195 files...\n",
            "  Processing 160/195 files...\n",
            "  Processing 180/195 files...\n",
            "[OK] Processed 195 files\n",
            "  [5/5] Building manifest and ground truth...\n",
            "[OK] 20231207 -> /root/lemm/core_multimodal_tmp/20231207\n",
            "\n",
            "============================================================\n",
            "Processing 20231221...\n",
            "============================================================\n",
            "  [1/5] Aligning metrics to 30s grid...\n",
            "  [2/5] Selecting core metrics...\n",
            "  [metrics] Created 7 parquets in /root/lemm/core_metrics_tmp/20231221\n",
            "  [3/5] Cropping to fault window...\n",
            "  [crop] 3592 -> 91 timesteps (2.5%)\n",
            "  [crop] Window: 2023-12-20 02:17:00+00:00 to 2023-12-20 03:02:00+00:00\n",
            "  [crop] Fault at index 30/91\n",
            "  [4/5] Aggregating logs to services...\n",
            "[INFO] Largest: productpage-v1-94d68db49-74tjw_messages_structured.csv (16.9 GB)\n",
            "[INFO] Processing 370 files with 20 CPU workers...\n",
            "  Processing 20/370 files...\n",
            "  Processing 40/370 files...\n",
            "  Processing 60/370 files...\n",
            "  Processing 80/370 files...\n",
            "  Processing 100/370 files...\n",
            "  Processing 120/370 files...\n",
            "  Processing 140/370 files...\n",
            "  Processing 160/370 files...\n",
            "  Processing 180/370 files...\n",
            "  Processing 200/370 files...\n",
            "  Processing 220/370 files...\n",
            "  Processing 240/370 files...\n",
            "  Processing 260/370 files...\n",
            "  Processing 280/370 files...\n",
            "  Processing 300/370 files...\n",
            "  Processing 320/370 files...\n",
            "  Processing 340/370 files...\n",
            "  Processing 360/370 files...\n",
            "[OK] Processed 370 files\n",
            "  [5/5] Building manifest and ground truth...\n",
            "[OK] 20231221 -> /root/lemm/core_multimodal_tmp/20231221\n",
            "\n",
            "============================================================\n",
            "Processing 20240115...\n",
            "============================================================\n",
            "  [1/5] Aligning metrics to 30s grid...\n",
            "  [2/5] Selecting core metrics...\n",
            "  [metrics] Created 7 parquets in /root/lemm/core_metrics_tmp/20240115\n",
            "  [3/5] Cropping to fault window...\n",
            "  [crop] 5321 -> 91 timesteps (1.7%)\n",
            "  [crop] Window: 2024-01-14 07:11:30+00:00 to 2024-01-14 07:56:30+00:00\n",
            "  [crop] Fault at index 30/91\n",
            "  [4/5] Aggregating logs to services...\n",
            "[INFO] Largest: productpage-v1-94d68db49-jjkdt_messages_structured.csv (31.8 GB)\n",
            "[INFO] Processing 131 files with 20 CPU workers...\n",
            "  Processing 20/131 files...\n",
            "  Processing 40/131 files...\n",
            "  Processing 60/131 files...\n",
            "  Processing 80/131 files...\n",
            "  Processing 100/131 files...\n",
            "  Processing 120/131 files...\n",
            "[OK] Processed 131 files\n",
            "  [5/5] Building manifest and ground truth...\n",
            "[OK] 20240115 -> /root/lemm/core_multimodal_tmp/20240115\n",
            "\n",
            "============================================================\n",
            "Processing 20240215...\n",
            "============================================================\n",
            "  [1/5] Aligning metrics to 30s grid...\n",
            "  [2/5] Selecting core metrics...\n",
            "  [metrics] Created 7 parquets in /root/lemm/core_metrics_tmp/20240215\n",
            "  [3/5] Cropping to fault window...\n",
            "  [crop] 5527 -> 90 timesteps (1.6%)\n",
            "  [crop] Window: 2024-02-14 05:56:00+00:00 to 2024-02-14 06:41:00+00:00\n",
            "  [crop] Fault at index 30/90\n",
            "  [4/5] Aggregating logs to services...\n",
            "[INFO] Largest: loadgenerator-788d5978c4-w7bql_messages_structured.csv (0.5 GB)\n",
            "[INFO] Processing 2678 files with 20 CPU workers...\n",
            "  Processing 20/2678 files...\n",
            "  Processing 40/2678 files...\n",
            "  Processing 60/2678 files...\n",
            "  Processing 80/2678 files...\n",
            "  Processing 100/2678 files...\n",
            "  Processing 120/2678 files...\n",
            "  Processing 140/2678 files...\n",
            "  Processing 160/2678 files...\n",
            "  Processing 180/2678 files...\n",
            "  Processing 200/2678 files...\n",
            "  Processing 220/2678 files...\n",
            "  Processing 240/2678 files...\n",
            "  Processing 260/2678 files...\n",
            "  Processing 280/2678 files...\n",
            "  Processing 300/2678 files...\n",
            "  Processing 320/2678 files...\n",
            "  Processing 340/2678 files...\n",
            "  Processing 360/2678 files...\n",
            "  Processing 380/2678 files...\n",
            "  Processing 400/2678 files...\n",
            "  Processing 420/2678 files...\n",
            "  Processing 440/2678 files...\n",
            "  Processing 460/2678 files...\n",
            "  Processing 480/2678 files...\n",
            "  Processing 500/2678 files...\n",
            "  Processing 520/2678 files...\n",
            "  Processing 540/2678 files...\n",
            "  Processing 560/2678 files...\n",
            "  Processing 580/2678 files...\n",
            "  Processing 600/2678 files...\n",
            "  Processing 620/2678 files...\n",
            "  Processing 640/2678 files...\n",
            "  Processing 660/2678 files...\n",
            "  Processing 680/2678 files...\n",
            "  Processing 700/2678 files...\n",
            "  Processing 720/2678 files...\n",
            "  Processing 740/2678 files...\n",
            "  Processing 760/2678 files...\n",
            "  Processing 780/2678 files...\n",
            "  Processing 800/2678 files...\n",
            "  Processing 820/2678 files...\n",
            "  Processing 840/2678 files...\n",
            "  Processing 860/2678 files...\n",
            "  Processing 880/2678 files...\n",
            "  Processing 900/2678 files...\n",
            "  Processing 920/2678 files...\n",
            "  Processing 940/2678 files...\n",
            "  Processing 960/2678 files...\n",
            "  Processing 980/2678 files...\n",
            "  Processing 1000/2678 files...\n",
            "  Processing 1020/2678 files...\n",
            "  Processing 1040/2678 files...\n",
            "  Processing 1060/2678 files...\n",
            "  Processing 1080/2678 files...\n",
            "  Processing 1100/2678 files...\n",
            "  Processing 1120/2678 files...\n",
            "  Processing 1140/2678 files...\n",
            "  Processing 1160/2678 files...\n",
            "  Processing 1180/2678 files...\n",
            "  Processing 1200/2678 files...\n",
            "  Processing 1220/2678 files...\n",
            "  Processing 1240/2678 files...\n",
            "  Processing 1260/2678 files...\n",
            "  Processing 1280/2678 files...\n",
            "  Processing 1300/2678 files...\n",
            "  Processing 1320/2678 files...\n",
            "  Processing 1340/2678 files...\n",
            "  Processing 1360/2678 files...\n",
            "  Processing 1380/2678 files...\n",
            "  Processing 1400/2678 files...\n",
            "  Processing 1420/2678 files...\n",
            "  Processing 1440/2678 files...\n",
            "  Processing 1460/2678 files...\n",
            "  Processing 1480/2678 files...\n",
            "  Processing 1500/2678 files...\n",
            "  Processing 1520/2678 files...\n",
            "  Processing 1540/2678 files...\n",
            "  Processing 1560/2678 files...\n",
            "  Processing 1580/2678 files...\n",
            "  Processing 1600/2678 files...\n",
            "  Processing 1620/2678 files...\n",
            "  Processing 1640/2678 files...\n",
            "  Processing 1660/2678 files...\n",
            "  Processing 1680/2678 files...\n",
            "  Processing 1700/2678 files...\n",
            "  Processing 1720/2678 files...\n",
            "  Processing 1740/2678 files...\n",
            "  Processing 1760/2678 files...\n",
            "  Processing 1780/2678 files...\n",
            "  Processing 1800/2678 files...\n",
            "  Processing 1820/2678 files...\n",
            "  Processing 1840/2678 files...\n",
            "  Processing 1860/2678 files...\n",
            "  Processing 1880/2678 files...\n",
            "  Processing 1900/2678 files...\n",
            "  Processing 1920/2678 files...\n",
            "  Processing 1940/2678 files...\n",
            "  Processing 1960/2678 files...\n",
            "  Processing 1980/2678 files...\n",
            "  Processing 2000/2678 files...\n",
            "  Processing 2020/2678 files...\n",
            "  Processing 2040/2678 files...\n",
            "  Processing 2060/2678 files...\n",
            "  Processing 2080/2678 files...\n",
            "  Processing 2100/2678 files...\n",
            "  Processing 2120/2678 files...\n",
            "  Processing 2140/2678 files...\n",
            "  Processing 2160/2678 files...\n",
            "  Processing 2180/2678 files...\n",
            "  Processing 2200/2678 files...\n",
            "  Processing 2220/2678 files...\n",
            "  Processing 2240/2678 files...\n",
            "  Processing 2260/2678 files...\n",
            "  Processing 2280/2678 files...\n",
            "  Processing 2300/2678 files...\n",
            "  Processing 2320/2678 files...\n",
            "  Processing 2340/2678 files...\n",
            "  Processing 2360/2678 files...\n",
            "  Processing 2380/2678 files...\n",
            "  Processing 2400/2678 files...\n",
            "  Processing 2420/2678 files...\n",
            "  Processing 2440/2678 files...\n",
            "  Processing 2460/2678 files...\n",
            "  Processing 2480/2678 files...\n",
            "  Processing 2500/2678 files...\n",
            "  Processing 2520/2678 files...\n",
            "  Processing 2540/2678 files...\n",
            "  Processing 2560/2678 files...\n",
            "  Processing 2580/2678 files...\n",
            "  Processing 2600/2678 files...\n",
            "  Processing 2620/2678 files...\n",
            "  Processing 2640/2678 files...\n",
            "  Processing 2660/2678 files...\n",
            "[OK] Processed 2678 files\n",
            "  [5/5] Building manifest and ground truth...\n",
            "[OK] 20240215 -> /root/lemm/core_multimodal_tmp/20240215\n",
            "\n",
            "==========================================================================================\n",
            "METRIC ALIGNMENT VALIDATION REPORT\n",
            "==========================================================================================\n",
            "Scenario     Metric                               src_s  ref_s  tol_s  drift   nan%   imp%\n",
            "------------------------------------------------------------------------------------------\n",
            "20231207     pod_cpu_usage_total                      1     30     15    0.0   0.0%   0.0%\n",
            "20231207     pod_memory_working_set                   1     30     15    0.0   0.0%   0.0%\n",
            "20231207     pod_network_rx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20231207     pod_network_tx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20231221     pod_cpu_usage_total                      1     30     15    0.0   0.0%   0.0%\n",
            "20231221     pod_memory_working_set                   1     30     15    0.0   0.0%   0.0%\n",
            "20231221     pod_network_rx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20231221     pod_network_tx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20240115     pod_cpu_usage_total                      1     30     15    0.0   0.0%   0.0%\n",
            "20240115     pod_memory_working_set                   1     30     15    0.0   0.0%   0.0%\n",
            "20240115     pod_network_rx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20240115     pod_network_tx_bytes                     1     30     15    0.0   0.0%   0.0%\n",
            "20240215     pod_cpu_usage_total                    900     30    450  510.0  45.5%   4.2%\n",
            "20240215     pod_memory_working_set                 900     30    450  510.0  45.5%   4.2%\n",
            "20240215     pod_network_rx_bytes                  1140     30    570  630.0  40.2%   3.7%\n",
            "20240215     pod_network_tx_bytes                  1140     30    570  630.0  40.2%   3.7%\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "id": "efbc86a7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}