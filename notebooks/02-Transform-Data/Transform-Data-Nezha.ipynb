{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nezha – Data Transformation\n",
        "\n",
        "Converts raw Nezha (OnlineBoutique + TrainTicket) into the unified multimodal format:\n",
        "metrics aligned to 30s bins, logs aggregated at service level, manifest/ground_truth JSONs.\n",
        "\n",
        "Produces 7 metric parquets (4 core + 3 Nezha-only extras), service-level logs, and per-scenario JSONs.\n",
        "\n",
        "| Date | App | Scenarios | Fault types |\n",
        "|------|-----|-----------|-------------|\n",
        "| 2022-08-22 | OnlineBoutique | 24 | cpu_contention, network_delay, exception, return |\n",
        "| 2022-08-23 | OnlineBoutique | 35 | cpu_contention, network_delay, exception, return |\n",
        "| 2023-01-29 | TrainTicket | 28 | cpu_contention, exception, return |\n",
        "| 2023-01-30 | TrainTicket | 17 | cpu_contention, network_delay |"
      ],
      "id": "ccc272c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === CONFIGURATION ===\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "NEZHA_DIR = Path(\"/root/lemm/Nezha\")\n",
        "OUTPUT_BASE = Path(\"/root/lemm\")\n",
        "\n",
        "# ============================================================================\n",
        "# ALL NEZHA DATES\n",
        "# ============================================================================\n",
        "# OnlineBoutique: 2022-08-22, 2022-08-23\n",
        "# Train-Ticket:   2023-01-29, 2023-01-30\n",
        "ALL_DATES = [\"2022-08-22\", \"2022-08-23\", \"2023-01-29\", \"2023-01-30\"]\n",
        "\n",
        "# Known problematic scenarios (exclude)\n",
        "EXCLUDED_SCENARIOS = {\n",
        "    # Bad data\n",
        "    \"20220822_nezha_22\",  # adservice timestamps corrupted (year 2028)\n",
        "    \"20220822_nezha_23\",  # adservice timestamps corrupted (year 2028)\n",
        "    \"20230130_nezha_15\",  # ts-security-service CPU=0 in original dataset\n",
        "    \"20230130_nezha_16\",  # ts-security-service CPU=0 in original dataset\n",
        "    # Poor signal (Class D)\n",
        "    \"20220822_nezha_14\",  # network_delay no signal\n",
        "    \"20220823_nezha_21\",  # network_delay no signal\n",
        "    \"20220823_nezha_24\",  # network_delay no signal\n",
        "}\n",
        "\n",
        "# Time window\n",
        "PRE_FAULT_MINUTES = 3\n",
        "POST_FAULT_MINUTES = 7\n",
        "BIN_SECONDS = 30\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE METRIC MAPPING: ALL 19 METRICS AVAILABLE IN NEZHA\n",
        "# ============================================================================\n",
        "# We now use ALL metrics (not just CORE + EXTRA) to detect anomalies\n",
        "# that might be visible in metrics we were previously ignoring\n",
        "\n",
        "# Complete mapping: Nezha column -> internal name -> parquet file\n",
        "COMPLETE_COLUMN_MAP = {\n",
        "    # CORE (4 metrics - for LEMMA compatibility)\n",
        "    \"CpuUsageRate(%)\": \"cpu_usage_rate\",\n",
        "    \"MemoryUsage(Mi)\": \"memory_usage\",\n",
        "    \"NetworkReceiveBytes\": \"network_rx_bytes\",\n",
        "    \"NetworkTransmitBytes\": \"network_tx_bytes\",\n",
        "    \n",
        "    # EXTRA (3 metrics - Nezha-specific)\n",
        "    \"PodServerLatencyP95(s)\": \"latency_server_p95\",\n",
        "    \"PodClientLatencyP95(s)\": \"latency_client_p95\",\n",
        "    \"PodWorkload(Ops)\": \"workload_ops\",\n",
        "    \n",
        "    # ADDITIONAL METRICS (12 metrics we were NOT using - may contain anomalies!)\n",
        "    \"CpuUsage(m)\": \"cpu_usage_m\",\n",
        "    \"MemoryUsageRate(%)\": \"memory_usage_rate\",\n",
        "    \"PodSuccessRate(%)\": \"pod_success_rate\",  # IMPORTANT: success rate for fault detection\n",
        "    \"SyscallRead\": \"syscall_read\",\n",
        "    \"SyscallWrite\": \"syscall_write\",\n",
        "    \"NodeCpuUsageRate(%)\": \"node_cpu_usage_rate\",  # Node-level metrics\n",
        "    \"NodeMemoryUsageRate(%)\": \"node_memory_usage_rate\",\n",
        "    \"NodeNetworkReceiveBytes\": \"node_network_rx_bytes\",\n",
        "    \"PodClientLatencyP90(s)\": \"latency_client_p90\",  # Other latency percentiles\n",
        "    \"PodClientLatencyP99(s)\": \"latency_client_p99\",\n",
        "    \"PodServerLatencyP90(s)\": \"latency_server_p90\",\n",
        "    \"PodServerLatencyP99(s)\": \"latency_server_p99\",\n",
        "}\n",
        "\n",
        "# Generate file map for all metrics\n",
        "ALL_FILE_MAP = {}\n",
        "for nezha_col, internal_name in COMPLETE_COLUMN_MAP.items():\n",
        "    ALL_FILE_MAP[internal_name] = f\"pod_{internal_name}.parquet\"\n",
        "\n",
        "# Backward compatibility: CORE mapping (for LEMMA compatibility)\n",
        "CORE_COLUMN_MAP = {\n",
        "    \"CpuUsageRate(%)\": \"cpu_usage_rate\",\n",
        "    \"MemoryUsage(Mi)\": \"memory_usage\",\n",
        "    \"NetworkReceiveBytes\": \"network_rx_bytes\",\n",
        "    \"NetworkTransmitBytes\": \"network_tx_bytes\",\n",
        "}\n",
        "\n",
        "# Backward compatibility: EXTRA mapping\n",
        "EXTRA_COLUMN_MAP = {\n",
        "    \"PodServerLatencyP95(s)\": \"latency_server_p95\",\n",
        "    \"PodClientLatencyP95(s)\": \"latency_client_p95\",\n",
        "    \"PodWorkload(Ops)\": \"workload_ops\",\n",
        "}\n",
        "\n",
        "# Metric order: CORE first (for LEMMA compatibility), then EXTRA, then ADDITIONAL\n",
        "METRIC_ORDER = [\n",
        "    # CORE (4)\n",
        "    \"cpu_usage_rate\", \"memory_usage\", \"network_rx_bytes\", \"network_tx_bytes\",\n",
        "    # EXTRA (3)\n",
        "    \"latency_server_p95\", \"latency_client_p95\", \"workload_ops\",\n",
        "    # ADDITIONAL (12)\n",
        "    \"cpu_usage_m\", \"memory_usage_rate\", \"pod_success_rate\",\n",
        "    \"syscall_read\", \"syscall_write\",\n",
        "    \"node_cpu_usage_rate\", \"node_memory_usage_rate\", \"node_network_rx_bytes\",\n",
        "    \"latency_client_p90\", \"latency_client_p99\", \"latency_server_p90\", \"latency_server_p99\",\n",
        "]\n",
        "\n",
        "# Backward compatibility\n",
        "CORE_FILE_MAP = {k: v for k, v in ALL_FILE_MAP.items() if k in CORE_COLUMN_MAP.values()}\n",
        "\n",
        "# Verify paths\n",
        "fault_list_path = NEZHA_DIR / \"rca_data\" / \"2022-08-22\"  / f\"2022-08-22-fault_list.json\"\n",
        "print(f\"[OK] Nezha dir exists: {NEZHA_DIR.exists()}\")\n",
        "print(f\"[OK] Fault list exists: {fault_list_path.exists()}\")\n",
        "print(f\"[OK] Output base exists: {OUTPUT_BASE.exists()}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[OK] Nezha dir exists: True\n",
            "[OK] Fault list exists: True\n",
            "[OK] Output base exists: True\n"
          ]
        }
      ],
      "id": "712d3aa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Functions\n",
        "\n",
        "Uses `inject_time` (string, parsed as UTC) instead of `inject_timestamp` (epoch) because of timezone inconsistencies in some Nezha dates."
      ],
      "id": "f117a81e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === LOAD FUNCTIONS ===\n",
        "def load_and_flatten_faults(date: str) -> list[dict]:\n",
        "    \"\"\"Load fault_list.json and flatten to simple list.\n",
        "    \n",
        "    NOTE: We use inject_time parsed as UTC instead of epoch (inject_timestamp)\n",
        "    because some Nezha datasets have incorrect timezone in epoch.\n",
        "    \"\"\"\n",
        "    path = NEZHA_DIR / \"rca_data\" / date / f\"{date}-fault_list.json\"\n",
        "    by_hour = json.loads(path.read_text())\n",
        "    \n",
        "    faults = []\n",
        "    for hour, hour_faults in by_hour.items():\n",
        "        for fault in hour_faults:\n",
        "            # Usar inject_time parseado como UTC (el epoch tiene bugs en Train-Ticket)\n",
        "            inject_ts_corrected = int(pd.Timestamp(fault[\"inject_time\"], tz=\"UTC\").timestamp())\n",
        "            faults.append({\n",
        "                \"date\": date,\n",
        "                \"inject_time\": fault[\"inject_time\"],\n",
        "                \"inject_timestamp\": inject_ts_corrected,\n",
        "                \"inject_pod\": fault[\"inject_pod\"],\n",
        "                \"inject_type\": fault[\"inject_type\"],\n",
        "            })\n",
        "    \n",
        "    return sorted(faults, key=lambda x: x[\"inject_timestamp\"])\n",
        "\n",
        "# Show summary of all dates\n",
        "print(\"=\"*70)\n",
        "print(\"NEZHA DATASET SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "for date in ALL_DATES:\n",
        "    path = NEZHA_DIR / \"rca_data\" / date / f\"{date}-fault_list.json\"\n",
        "    if path.exists():\n",
        "        faults = load_and_flatten_faults(date)\n",
        "        fault_types = set(f[\"inject_type\"] for f in faults)\n",
        "        print(f\"   {date}: {len(faults):2} faults | Types: {', '.join(sorted(fault_types))}\")\n",
        "    else:\n",
        "        print(f\"   {date}: [WARN] Not found\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "NEZHA DATASET SUMMARY\n",
            "======================================================================\n",
            "   2022-08-22: 24 faults | Types: cpu_consumed, cpu_contention, exception, network_delay, return\n",
            "   2022-08-23: 32 faults | Types: cpu_consumed, cpu_contention, exception, network_delay, return\n",
            "   2023-01-29: 28 faults | Types: cpu_contention, exception, return\n",
            "   2023-01-30: 17 faults | Types: cpu_contention, network_delay\n"
          ]
        }
      ],
      "id": "1ee687f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "5d579765"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === VALIDATION AND BINNING FUNCTIONS ===\n",
        "\n",
        "def get_available_pods(date: str) -> set[str]:\n",
        "    \"\"\"Get pods that have metrics for this day.\"\"\"\n",
        "    metric_dir = NEZHA_DIR / \"rca_data\" / date / \"metric\"\n",
        "    return {f.stem.replace(\"_metric\", \"\") for f in metric_dir.glob(\"*_metric.csv\")}\n",
        "\n",
        "def get_available_log_minutes(date: str) -> set[str]:\n",
        "    \"\"\"Get HH_MM minutes that have log files.\"\"\"\n",
        "    log_dir = NEZHA_DIR / \"rca_data\" / date / \"log\"\n",
        "    return {f.stem.replace(\"_log\", \"\") for f in log_dir.glob(\"*_log.csv\")}\n",
        "\n",
        "def calculate_bins(fault: dict) -> tuple:\n",
        "    \"\"\"Calculate window and verify fault_idx is valid.\"\"\"\n",
        "    inject_ts = pd.Timestamp(fault[\"inject_timestamp\"], unit=\"s\", tz=\"UTC\")\n",
        "    \n",
        "    t_start = (inject_ts - pd.Timedelta(minutes=PRE_FAULT_MINUTES)).floor(f\"{BIN_SECONDS}s\")\n",
        "    t_end = (inject_ts + pd.Timedelta(minutes=POST_FAULT_MINUTES)).ceil(f\"{BIN_SECONDS}s\")\n",
        "    bins = pd.date_range(start=t_start, end=t_end, freq=f\"{BIN_SECONDS}s\", tz=\"UTC\")\n",
        "    \n",
        "    fault_bin_ts = inject_ts.floor(f\"{BIN_SECONDS}s\")\n",
        "    fault_idx = int((bins == fault_bin_ts).argmax())\n",
        "    \n",
        "    # Verify fault_bin is within window\n",
        "    in_range = 0 <= fault_idx < len(bins)\n",
        "    matches = bins[fault_idx] == fault_bin_ts if in_range else False\n",
        "    \n",
        "    return bins, fault_bin_ts, fault_idx, in_range and matches\n",
        "\n",
        "def check_log_coverage(bins: pd.DatetimeIndex, available_minutes: set[str]) -> tuple[int, int]:\n",
        "    \"\"\"Cuenta cuántos minutos de la ventana tienen logs disponibles.\"\"\"\n",
        "    needed_minutes = set()\n",
        "    for b in bins:\n",
        "        needed_minutes.add(f\"{b.hour:02d}_{b.minute:02d}\")\n",
        "    \n",
        "    found = len(needed_minutes & available_minutes)\n",
        "    total = len(needed_minutes)\n",
        "    return found, total\n",
        "\n",
        "# Show statistics per date\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICS PER DATE\")\n",
        "print(\"=\"*70)\n",
        "for date in ALL_DATES:\n",
        "    pods = get_available_pods(date)\n",
        "    log_mins = get_available_log_minutes(date)\n",
        "    print(f\"   {date}: {len(pods):2} pods with metrics | {len(log_mins):3} log minutes\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STATISTICS PER DATE\n",
            "======================================================================\n",
            "   2022-08-22: 10 pods with metrics |  72 log minutes\n",
            "   2022-08-23: 10 pods with metrics |  96 log minutes\n",
            "   2023-01-29: 46 pods with metrics |  84 log minutes\n",
            "   2023-01-30: 46 pods with metrics |  51 log minutes\n"
          ]
        }
      ],
      "id": "2656bc55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PROCESSING FUNCTIONS ===\n",
        "#### Includes log cleaning identical to LEMMA (clean_log_content_vectorized)\n",
        "#### and template counting for full consistency"
      ],
      "id": "8f54a717"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "import json as json_lib\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# ============================================================================\n",
        "# LOG CLEANING (IDENTICAL TO LEMMA)\n",
        "# ============================================================================\n",
        "\n",
        "# Whitelist of common ports to preserve (not replaced by <NUM>)\n",
        "_WHITELIST_PORTS = r'80|443|8080|8443|3000|3306|5432|5631|5672|5900|6379|7001|7070|8081|9000|9001|9090|9200|9300|9411|9999'\n",
        "\n",
        "\n",
        "def clean_log_content_vectorized(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Vectorized version of clean_log_content using pandas str methods.\n",
        "    IDENTICAL TO LEMMA - normalizes logs for better model generalization.\n",
        "    \n",
        "    Replaces:\n",
        "    - Timestamps → <TS>\n",
        "    - UUIDs → <UUID>\n",
        "    - Hex strings → <HEX>\n",
        "    - IPs → <HOST>\n",
        "    - Pod suffixes → -<RAND>\n",
        "    - Large numbers → <NUM>\n",
        "    - Preserves known ports and HTTP status codes\n",
        "    \"\"\"\n",
        "    s = series.fillna('')\n",
        "    # Remove ANSI escape codes\n",
        "    s = s.str.replace(r'\\x1b\\[[0-9;]*m', '', regex=True)\n",
        "    # Timestamps\n",
        "    s = s.str.replace(r'\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z?', '<TS>', regex=True)\n",
        "    # UUIDs\n",
        "    s = s.str.replace(r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}', '<UUID>', regex=True)\n",
        "    # Long hex strings\n",
        "    s = s.str.replace(r'\\b[a-fA-F0-9]{24,}\\b', '<HEX>', regex=True)\n",
        "    # IP addresses (with optional port)\n",
        "    s = s.str.replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d{2,5})?', '<HOST>', regex=True)\n",
        "    # Pod/deployment suffixes (random hashes)\n",
        "    s = s.str.replace(r'(?<=[a-zA-Z0-9])-[a-z0-9]{6,12}-[a-z0-9]{4,6}\\b', '-<RAND>', regex=True)\n",
        "    # IDs in URL paths\n",
        "    s = s.str.replace(r'(?<=/)\\d{4,}(?=/|$|\\s|\")', ':id', regex=True)\n",
        "    # Preserve ports (mark temporarily)\n",
        "    s = s.str.replace(rf'(?i)(\\bport\\b|listen(?:ing)?(?:\\s+on)?|bind(?:ing)?(?:\\s+to)?|socket)[:\\s]+(\\d{{2,5}})', r'\\1 __PRT_\\2__', regex=True)\n",
        "    s = s.str.replace(rf':({_WHITELIST_PORTS})\\b', r':__PRT_\\1__', regex=True)\n",
        "    # Preserve HTTP status codes\n",
        "    s = s.str.replace(r'(?i)(https?|status|code)[:\\s]*([1-5]\\d\\d)\\b', r'\\1 __HTTP_\\2__', regex=True)\n",
        "    # Preserve version numbers\n",
        "    s = s.str.replace(r'\\bv?(\\d+(?:\\.\\d+){1,3})\\b', r'__VER_\\1__', regex=True)\n",
        "    # Replace large numbers\n",
        "    s = s.str.replace(r'\\b\\d{4,}\\b', '<NUM>', regex=True)\n",
        "    # Restore preserved tokens\n",
        "    s = s.str.replace(r'__PRT_(\\d+)__', r'\\1', regex=True)\n",
        "    s = s.str.replace(r'__HTTP_(\\d+)__', r'\\1', regex=True)\n",
        "    s = s.str.replace(r'__VER_([^_]+)__', r'\\1', regex=True)\n",
        "    # Lowercase and normalize whitespace\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# METRIC LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_metrics_for_pod(date: str, pod: str, bins: pd.DatetimeIndex) -> dict[str, pd.Series]:\n",
        "    \"\"\"Carga y alinea métricas CORE + EXTRA de un pod a los bins de 30s.\"\"\"\n",
        "    path = NEZHA_DIR / \"rca_data\" / date / \"metric\" / f\"{pod}_metric.csv\"\n",
        "    if not path.exists():\n",
        "        return {}\n",
        "    \n",
        "    df = pd.read_csv(path)\n",
        "    if \"TimeStamp\" not in df.columns:\n",
        "        return {}\n",
        "    \n",
        "    df[\"ts\"] = pd.to_datetime(df[\"TimeStamp\"], unit=\"s\", utc=True)\n",
        "    df = df.set_index(\"ts\").sort_index()\n",
        "    \n",
        "    # Filter to window with margin\n",
        "    margin = pd.Timedelta(minutes=2)\n",
        "    df = df[(df.index >= bins[0] - margin) & (df.index <= bins[-1] + margin)]\n",
        "    \n",
        "    result = {}\n",
        "    \n",
        "    # UNIFICADO CON LEMMA: ffill(limit=1) sin fillna(0)\n",
        "    # limit=1 because Nezha has data every 60s -> resampling to 30s only 1 bin vacío\n",
        "    # Mantener NaN donde no hay datos (el dataloader hará mask + nan_to_num)\n",
        "    \n",
        "    # Load ALL available metrics (not just CORE + EXTRA)\n",
        "    for nezha_col, internal_name in COMPLETE_COLUMN_MAP.items():\n",
        "        if nezha_col in df.columns:\n",
        "            series = df[nezha_col].resample(f\"{BIN_SECONDS}s\").mean()\n",
        "            series = series.reindex(bins).ffill(limit=1)  # NO fillna(0)\n",
        "            result[internal_name] = series\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOG LOADING & EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def load_logs_for_window(date: str, bins: pd.DatetimeIndex) -> pd.DataFrame:\n",
        "    \"\"\"Carga logs de los archivos HH_MM_log.csv necesarios.\"\"\"\n",
        "    log_dir = NEZHA_DIR / \"rca_data\" / date / \"log\"\n",
        "    \n",
        "    # Identify needed minutes\n",
        "    needed = {f\"{b.hour:02d}_{b.minute:02d}\" for b in bins}\n",
        "    needed |= {f\"{(b - pd.Timedelta(minutes=1)).hour:02d}_{(b - pd.Timedelta(minutes=1)).minute:02d}\" for b in bins}\n",
        "    \n",
        "    dfs = []\n",
        "    for hh_mm in sorted(needed):\n",
        "        path = log_dir / f\"{hh_mm}_log.csv\"\n",
        "        if path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(path, usecols=[\"Timestamp\", \"PodName\", \"Log\"], \n",
        "                                encoding='utf-8', encoding_errors='ignore')\n",
        "                dfs.append(df)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    if not dfs:\n",
        "        return pd.DataFrame(columns=[\"Timestamp\", \"PodName\", \"Log\"])\n",
        "    \n",
        "    logs = pd.concat(dfs, ignore_index=True)\n",
        "    logs[\"ts\"] = pd.to_datetime(logs[\"Timestamp\"], utc=True, errors='coerce')\n",
        "    logs = logs.dropna(subset=[\"ts\"])\n",
        "    logs[\"bin_start\"] = logs[\"ts\"].dt.floor(f\"{BIN_SECONDS}s\")\n",
        "    \n",
        "    return logs[(logs[\"bin_start\"] >= bins[0]) & (logs[\"bin_start\"] <= bins[-1])]\n",
        "\n",
        "\n",
        "def extract_log_message(log_json: str) -> str:\n",
        "    \"\"\"Extrae mensaje del JSON anidado de Nezha.\"\"\"\n",
        "    if not isinstance(log_json, str):\n",
        "        return \"\"\n",
        "    try:\n",
        "        outer = json_lib.loads(log_json)\n",
        "        inner_raw = outer.get(\"log\", \"\")\n",
        "        if isinstance(inner_raw, str):\n",
        "            try:\n",
        "                inner = json_lib.loads(inner_raw)\n",
        "                return inner.get(\"message\", inner.get(\"msg\", \"\"))\n",
        "            except:\n",
        "                return inner_raw.strip()\n",
        "        return \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOG AGGREGATION (IDÉNTICO A LEMMA - con limpieza + template counting)\n",
        "# ============================================================================\n",
        "\n",
        "def aggregate_logs_to_services(logs_df: pd.DataFrame, bins: pd.DatetimeIndex, services: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Agrega logs por SERVICIO y bin (IDÉNTICO A LEMMA).\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Extrae mensaje del JSON anidado de Nezha\n",
        "    2. Limpia con clean_log_content_vectorized (normaliza timestamps, IPs, etc.)\n",
        "    3. Agrupa por (service, bin_start, template_limpio)\n",
        "    4. Cuenta ocurrencias de cada template\n",
        "    5. Formatea como \"template:x{count}\" con most_common(10)\n",
        "    \n",
        "    Resultado: mismo formato que LEMMA logs\n",
        "    \"\"\"\n",
        "    # Estructura: service_bins[service][bin] = Counter({template: count})\n",
        "    service_bins = {svc: {b: Counter() for b in bins} for svc in services}\n",
        "    \n",
        "    if not logs_df.empty:\n",
        "        # 1. Extraer mensaje del JSON\n",
        "        logs_df = logs_df.copy()\n",
        "        logs_df[\"text\"] = logs_df[\"Log\"].apply(extract_log_message)\n",
        "        \n",
        "        # 2. Limpiar con clean_log_content_vectorized (IGUAL QUE LEMMA)\n",
        "        logs_df[\"clean_text\"] = clean_log_content_vectorized(logs_df[\"text\"])\n",
        "        \n",
        "        # 3. Extract service from pod name\n",
        "        logs_df[\"service\"] = logs_df[\"PodName\"].apply(extract_service_from_pod)\n",
        "        \n",
        "        # 4. Group by (service, bin_start, clean_text) and count\n",
        "        for _, row in logs_df.iterrows():\n",
        "            svc = row[\"service\"]\n",
        "            bin_start = row[\"bin_start\"]\n",
        "            template = row[\"clean_text\"]\n",
        "            \n",
        "            if svc in service_bins and bin_start in service_bins[svc] and template:\n",
        "                service_bins[svc][bin_start][template] += 1\n",
        "    \n",
        "    # 5. Construir DataFrame con formato \"template:x{count}\" (IGUAL QUE LEMMA)\n",
        "    columns_data = {}\n",
        "    for svc in services:\n",
        "        col = []\n",
        "        for b in bins:\n",
        "            templates = service_bins[svc][b]\n",
        "            if templates:\n",
        "                # most_common(10) igual que LEMMA\n",
        "                text = \" | \".join(f\"{t}:x{c}\" for t, c in templates.most_common(10))\n",
        "            else:\n",
        "                text = \"\"\n",
        "            col.append(text)\n",
        "        columns_data[svc] = col\n",
        "    \n",
        "    texts_df = pd.DataFrame(columns_data, index=bins)\n",
        "    return texts_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def generate_scenario_id(date: str, idx: int) -> str:\n",
        "    \"\"\"Genera ID simple: YYYYMMDD_nezha_{idx}\"\"\"\n",
        "    date_compact = date.replace(\"-\", \"\")\n",
        "    return f\"{date_compact}_nezha_{idx}\"\n",
        "\n",
        "\n",
        "def extract_service_from_pod(pod_name: str) -> str:\n",
        "    \"\"\"Extrae nombre de servicio del pod (ej: frontend-579b9bff58-t2dbm → frontend)\"\"\"\n",
        "    parts = pod_name.split(\"-\")\n",
        "    if len(parts) >= 3:\n",
        "        # El deployment es todo menos las últimas 2 partes (replicaset hash + pod hash)\n",
        "        return \"-\".join(parts[:-2])\n",
        "    return pod_name\n",
        "\n",
        "\n",
        "def build_service_mappings(pods: list[str]) -> tuple[list[str], dict, list[int]]:\n",
        "    \"\"\"Construye services, service_to_idx, pod_to_service_idx.\"\"\"\n",
        "    pod_services = [extract_service_from_pod(p) for p in pods]\n",
        "    services = sorted(set(pod_services))\n",
        "    service_to_idx = {s: i for i, s in enumerate(services)}\n",
        "    pod_to_service_idx = [service_to_idx[s] for s in pod_services]\n",
        "    return services, service_to_idx, pod_to_service_idx\n",
        "\n",
        "\n",
        "print(\"[OK] Processing functions defined (log cleaning identical to LEMMA)\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[OK] Processing functions defined (log cleaning identical to LEMMA)\n"
          ]
        }
      ],
      "id": "2c4504a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execute Pipeline\n",
        "\n",
        "Loops over all dates, loads fault lists, and for each fault: calculates 30s bins, aligns metrics, aggregates logs by service, and saves everything (7 parquets + logs + JSONs).### Excluded Scenarios\n",
        "\n",
        "Known problematic scenarios are automatically skipped:\n",
        "- Corrupted timestamps (adservice 2028)\n",
        "- Missing metrics (CPU=0)\n",
        "- No detectable signal (Class D)"
      ],
      "id": "0ab163ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === PROCESS ALL SCENARIOS FROM ALL DATES ===\n",
        "\n",
        "def process_single_scenario(fault: dict, idx: int, pods: list[str]) -> dict:\n",
        "    \"\"\"Procesa un fallo y guarda todos los archivos. Retorna info del resultado.\"\"\"\n",
        "    scenario_id = generate_scenario_id(fault[\"date\"], idx)\n",
        "    \n",
        "    # Check if in excluded list\n",
        "    if scenario_id in EXCLUDED_SCENARIOS:\n",
        "        return {\"scenario_id\": scenario_id, \"status\": \"EXCLUDED\", \"reason\": \"Known bad data\"}\n",
        "    \n",
        "    # 1. Calcular bins\n",
        "    bins, fault_bin_ts, fault_idx, _ = calculate_bins(fault)\n",
        "    \n",
        "    # 2. Load metrics (ALL available metrics - up to 19 total)\n",
        "    all_metric_names = list(COMPLETE_COLUMN_MAP.values())\n",
        "    metrics = {name: {} for name in all_metric_names}\n",
        "    for pod in pods:\n",
        "        pod_metrics = load_metrics_for_pod(fault[\"date\"], pod, bins)\n",
        "        for metric_name, series in pod_metrics.items():\n",
        "            metrics[metric_name][pod] = series\n",
        "    \n",
        "    # UNIFICADO CON LEMMA: mantener NaN para pods sin datos\n",
        "    # El dataloader usará mask = ~isnan() y luego nan_to_num()\n",
        "    metrics_dfs = {}\n",
        "    for metric_name, pod_dict in metrics.items():\n",
        "        if pod_dict:\n",
        "            df = pd.DataFrame(pod_dict)\n",
        "            df = df.reindex(columns=pods)  # NO fillna(0) - mantener NaN\n",
        "            metrics_dfs[metric_name] = df\n",
        "    \n",
        "    # 3. Build mappings (needed for service-level logs)\n",
        "    services, service_to_idx, pod_to_service_idx = build_service_mappings(pods)\n",
        "    \n",
        "    # 4. Load and aggregate logs by SERVICE (consistent with LEMMA)\n",
        "    logs_raw = load_logs_for_window(fault[\"date\"], bins)\n",
        "    logs_texts = aggregate_logs_to_services(logs_raw, bins, services)\n",
        "    \n",
        "    # 5. Crear directorios\n",
        "    metrics_dir = OUTPUT_BASE / \"core_metrics_tmp\" / scenario_id\n",
        "    logs_dir = OUTPUT_BASE / \"core_logs_tmp\" / scenario_id\n",
        "    multimodal_dir = OUTPUT_BASE / \"core_multimodal_tmp\" / scenario_id\n",
        "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "    logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    multimodal_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # 6. Save metrics (ALL available metrics - up to 19 files)\n",
        "    for metric_name, df in metrics_dfs.items():\n",
        "        if metric_name in ALL_FILE_MAP:\n",
        "            df.to_parquet(metrics_dir / ALL_FILE_MAP[metric_name])\n",
        "    \n",
        "    # 7. Save logs by SERVICE (consistent with LEMMA)\n",
        "    logs_texts.to_parquet(logs_dir / \"logs_service_texts.parquet\")\n",
        "    \n",
        "    # 8. Construir pod mappings\n",
        "    pod_to_idx = {p: i for i, p in enumerate(pods)}\n",
        "    \n",
        "    root_cause_pod = fault[\"inject_pod\"]\n",
        "    root_cause_pod_idx = pods.index(root_cause_pod)\n",
        "    root_cause_service = extract_service_from_pod(root_cause_pod)\n",
        "    root_cause_service_idx = service_to_idx[root_cause_service]\n",
        "    \n",
        "    # 9. Save ground_truth.json\n",
        "    ground_truth = {\n",
        "        \"scenario_id\": scenario_id,\n",
        "        \"fault_timestamp_raw\": str(pd.Timestamp(fault[\"inject_timestamp\"], unit=\"s\", tz=\"UTC\")),\n",
        "        \"fault_bin\": str(fault_bin_ts),\n",
        "        \"fault_time_idx\": fault_idx,\n",
        "        \"root_cause_service\": root_cause_service,\n",
        "        \"root_cause_service_idx\": root_cause_service_idx,\n",
        "        \"root_cause_deployment\": root_cause_pod.rsplit(\"-\", 1)[0],\n",
        "        \"root_cause_pods\": [root_cause_pod],\n",
        "        \"root_cause_pod_indices\": [root_cause_pod_idx],\n",
        "        \"pod_to_idx\": pod_to_idx,\n",
        "        \"fault_type\": fault[\"inject_type\"],\n",
        "    }\n",
        "    with open(multimodal_dir / \"ground_truth.json\", \"w\") as f:\n",
        "        json.dump(ground_truth, f, indent=2)\n",
        "    \n",
        "    # 10. Save manifest.json (unified schema - ALL metrics)\n",
        "    # Only include metrics that were actually loaded\n",
        "    available_metrics = [k for k in METRIC_ORDER if k in metrics_dfs]\n",
        "    manifest = {\n",
        "        \"scenario_id\": scenario_id,\n",
        "        \"dataset\": \"nezha\",\n",
        "        \"time_start\": str(bins[0]),\n",
        "        \"time_end\": str(bins[-1]),\n",
        "        \"n_timesteps\": len(bins),\n",
        "        \"n_pods\": len(pods),\n",
        "        \"n_services\": len(services),\n",
        "        \"n_metrics\": len(available_metrics),\n",
        "        \"window_T\": f\"{BIN_SECONDS}s\",\n",
        "        \"metrics_files\": [str(metrics_dir / ALL_FILE_MAP[k]) for k in available_metrics if k in ALL_FILE_MAP],\n",
        "        \"logs_texts_file\": str(logs_dir / \"logs_service_texts.parquet\"),\n",
        "        \"pods\": pods,\n",
        "        \"services\": services,\n",
        "        \"service_to_idx\": service_to_idx,\n",
        "        \"pod_to_service_idx\": pod_to_service_idx,\n",
        "    }\n",
        "    with open(multimodal_dir / \"manifest.json\", \"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    \n",
        "    # Calcular cobertura de logs\n",
        "    log_coverage = (logs_texts.astype(str).apply(lambda x: x.str.len()) > 0).values.mean()\n",
        "    \n",
        "    return {\n",
        "        \"scenario_id\": scenario_id,\n",
        "        \"status\": \"OK\",\n",
        "        \"fault_type\": fault[\"inject_type\"],\n",
        "        \"inject_pod\": fault[\"inject_pod\"],\n",
        "        \"n_bins\": len(bins),\n",
        "        \"log_coverage\": log_coverage,\n",
        "    }\n",
        "\n",
        "\n",
        "def process_all_dates():\n",
        "    \"\"\"Process ALL Nezha dates at once.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"PROCESSING ALL NEZHA DATASETS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"   Dates: {ALL_DATES}\")\n",
        "    print(f\"   Excluded: {len(EXCLUDED_SCENARIOS)} known problematic scenarios\")\n",
        "    print()\n",
        "    \n",
        "    for date in ALL_DATES:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {date}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Check exists\n",
        "        fault_list_path = NEZHA_DIR / \"rca_data\" / date / f\"{date}-fault_list.json\"\n",
        "        if not fault_list_path.exists():\n",
        "            print(f\"   [WARN] Not found: {fault_list_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Load faults\n",
        "        faults = load_and_flatten_faults(date)\n",
        "        pods = sorted(get_available_pods(date))\n",
        "        \n",
        "        print(f\"   Faults: {len(faults)} | Pods: {len(pods)}\")\n",
        "        \n",
        "        # Process each fault\n",
        "        date_results = []\n",
        "        for idx, fault in enumerate(faults):\n",
        "            result = process_single_scenario(fault, idx, pods)\n",
        "            date_results.append(result)\n",
        "            \n",
        "            if result.get(\"status\") == \"EXCLUDED\":\n",
        "                print(f\"   [SKIP] {result['scenario_id']} | EXCLUDED\")\n",
        "            else:\n",
        "                print(f\"   [OK] {result['scenario_id']} | {result['fault_type']:15} | logs: {result['log_coverage']:.1%}\")\n",
        "        \n",
        "        all_results.extend(date_results)\n",
        "        \n",
        "        # Day summary\n",
        "        n_ok = sum(1 for r in date_results if r.get(\"status\") == \"OK\")\n",
        "        n_excl = sum(1 for r in date_results if r.get(\"status\") == \"EXCLUDED\")\n",
        "        print(f\"\\n   {date}: {n_ok} processed, {n_excl} excluded\")\n",
        "    \n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    total_ok = sum(1 for r in all_results if r.get(\"status\") == \"OK\")\n",
        "    total_excl = sum(1 for r in all_results if r.get(\"status\") == \"EXCLUDED\")\n",
        "    \n",
        "    print(f\"   Total scenarios: {len(all_results)}\")\n",
        "    print(f\"   Processed:       {total_ok}\")\n",
        "    print(f\"   Excluded:        {total_excl}\")\n",
        "    \n",
        "    # Per date\n",
        "    for date in ALL_DATES:\n",
        "        date_compact = date.replace(\"-\", \"\")\n",
        "        date_results = [r for r in all_results if r[\"scenario_id\"].startswith(date_compact)]\n",
        "        n_ok = sum(1 for r in date_results if r.get(\"status\") == \"OK\")\n",
        "        print(f\"   {date}: {n_ok} valid scenarios\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EJECUTAR PROCESAMIENTO COMPLETO\n",
        "# ============================================================================\n",
        "results = process_all_dates()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PROCESSING ALL NEZHA DATASETS\n",
            "================================================================================\n",
            "   Dates: ['2022-08-22', '2022-08-23', '2023-01-29', '2023-01-30']\n",
            "   Excluded: 7 known problematic scenarios\n",
            "\n",
            "\n",
            "============================================================\n",
            "Processing 2022-08-22...\n",
            "============================================================\n",
            "   Faults: 24 | Pods: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "   [OK] 20220822_nezha_0 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220822_nezha_1 | return          | logs: 31.8%\n",
            "   [OK] 20220822_nezha_2 | cpu_consumed    | logs: 31.8%\n",
            "   [OK] 20220822_nezha_3 | exception       | logs: 31.8%\n",
            "   [OK] 20220822_nezha_4 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220822_nezha_5 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220822_nezha_6 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220822_nezha_7 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220822_nezha_8 | exception       | logs: 28.2%\n",
            "   [OK] 20220822_nezha_9 | return          | logs: 30.5%\n",
            "   [OK] 20220822_nezha_10 | cpu_contention  | logs: 32.7%\n",
            "   [OK] 20220822_nezha_11 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220822_nezha_12 | cpu_contention  | logs: 45.5%\n",
            "   [OK] 20220822_nezha_13 | network_delay   | logs: 31.8%\n",
            "   [SKIP] 20220822_nezha_14 | EXCLUDED\n",
            "   [OK] 20220822_nezha_15 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220822_nezha_16 | cpu_consumed    | logs: 31.8%\n",
            "   [OK] 20220822_nezha_17 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220822_nezha_18 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220822_nezha_19 | cpu_consumed    | logs: 54.5%\n",
            "   [OK] 20220822_nezha_20 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220822_nezha_21 | cpu_contention  | logs: 30.5%\n",
            "   [SKIP] 20220822_nezha_22 | EXCLUDED\n",
            "   [SKIP] 20220822_nezha_23 | EXCLUDED\n",
            "\n",
            "   2022-08-22: 21 processed, 3 excluded\n",
            "\n",
            "============================================================\n",
            "Processing 2022-08-23...\n",
            "============================================================\n",
            "   Faults: 32 | Pods: 10\n",
            "   [OK] 20220823_nezha_0 | cpu_contention  | logs: 31.4%\n",
            "   [OK] 20220823_nezha_1 | return          | logs: 31.8%\n",
            "   [OK] 20220823_nezha_2 | cpu_consumed    | logs: 31.8%\n",
            "   [OK] 20220823_nezha_3 | exception       | logs: 30.5%\n",
            "   [OK] 20220823_nezha_4 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220823_nezha_5 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220823_nezha_6 | network_delay   | logs: 30.5%\n",
            "   [OK] 20220823_nezha_7 | exception       | logs: 26.8%\n",
            "   [OK] 20220823_nezha_8 | return          | logs: 30.5%\n",
            "   [OK] 20220823_nezha_9 | cpu_consumed    | logs: 31.8%\n",
            "   [OK] 20220823_nezha_10 | cpu_contention  | logs: 36.4%\n",
            "   [OK] 20220823_nezha_11 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220823_nezha_12 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220823_nezha_13 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220823_nezha_14 | cpu_consumed    | logs: 30.5%\n",
            "   [OK] 20220823_nezha_15 | network_delay   | logs: 31.8%\n",
            "   [OK] 20220823_nezha_16 | cpu_contention  | logs: 31.8%\n",
            "   [OK] 20220823_nezha_17 | network_delay   | logs: 30.5%\n",
            "   [OK] 20220823_nezha_18 | return          | logs: 42.7%\n",
            "   [OK] 20220823_nezha_19 | cpu_consumed    | logs: 30.5%\n",
            "   [OK] 20220823_nezha_20 | exception       | logs: 31.8%\n",
            "   [SKIP] 20220823_nezha_21 | EXCLUDED\n",
            "   [OK] 20220823_nezha_22 | cpu_contention  | logs: 45.5%\n",
            "   [OK] 20220823_nezha_23 | cpu_consumed    | logs: 34.5%\n",
            "   [SKIP] 20220823_nezha_24 | EXCLUDED\n",
            "   [OK] 20220823_nezha_25 | cpu_contention  | logs: 30.5%\n",
            "   [OK] 20220823_nezha_26 | cpu_consumed    | logs: 30.5%\n",
            "   [OK] 20220823_nezha_27 | network_delay   | logs: 30.5%\n",
            "   [OK] 20220823_nezha_28 | return          | logs: 30.5%\n",
            "   [OK] 20220823_nezha_29 | exception       | logs: 30.5%\n",
            "   [OK] 20220823_nezha_30 | cpu_contention  | logs: 30.5%\n",
            "   [OK] 20220823_nezha_31 | cpu_consumed    | logs: 31.8%\n",
            "\n",
            "   2022-08-23: 30 processed, 2 excluded\n",
            "\n",
            "============================================================\n",
            "Processing 2023-01-29...\n",
            "============================================================\n",
            "   Faults: 28 | Pods: 46\n",
            "   [OK] 20230129_nezha_0 | return          | logs: 15.9%\n",
            "   [OK] 20230129_nezha_1 | return          | logs: 32.0%\n",
            "   [OK] 20230129_nezha_2 | return          | logs: 31.4%\n",
            "   [OK] 20230129_nezha_3 | return          | logs: 16.3%\n",
            "   [OK] 20230129_nezha_4 | return          | logs: 15.7%\n",
            "   [OK] 20230129_nezha_5 | return          | logs: 31.2%\n",
            "   [OK] 20230129_nezha_6 | exception       | logs: 36.7%\n",
            "   [OK] 20230129_nezha_7 | return          | logs: 28.7%\n",
            "   [OK] 20230129_nezha_8 | return          | logs: 18.4%\n",
            "   [OK] 20230129_nezha_9 | return          | logs: 21.7%\n",
            "   [OK] 20230129_nezha_10 | return          | logs: 15.5%\n",
            "   [OK] 20230129_nezha_11 | return          | logs: 26.2%\n",
            "   [OK] 20230129_nezha_12 | exception       | logs: 22.0%\n",
            "   [OK] 20230129_nezha_13 | exception       | logs: 18.4%\n",
            "   [OK] 20230129_nezha_14 | exception       | logs: 19.7%\n",
            "   [OK] 20230129_nezha_15 | exception       | logs: 15.2%\n",
            "   [OK] 20230129_nezha_16 | exception       | logs: 32.1%\n",
            "   [OK] 20230129_nezha_17 | exception       | logs: 29.4%\n",
            "   [OK] 20230129_nezha_18 | exception       | logs: 22.8%\n",
            "   [OK] 20230129_nezha_19 | exception       | logs: 15.9%\n",
            "   [OK] 20230129_nezha_20 | exception       | logs: 33.2%\n",
            "   [OK] 20230129_nezha_21 | exception       | logs: 16.5%\n",
            "   [OK] 20230129_nezha_22 | exception       | logs: 19.5%\n",
            "   [OK] 20230129_nezha_23 | exception       | logs: 14.3%\n",
            "   [OK] 20230129_nezha_24 | cpu_contention  | logs: 15.5%\n",
            "   [OK] 20230129_nezha_25 | cpu_contention  | logs: 17.2%\n",
            "   [OK] 20230129_nezha_26 | cpu_contention  | logs: 16.6%\n",
            "   [OK] 20230129_nezha_27 | cpu_contention  | logs: 16.0%\n",
            "\n",
            "   2023-01-29: 28 processed, 0 excluded\n",
            "\n",
            "============================================================\n",
            "Processing 2023-01-30...\n",
            "============================================================\n",
            "   Faults: 17 | Pods: 46\n",
            "   [OK] 20230130_nezha_0 | network_delay   | logs: 20.8%\n",
            "   [OK] 20230130_nezha_1 | cpu_contention  | logs: 22.4%\n",
            "   [OK] 20230130_nezha_2 | network_delay   | logs: 16.1%\n",
            "   [OK] 20230130_nezha_3 | network_delay   | logs: 15.5%\n",
            "   [OK] 20230130_nezha_4 | network_delay   | logs: 23.2%\n",
            "   [OK] 20230130_nezha_5 | network_delay   | logs: 20.7%\n",
            "   [OK] 20230130_nezha_6 | cpu_contention  | logs: 20.5%\n",
            "   [OK] 20230130_nezha_7 | network_delay   | logs: 21.9%\n",
            "   [OK] 20230130_nezha_8 | network_delay   | logs: 23.9%\n",
            "   [OK] 20230130_nezha_9 | cpu_contention  | logs: 16.1%\n",
            "   [OK] 20230130_nezha_10 | network_delay   | logs: 17.7%\n",
            "   [OK] 20230130_nezha_11 | network_delay   | logs: 15.0%\n",
            "   [OK] 20230130_nezha_12 | network_delay   | logs: 15.7%\n",
            "   [OK] 20230130_nezha_13 | network_delay   | logs: 18.9%\n",
            "   [OK] 20230130_nezha_14 | network_delay   | logs: 13.6%\n",
            "   [SKIP] 20230130_nezha_15 | EXCLUDED\n",
            "   [SKIP] 20230130_nezha_16 | EXCLUDED\n",
            "\n",
            "   2023-01-30: 15 processed, 2 excluded\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY\n",
            "================================================================================\n",
            "   Total scenarios: 101\n",
            "   Processed:       94\n",
            "   Excluded:        7\n",
            "   2022-08-22: 21 valid scenarios\n",
            "   2022-08-23: 30 valid scenarios\n",
            "   2023-01-29: 28 valid scenarios\n",
            "   2023-01-30: 15 valid scenarios\n"
          ]
        }
      ],
      "id": "2bb9770f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Processing Functions\n",
        "\n",
        "Log cleaning is identical to LEMMA (`clean_log_content_vectorized`). Metrics are loaded from CSV, resampled to 30s, and aligned with `ffill(limit=1)`."
      ],
      "id": "acee41ca"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}